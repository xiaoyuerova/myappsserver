Uid,Title,Author,Abstract,Title_CN,Abstract_CN,Link,Meeting,Complete,Locked,Select1,Select2
1625,Bridging Differential Privacy and Byzantine-Robustness via Model Aggregation.,"Heng Zhu, Qing Ling","This paper aims at jointly addressing two seemly conflicting issues in federated learning: differential privacy (DP) and Byzantine-robustness, which are particularly challenging when the distributed data are non-i.i.d. (independent and identically distributed). The standard DP mechanisms add noise to the transmitted messages, and entangles with robust stochastic gradient aggregation to defend against Byzantine attacks. In this paper, we decouple the two issues via robust stochastic model aggregation, in the sense that our proposed DP mechanisms and the defense against Byzantine attacks have separated influence on the learning performance. Leveraging robust stochastic model aggregation, at each iteration, each worker calculates the difference between the local model and the global one, followed by sending the element-wise signs to the master node, which enables robustness to Byzantine attacks. Further, we design two DP mechanisms to perturb the uploaded signs for the purpose of privacy preservation, and prove that they are $(\epsilon,0)$-DP by exploiting the properties of noise distributions. With the tools of Moreau envelop and proximal point projection, we establish the convergence of the proposed algorithm when the cost function is nonconvex. We analyze the trade-off between privacy preservation and learning performance, and show that the influence of our proposed DP mechanisms is decoupled with that of robust stochastic model aggregation. Numerical experiments demonstrate the effectiveness of the proposed algorithm.",通过模型聚集弥合差异隐私和拜占庭式bub。,本文旨在共同解决联邦学习中的两个看似相互矛盾的问题：差异隐私（DP）和拜占庭式企业，这在分布式数据是非i.i.d时尤其具有挑战性。（独立和相同分布）。标准的DP机制为传输消息增加了噪音，并与强大的随机梯度聚集纠缠以防御拜占庭式攻击。在本文中，我们通过强大的随机模型聚合使这两个问题解除了这两个问题，从某种意义上说，我们提出的DP机制和对拜占庭式攻击的辩护对学习绩效的影响分开了。在每次迭代中，利用强大的随机模型聚合，每个工人都计算本地模型和全局模型之间的差异，然后将元素符号发送给主节点，这使拜占庭式攻击能够鲁棒性。此外，我们设计了两种DP机制来扰动上传的标志，以保存隐私，并通过利用噪声分布的属性来证明它们是$（\ epsilon，0）$ -DP。借助Moreau信封和近端投影的工具，当成本函数是非convex时，我们确定了所提出算法的收敛性。我们分析了隐私保护和学习绩效之间的权衡，并表明我们提出的DP机制的影响与强大的随机模型聚集是解耦的。数值实验证明了所提出的算法的有效性。,https://arxiv.org/abs/2205.00107,IJCAI,True,False,False,False
1626,Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization.,"Yanyu Li, Pu Zhao, Geng Yuan, Xue Lin, Yanzhi Wang, Xin Chen","Neural architecture search (NAS) and network pruning are widely studied efficient AI techniques, but not yet perfect. NAS performs exhaustive candidate architecture search, incurring tremendous search cost. Though (structured) pruning can simply shrink model dimension, it remains unclear how to decide the per-layer sparsity automatically and optimally. In this work, we revisit the problem of layer-width optimization and propose Pruning-as-Search (PaS), an end-to-end channel pruning method to search out desired sub-network automatically and efficiently. Specifically, we add a depth-wise binary convolution to learn pruning policies directly through gradient descent. By combining the structural reparameterization and PaS, we successfully searched out a new family of VGG-like and lightweight networks, which enable the flexibility of arbitrary width with respect to each layer instead of each stage. Experimental results show that our proposed architecture outperforms prior arts by around $1.0\%$ top-1 accuracy under similar inference speed on ImageNet-1000 classification task. Furthermore, we demonstrate the effectiveness of our width search on complex tasks including instance segmentation and image translation. Code and models are released.",修剪 - 搜索：通过通道修剪和结构重新聚体化的有效神经体系结构搜索。,神经体系结构搜索（NAS）和网络修剪经过广泛研究，但尚不完美。NAS执行详尽的候选架构搜索，并产生了巨大的搜索成本。尽管（结构化的）修剪可以简单地缩小模型维度，但尚不清楚如何自动和最佳地决定每层稀疏度。在这项工作中，我们重新审视了图层宽度优化的问题，并提出了修剪 - 搜索（PAS），这是一种端到端的通道修剪方法，可以自动有效地搜索所需的子网络。具体而言，我们添加了深度二进制卷积，以直接通过梯度下降来学习修剪政策。通过结合结构重新聚体化和PA，我们成功地搜索了一个新的类似VGG和轻质网络的家族，这可以使任意宽度相对于每个阶段而不是每个阶段的灵活性。实验结果表明，在Imagenet-1000分类任务上，我们提出的架构的表现优于先前的艺术$ 1.0 \％$ $ TOP-1的准确性。此外，我们证明了我们对复杂任务（包括实例分割和图像翻译）的宽度搜索的有效性。代码和模型已发布。,https://arxiv.org/abs/2206.01198,IJCAI,True,False,False,False
1627,SoFaiR: Single Shot Fair Representation Learning.,"Xavier Gitiaux, Huzefa Rangwala","To avoid discriminatory uses of their data, organizations can learn to map them into a representation that filters out information related to sensitive attributes. However, all existing methods in fair representation learning generate a fairness-information trade-off. To achieve different points on the fairness-information plane, one must train different models. In this paper, we first demonstrate that fairness-information trade-offs are fully characterized by rate-distortion trade-offs. Then, we use this key result and propose SoFaiR, a single shot fair representation learning method that generates with one trained model many points on the fairness-information plane. Besides its computational saving, our single-shot approach is, to the extent of our knowledge, the first fair representation learning method that explains what information is affected by changes in the fairness / distortion properties of the representation. Empirically, we find on three datasets that SoFaiR achieves similar fairness-information trade-offs as its multi-shot counterparts.",Sofair：单拍的公平代表学习。,为了避免其数据的歧视用途，组织可以学会将其映射到一个表示与敏感属性有关的信息的表示中。但是，公平代表学习中的所有现有方法都会产生公平的信息权衡。为了在公平信息平面上实现不同的观点，必须训练不同的模型。在本文中，我们首先证明了公平信息的权衡取决于利率差异。然后，我们使用此关键结果，并提出Sofair，这是一种单镜头公平表示学习方法，该方法在公平信息传播平面上使用一个训练有素的模型生成许多点。除了其计算节省外，我们的单发方法是，在我们的知识范围内，是第一个公平表示学习方法，该方法解释了代表的公平 /失真属性的变化影响了哪些信息。从经验上讲，我们在三个数据集中发现，SOFAIR实现了与多拍的相似的公平信息权衡取舍。,https://arxiv.org/abs/2204.12556,IJCAI,True,False,False,False
1628,RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training.,"Luya Wang, Feng Liang, Yangguang Li, Honggang Zhang, Wanli Ouyang, Jing Shao","Recently, self-supervised vision transformers have attracted unprecedented attention for their impressive representation learning ability. However, the dominant method, contrastive learning, mainly relies on an instance discrimination pretext task, which learns a global understanding of the image. This paper incorporates local feature learning into self-supervised vision transformers via Reconstructive Pre-training (RePre). Our RePre extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective. RePre is equipped with a lightweight convolution-based decoder that fuses the multi-hierarchy features from the transformer encoder. The multi-hierarchy features provide rich supervisions from low to high semantic information, which are crucial for our RePre. Our RePre brings decent improvements on various contrastive frameworks with different vision transformer architectures. Transfer performance in downstream tasks outperforms supervised pre-training and state-of-the-art (SOTA) self-supervised counterparts.",repre：通过重建性预训练改善自我监督的视力变压器。,最近，自我监督的视觉变形金刚因其令人印象深刻的表示能力而引起了前所未有的关注。但是，主要的方法是对比学习，主要依赖于实例歧视借口任务，该任务了解了对图像的全球理解。本文通过重建预训练（REPRE）将本地特征学习纳入自我监督的视觉变压器中。我们的repre通过添加一个分支来与现有的对比目标并行重建原始图像像素来扩展对比框架。Repre配备了基于轻量级卷积的解码器，该解码器融合了变压器编码器的多层次结构功能。多等级结构功能可提供从低到高语义信息的丰富监督，这对我们的repre至关重要。我们的代表对具有不同视觉变压器体系结构的各种对比框架进行了体面的改进。下游任务中的转移性能优于监督预训练和最先进的（SOTA）自我监督的同行。,https://arxiv.org/abs/2201.06857,IJCAI,True,False,False,False
1629,Region-Aware Metric Learning for Open World Semantic Segmentation via Meta-Channel Aggregation.,"Hexin Dong, Zifan Chen, Mingze Yuan, Yutong Xie, Jie Zhao, Fei Yu, Bin Dong, Li Zhang","As one of the most challenging and practical segmentation tasks, open-world semantic segmentation requires the model to segment the anomaly regions in the images and incrementally learn to segment out-of-distribution (OOD) objects, especially under a few-shot condition. The current state-of-the-art (SOTA) method, Deep Metric Learning Network (DMLNet), relies on pixel-level metric learning, with which the identification of similar regions having different semantics is difficult. Therefore, we propose a method called region-aware metric learning (RAML), which first separates the regions of the images and generates region-aware features for further metric learning. RAML improves the integrity of the segmented anomaly regions. Moreover, we propose a novel meta-channel aggregation (MCA) module to further separate anomaly regions, forming high-quality sub-region candidates and thereby improving the model performance for OOD objects. To evaluate the proposed RAML, we have conducted extensive experiments and ablation studies on Lost And Found and Road Anomaly datasets for anomaly segmentation and the CityScapes dataset for incremental few-shot learning. The results show that the proposed RAML achieves SOTA performance in both stages of open world segmentation. Our code and appendix are available at https://github.com/czifan/RAML.",通过元通道聚合，开放世界语义分割的区域感知度量学习。,作为最具挑战性和实用的分割任务之一，开放世界的语义细分要求模型在图像中分割异常区域，并逐步学会将分布分布（OOD）对象细分，尤其是在几个弹药条件下。当前的最新方法（SOTA）方法，深度度量学习网络（DMLNET）依赖于像素级度量学习，因此很难识别具有不同语义的类似区域。因此，我们提出了一种称为区域感知度量学习（RAML）的方法，该方法首先将图像区域分开，并生成具有进一步度量学习的区域感知特征。RAML改善了分段异常区域的完整性。此外，我们提出了一种新型的元通道聚集（MCA）模块，以进一步分离异常区域，形成高质量的子区域候选物，从而改善OOD对象的模型性能。为了评估所提出的RAML，我们已经对丢失和发现以及用于异常分割的道路异常数据集进行了广泛的实验和消融研究，并进行了CityScapes数据集，以逐步学习。结果表明，拟议的RAML在开放世界细分的两个阶段都达到了SOTA性能。我们的代码和附录可在https://github.com/czifan/raml上找到。,https://arxiv.org/abs/2205.08083,IJCAI,True,False,False,False
1630,"Bootstrapping Informative Graph Augmentation via A Meta Learning
  Approach","Hang Gao, Jiangmeng Li, Wenwen Qiang, Lingyu Si, Changwen Zheng, Fuchun Sun","Recent works explore learning graph representations in a self-supervised manner. In graph contrastive learning, benchmark methods apply various graph augmentation approaches. However, most of the augmentation methods are non-learnable, which causes the issue of generating unbeneficial augmented graphs. Such augmentation may degenerate the representation ability of graph contrastive learning methods. Therefore, we motivate our method to generate augmented graph by a learnable graph augmenter, called MEta Graph Augmentation (MEGA). We then clarify that a ""good"" graph augmentation must have uniformity at the instance-level and informativeness at the feature-level. To this end, we propose a novel approach to learning a graph augmenter that can generate an augmentation with uniformity and informativeness. The objective of the graph augmenter is to promote our feature extraction network to learn a more discriminative feature representation, which motivates us to propose a meta-learning paradigm. Empirically, the experiments across multiple benchmark datasets demonstrate that MEGA outperforms the state-of-the-art methods in graph self-supervised learning tasks. Further experimental studies prove the effectiveness of different terms of MEGA.","通过元学习进行引导信息增强图
  方法",最近的作品以一种自我监督的方式探索学习图表。在图对比度学习中，基准方法采用了各种图形增强方法。但是，大多数增强方法是不可行的，这导致产生无晶增强图的问题。这种增强可能会退化图形对比学习方法的表示能力。因此，我们激励我们通过可学习的图表增强器（称为Meta Graph Exmentation（Mega））生成增强图的方法。然后，我们澄清说，“良好”的图形扩展必须在实例级别和功能级别的信息级别上具有统一性。为此，我们提出了一种新型的方法来学习增强图，该方法可以以统一性和信息性产生增强。图表增强器的目的是促进我们的功能提取网络，以学习更具歧视性的特征表示，这激发了我们提出元学习范式。从经验上讲，多个基准数据集的实验表明，大型的表现优于图形自学学习任务中最新方法。进一步的实验研究证明了不同术语的有效性。,https://arxiv.org/abs/2201.03812,IJCAI,True,False,False,False
1631,Multi-Agent Reinforcement Learning for Traffic Signal Control through Universal Communication Method.,"Qize Jiang, Minhao Qin, Shengmin Shi, Weiwei Sun, Baihua Zheng","How to coordinate the communication among intersections effectively in real complex traffic scenarios with multi-intersection is challenging. Existing approaches only enable the communication in a heuristic manner without considering the content/importance of information to be shared. In this paper, we propose a universal communication form UniComm between intersections. UniComm embeds massive observations collected at one agent into crucial predictions of their impact on its neighbors, which improves the communication efficiency and is universal across existing methods. We also propose a concise network UniLight to make full use of communications enabled by UniComm. Experimental results on real datasets demonstrate that UniComm universally improves the performance of existing state-of-the-art methods, and UniLight significantly outperforms existing methods on a wide range of traffic situations.",通过通用通信方法进行交通信号控制的多代理增强学习。,如何在实际复杂的交通情况下与多交流有效地协调交叉点之间的交流是具有挑战性的。现有方法仅以启发式方式使通信无需考虑要共享信息的内容/重要性。在本文中，我们提出了在交集之间的普遍通信形式。Unicomm嵌入了一种在一个代理商中收集的大量观察结果，以对其对邻居的影响进行关键预测，从而提高了沟通效率，并且在现有方法中是普遍的。我们还提出了一个简洁的网络UNILIGHT，以充分利用Unicomm启用的通信。真实数据集的实验结果表明，Unicomm普遍提高现有最新方法的性能，而单一的实验性能在广泛的交通情况下明显优于现有方法。,https://arxiv.org/abs/2204.12190,IJCAI,True,False,False,False
1632,Domain Adaptation via Maximizing Surrogate Mutual Information.,"Haiteng Zhao, Chang Ma, Qinyu Chen, Zhihong Deng","Unsupervised domain adaptation (UDA), which is an important topic in transfer learning, aims to predict unlabeled data from target domain with access to labeled data from the source domain. In this work, we propose a novel framework called SIDA (Surrogate Mutual Information Maximization Domain Adaptation) with strong theoretical guarantees. To be specific, SIDA implements adaptation by maximizing mutual information (MI) between features. In the framework, a surrogate joint distribution models the underlying joint distribution of the unlabeled target domain. Our theoretical analysis validates SIDA by bounding the expected risk on target domain with MI and surrogate distribution bias. Experiments show that our approach is comparable with state-of-the-art unsupervised adaptation methods on standard UDA tasks.",域通过最大化替代互信息的适应性。,无监督的域适应性（UDA）是转移学习的重要主题，旨在通过从源域中访问标记数据的目标域中预测未标记的数据。在这项工作中，我们提出了一个名为SIDA（替代信息最大化域适应性）的新型框架，并具有强大的理论保证。具体而言，SIDA通过在特征之间最大化相互信息（MI）来实现适应。在框架中，替代联合分布模拟未标记目标域的基本关节分布。我们的理论分析通过将目标域的预期风险与MI和替代分布偏置界定来验证SIDA。实验表明，我们的方法与标准UDA任务的最新无监督适应方法相媲美。,https://arxiv.org/abs/2110.12184,IJCAI,True,False,False,False
1633,Neural Subgraph Explorer: Reducing Noisy Information via Target-oriented Syntax Graph Pruning.,"Bowen Xing, Ivor W. Tsang","Recent years have witnessed the emerging success of leveraging syntax graphs for the target sentiment classification task. However, we discover that existing syntax-based models suffer from two issues: noisy information aggregation and loss of distant correlations. In this paper, we propose a novel model termed Neural Subgraph Explorer, which (1) reduces the noisy information via pruning target-irrelevant nodes on the syntax graph; (2) introduces beneficial first-order connections between the target and its related words into the obtained graph. Specifically, we design a multi-hop actions score estimator to evaluate the value of each word regarding the specific target. The discrete action sequence is sampled through Gumble-Softmax and then used for both of the syntax graph and the self-attention graph. To introduce the first-order connections between the target and its relevant words, the two pruned graphs are merged. Finally, graph convolution is conducted on the obtained unified graph to update the hidden states. And this process is stacked with multiple layers. To our knowledge, this is the first attempt of target-oriented syntax graph pruning in this task. Experimental results demonstrate the superiority of our model, which achieves new state-of-the-art performance.",神经子图探索器：通过面向目标的语法图修剪来减少嘈杂的信息。,近年来见证了利用语法图对目标情感分类任务的新兴成功。但是，我们发现现有的基于语法的模型遇到了两个问题：嘈杂的信息聚集和遥远相关性的丧失。在本文中，我们提出了一个称为神经子图探险家的新型模型，该模型（1）通过在语法图上的修剪靶标的节点减少嘈杂的信息；（2）将目标及其相关词之间的有益的一阶连接引入获得的图。具体而言，我们设计了一个多跳动动作分数估计器，以评估每个单词有关特定目标的值。通过Gumble-Softmax对离散动作序列进行采样，然后用于语法图和自我发项图。为了引入目标与其相关词之间的一阶连接，合并了两个修剪图。最后，在获得的统一图上进行图形卷积以更新隐藏状态。此过程堆叠了多层。据我们所知，这是该任务中面向目标的语法图的首次尝试。实验结果证明了我们的模型的优势，该模型实现了新的最先进的性能。,https://arxiv.org/abs/2205.10970,IJCAI,True,False,False,False
1634,HashNWalk: Hash and Random Walk Based Anomaly Detection in Hyperedge Streams.,"Geon Lee, Minyoung Choe, Kijung Shin","Sequences of group interactions, such as emails, online discussions, and co-authorships, are ubiquitous; and they are naturally represented as a stream of hyperedges. Despite their broad potential applications, anomaly detection in hypergraphs (i.e., sets of hyperedges) has received surprisingly little attention, compared to that in graphs. While it is tempting to reduce hypergraphs to graphs and apply existing graph-based methods, according to our experiments, taking higher-order structures of hypergraphs into consideration is worthwhile. We propose HashNWalk, an incremental algorithm that detects anomalies in a stream of hyperedges. It maintains and updates a constant-size summary of the structural and temporal information about the stream. Using the summary, which is the form of a proximity matrix, HashNWalk measures the anomalousness of each new hyperedge as it appears. HashNWalk is (a) Fast: it processes each hyperedge in near real-time and billions of hyperedges within a few hours, (b) Space Efficient: the size of the maintained summary is a predefined constant, (c) Effective: it successfully detects anomalous hyperedges in real-world hypergraphs.",Hashnwalk：Hash和随机步行基于Hyperdegy流中的异常检测。,群体互动的序列，例如电子邮件，在线讨论和共同创作，无处不在；而且它们自然地表示为一系列Hyperedges。尽管具有广泛的潜在应用，但与图中相比，相比之下，超图中的异常检测（即，超级中期集）的注意力很少。根据我们的实验，虽然诱使减少图形的超图并应用现有的基于图的方法，但考虑到高阶层的高阶结构值得考虑。我们提出了Hashnwalk，这是一种增量算法，可检测Hyperedges流中的异常。它维护和更新有关流的结构和时间信息的恒定尺寸摘要。使用摘要（即接近矩阵的形式）hashnwalk测量每个新的超edge的异常性。hashnwalk是（a）快速：它在几个小时内接近实时处理每个高架，（b）空间有效：维护摘要的大小是预定义常数，（c）有效：它成功地检测到了。现实世界中超图中的异常性超蛋白质。,https://arxiv.org/abs/2204.13822,IJCAI,True,False,False,False
1635,AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection.,"Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinghong Jiang, Feng Zhao, Bolei Zhou, Hang Zhao","Object detection through either RGB images or the LiDAR point clouds has been extensively explored in autonomous driving. However, it remains challenging to make these two data sources complementary and beneficial to each other. In this paper, we propose \textit{AutoAlign}, an automatic feature fusion strategy for 3D object detection. Instead of establishing deterministic correspondence with camera projection matrix, we model the mapping relationship between the image and point clouds with a learnable alignment map. This map enables our model to automate the alignment of non-homogenous features in a dynamic and data-driven manner. Specifically, a cross-attention feature alignment module is devised to adaptively aggregate \textit{pixel-level} image features for each voxel. To enhance the semantic consistency during feature alignment, we also design a self-supervised cross-modal feature interaction module, through which the model can learn feature aggregation with \textit{instance-level} feature guidance. Extensive experimental results show that our approach can lead to 2.3 mAP and 7.0 mAP improvements on the KITTI and nuScenes datasets, respectively. Notably, our best model reaches 70.9 NDS on the nuScenes testing leaderboard, achieving competitive performance among various state-of-the-arts.",AutoAlign：用于多模式3D对象检测的Pixel-Instance特征聚合。,通过RGB图像或LIDAR点云通过自主驾驶进行了广泛探索的对象检测。但是，使这两个数据源互补并且彼此有益仍然是一项挑战。在本文中，我们提出\ textit {autoAlign}，这是3D对象检测的自动功能融合策略。我们没有建立与摄像机投影矩阵的确定性对应关系，而是用可学习的对齐图对图像和点云之间的映射关系进行建模。该地图使我们的模型能够以动态和数据驱动的方式自动化非殖民特征的对齐。具体而言，设计了一个跨意义特征对齐模块，以适应每个体素的{Pixel-level}图像特征。为了增强功能对齐过程中的语义一致性，我们还设计了一个自我监视的跨模式功能交互模块，模型可以通过\ textit {instance-level}特征指导来学习特征聚合。广泛的实验结果表明，我们的方法可以分别在Kitti和Nuscenes数据集上进行2.3 MAP和7.0 MAP改进。值得注意的是，我们的最佳模型在Nuscenes测试排行榜上达到了70.9 NDS，在各种最先进的情况下达到了竞争性能。,https://arxiv.org/abs/2201.06493,IJCAI,True,False,False,False
1636,MNet: Rethinking 2D/3D Networks for Anisotropic Medical Image Segmentation.,"Zhangfu Dong, Yuting He, Xiaoming Qi, Yang Chen, Huazhong Shu, Jean-Louis Coatrieux, Guanyu Yang, Shuo Li","The nature of thick-slice scanning causes severe inter-slice discontinuities of 3D medical images, and the vanilla 2D/3D convolutional neural networks (CNNs) fail to represent sparse inter-slice information and dense intra-slice information in a balanced way, leading to severe underfitting to inter-slice features (for vanilla 2D CNNs) and overfitting to noise from long-range slices (for vanilla 3D CNNs). In this work, a novel mesh network (MNet) is proposed to balance the spatial representation inter axes via learning. 1) Our MNet latently fuses plenty of representation processes by embedding multi-dimensional convolutions deeply into basic modules, making the selections of representation processes flexible, thus balancing representation for sparse inter-slice information and dense intra-slice information adaptively. 2) Our MNet latently fuses multi-dimensional features inside each basic module, simultaneously taking the advantages of 2D (high segmentation accuracy of the easily recognized regions in 2D view) and 3D (high smoothness of 3D organ contour) representations, thus obtaining more accurate modeling for target regions. Comprehensive experiments are performed on four public datasets (CT\&MR), the results consistently demonstrate the proposed MNet outperforms the other methods. The code and datasets are available at: https://github.com/zfdong-code/MNet",MNET：重新思考各向异性医学图像分割的2D/3D网络。,厚板扫描的性质导致3D医学图像的严重片间不连续性，而香草2D/3D卷积神经网络（CNN）未能代表稀疏的切片间信息，并以平衡的方式以平衡的方式来代表稀疏的切片间信息和密集的内部内部信息严重降低了固定板间特征（用于香草2D CNN），并过度适合远程切片（用于香草3D CNN）的噪声。在这项工作中，提出了一个新颖的网络网络（MNET），以通过学习来平衡空间表示。1）我们的MNET通过将多维卷积嵌入深度融合到基本模块中，从而融合了大量表示过程，从而使表示过程的选择灵活，从而平衡了稀疏片间信息和密集的内部内部内部内部信息的表示。2）我们的MNET在每个基本模块内部融合了多维特征，同时获得了2D的优势（2D视图中易于识别区域的高分割精度）和3D（3D器官轮廓的高光滑度），从而获得了更准确的目标区域的建模。全面的实验是在四个公共数据集（CT \＆MR）上进行的，结果始终证明所提出的MNET优于其他方法。代码和数据集可在以下网址找到：https：//github.com/zfdong-code/mnet,https://arxiv.org/abs/2205.04846,IJCAI,True,False,False,False
1637,Membership Inference via Backdooring.,"Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang","Recently issued data privacy regulations like GDPR (General Data Protection Regulation) grant individuals the right to be forgotten. In the context of machine learning, this requires a model to forget about a training data sample if requested by the data owner (i.e., machine unlearning). As an essential step prior to machine unlearning, it is still a challenge for a data owner to tell whether or not her data have been used by an unauthorized party to train a machine learning model. Membership inference is a recently emerging technique to identify whether a data sample was used to train a target model, and seems to be a promising solution to this challenge. However, straightforward adoption of existing membership inference approaches fails to address the challenge effectively due to being originally designed for attacking membership privacy and suffering from several severe limitations such as low inference accuracy on well-generalized models. In this paper, we propose a novel membership inference approach inspired by the backdoor technology to address the said challenge. Specifically, our approach of Membership Inference via Backdooring (MIB) leverages the key observation that a backdoored model behaves very differently from a clean model when predicting on deliberately marked samples created by a data owner. Appealingly, MIB requires data owners' marking a small number of samples for membership inference and only black-box access to the target model, with theoretical guarantees for inference results. We perform extensive experiments on various datasets and deep neural network architectures, and the results validate the efficacy of our approach, e.g., marking only 0.1% of the training dataset is practically sufficient for effective membership inference.",会员推论通过后门。,最近发布的数据隐私法规（例如GDPR（通用数据保护法规））授予个人被遗忘的权利。在机器学习的背景下，如果数据所有者的要求（即机器学习），这需要模型忘记培训数据样本。作为在机器学习之前的重要步骤，数据所有者要判断未经授权的一方是否已经使用她的数据来训练机器学习模型仍然是一个挑战。会员推理是一种最近新兴的技术，可以确定是否使用数据样本来训练目标模型，并且似乎是应对这一挑战的有前途的解决方案。但是，由于最初是为攻击会员隐私而设计的，因此直接采用现有的会员推理方法无法有效应对挑战，并受到了一些严重的限制，例如对良好的模型的推理准确性低。在本文中，我们提出了一种新型的会员推理方法，灵感来自后门技术，以应对上述挑战。具体而言，我们通过后卫（MIB）的会员推理方法利用了一个关键观察，即在预测数据所有者创建的故意标记样本时，背式模型的行为与干净模型的行为非常不同。吸引人的是，MIB需要数据所有者标记少数用于成员推理的样本，并且仅对目标模型进行黑框访问，并具有理论保证的推理结果。我们在各种数据集和深层神经网络体系结构上进行了广泛的实验，结果验证了方法的功效，例如，仅标记0.1％的培训数据集几乎足以进行有效的会员推理。,https://arxiv.org/abs/2206.04823,IJCAI,True,False,False,False
1638,Absolute Wrong Makes Better: Boosting Weakly Supervised Object Detection via Negative Deterministic Information.,"Guanchun Wang, Xiangrong Zhang, Zelin Peng, Xu Tang, Huiyu Zhou, Licheng Jiao","Weakly supervised object detection (WSOD) is a challenging task, in which image-level labels (e.g., categories of the instances in the whole image) are used to train an object detector. Many existing methods follow the standard multiple instance learning (MIL) paradigm and have achieved promising performance. However, the lack of deterministic information leads to part domination and missing instances. To address these issues, this paper focuses on identifying and fully exploiting the deterministic information in WSOD. We discover that negative instances (i.e. absolutely wrong instances), ignored in most of the previous studies, normally contain valuable deterministic information. Based on this observation, we here propose a negative deterministic information (NDI) based method for improving WSOD, namely NDI-WSOD. Specifically, our method consists of two stages: NDI collecting and exploiting. In the collecting stage, we design several processes to identify and distill the NDI from negative instances online. In the exploiting stage, we utilize the extracted NDI to construct a novel negative contrastive learning mechanism and a negative guided instance selection strategy for dealing with the issues of part domination and missing instances, respectively. Experimental results on several public benchmarks including VOC 2007, VOC 2012 and MS COCO show that our method achieves satisfactory performance.",绝对错误使得更好：通过负面的确定性信息来增强弱监督的对象检测。,弱监督的对象检测（WSOD）是一项具有挑战性的任务，其中图像级标签（例如，整个图像中的实例类别）用于训练对象检测器。许多现有方法遵循标准的多个实例学习（MIL）范式，并实现了有希望的性能。但是，缺乏确定性信息会导致部分统治和缺失实例。为了解决这些问题，本文着重于识别和充分利用WSOD中的确定性信息。我们发现，在以前的大多数研究中都忽略了负面实例（即绝对错误的实例），通常包含有价值的确定性信息。基于此观察结果，我们在这里提出了一种基于负面的确定性信息（NDI）改进WSOD的方法，即NDI-WSOD。具体而言，我们的方法包括两个阶段：NDI收集和利用。在收集阶段，我们设计了几个过程，以识别和提炼NDI从在线的负面实例中。在利用阶段，我们利用提取的NDI来构建一种新型的负相关学习机制和负面的指导实例选择策略，分别处理部分统治和缺失实例。包括VOC 2007，VOC 2012和Coco女士在内的几个公共基准测试的实验结果表明，我们的方法可实现令人满意的性能。,https://arxiv.org/abs/2204.10068,IJCAI,True,False,False,False
1639,Parameter-Efficient Sparsity for Large Language Models Fine-Tuning.,"Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang, Songfang Huang, Shen Li, Junjie Bai","With the dramatically increased number of parameters in language models, sparsity methods have received ever-increasing research focus to compress and accelerate the models. While most research focuses on how to accurately retain appropriate weights while maintaining the performance of the compressed model, there are challenges in the computational overhead and memory footprint of sparse training when compressing large-scale language models. To address this problem, we propose a Parameter-efficient Sparse Training (PST) method to reduce the number of trainable parameters during sparse-aware training in downstream tasks. Specifically, we first combine the data-free and data-driven criteria to efficiently and accurately measure the importance of weights. Then we investigate the intrinsic redundancy of data-driven weight importance and derive two obvious characteristics i.e., low-rankness and structuredness. Based on that, two groups of small matrices are introduced to compute the data-driven importance of weights, instead of using the original large importance score matrix, which therefore makes the sparse training resource-efficient and parameter-efficient. Experiments with diverse networks (i.e., BERT, RoBERTa and GPT-2) on dozens of datasets demonstrate PST performs on par or better than previous sparsity methods, despite only training a small number of parameters. For instance, compared with previous sparsity methods, our PST only requires 1.5% trainable parameters to achieve comparable performance on BERT.",大语言模型微调的参数有效稀疏性。,随着语言模型中参数数量的急剧增加，稀疏方法已经获得了越来越多的研究重点，以压缩和加速模型。尽管大多数研究都集中于如何在保持压缩模型的性能的同时准确保留适当的权重，但在压缩大规模语言模型时，计算开销和记忆足迹的挑战。为了解决这个问题，我们提出了一种参数有效的稀疏训练（PST）方法，以减少下游任务中稀疏感知训练期间可训练参数的数量。具体而言，我们首先将无数据和数据驱动的标准组合在一起，以有效，准确地测量权重的重要性。然后，我们研究了数据驱动的权重重要性的内在冗余，并得出了两个明显的特征，即低级别和结构性。基于此，引入了两组小矩阵来计算权重的数据驱动的重要性，而不是使用原始的大重要得分矩阵，因此使稀疏的训练资源效率和参数效率。在数十个数据集上使用不同网络（即Bert，Roberta和GPT-2）进行的实验表明，尽管仅训练少量参数，但PST在PAR或PAR上的表现比以前的稀疏方法更好。例如，与以前的稀疏方法相比，我们的PST仅需要1.5％的可训练参数即可在BERT上实现可比的性能。,https://arxiv.org/abs/2205.11005,IJCAI,True,False,True,False
1640,Mixed Strategies for Security Games with General Defending Requirements.,"Rufan Bai, Haoxing Lin, Xinyu Yang, Xiaowei Wu, Minming Li, Weijia Jia","The Stackelberg security game is played between a defender and an attacker, where the defender needs to allocate a limited amount of resources to multiple targets in order to minimize the loss due to adversarial attack by the attacker. While allowing targets to have different values, classic settings often assume uniform requirements to defend the targets. This enables existing results that study mixed strategies (randomized allocation algorithms) to adopt a compact representation of the mixed strategies.   In this work, we initiate the study of mixed strategies for the security games in which the targets can have different defending requirements. In contrast to the case of uniform defending requirement, for which an optimal mixed strategy can be computed efficiently, we show that computing the optimal mixed strategy is NP-hard for the general defending requirements setting. However, we show that strong upper and lower bounds for the optimal mixed strategy defending result can be derived. We propose an efficient close-to-optimal Patching algorithm that computes mixed strategies that use only few pure strategies. We also study the setting when the game is played on a network and resource sharing is enabled between neighboring targets. Our experimental results demonstrate the effectiveness of our algorithm in several large real-world datasets.",具有一般防御要求的安全游戏的混合策略。,Stackelberg安全游戏是在防守者和攻击者之间进行的，后卫需要将有限数量的资源分配给多个目标，以最大程度地减少攻击者的对抗性攻击而导致的损失。在允许目标具有不同的值的同时，经典设置通常会假设统一要求捍卫目标。这使现有的结果可以研究混合策略（随机分配算法），可以采用混合策略的紧凑表示。在这项工作中，我们启动了针对安全游戏的混合策略的研究，其中目标可以具有不同的防守要求。与统一的防御要求相反，可以有效地计算出最佳的混合策略，我们表明，计算最佳混合策略对于一般的防御要求设置是NP-HARD。但是，我们表明，可以得出最佳混合策略防御结果的强上限和下限。我们提出了一种有效的近节修补算法，该算法计算仅使用少数纯策略的混合策略。我们还研究了游戏在网络上玩游戏时的设置，并且在相邻目标之间启用了资源共享。我们的实验结果证明了我们算法在几个大型现实数据集中的有效性。,https://arxiv.org/abs/2204.12158,IJCAI,True,False,False,False
1641,Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement.,"Bing Li, Jiaxin Chen, Dongming Zhang, Xiuguo Bao, Di Huang","Compressed video action recognition has recently drawn growing attention, since it remarkably reduces the storage and computational cost via replacing raw videos by sparsely sampled RGB frames and compressed motion cues (e.g., motion vectors and residuals). However, this task severely suffers from the coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB and motion modalities. To address the two issues above, this paper proposes a novel framework, namely Attentive Cross-modal Interaction Network with Motion Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for the RGB modality and the other for the motion modality. Particularly, the motion stream employs a multi-scale block embedded with a denoising module to enhance representation learning. The interaction between the two streams is then strengthened by introducing the Selective Motion Complement (SMC) and Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality with spatio-temporally attentive local motion features and CMA further combines the two modalities with selective feature augmentation. Extensive experiments on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the effectiveness and efficiency of MEACI-Net.",通过与运动增强的细心跨模式相互作用进行压缩视频动作识别的表示。,压缩视频动作识别最近引起了人们的关注，因为它通过用稀疏采样的RGB帧和压缩运动提示（例如运动向量和残差）替换原始视频，从而大大降低了存储和计算成本。但是，这项任务严重遭受了粗糙和嘈杂的动力学以及异质RGB和运动方式的融合不足。为了解决上面的两个问题，本文提出了一个新颖的框架，即具有运动增强的细心跨模式相互作用网络（MEACI-NET）。它遵循两流体系结构，即一个用于RGB模式，另一个用于运动模态。特别是，该运动流采用带有denoising模块的多尺度块来增强表示表示。然后，通过引入选择性运动补充（SMC）和跨模式增强（CMA）模块来加强两条流之间的相互作用，其中SMC与时空上的局部局部运动相互补充，CMA和CMA进一步将两种模态与两种模态相结合。选择性功能增强。对UCF-101，HMDB-51和Kinetics-400基准的广泛实验证明了MEACI-NET的有效性和效率。,https://arxiv.org/abs/2205.03569,IJCAI,True,False,False,False
1642,CERT: Continual Pre-training on Sketches for Library-oriented Code Generation.,"Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, Jian-Guang Lou","Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large-scale unlabelled code corpora and perform well in code generation. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present CERT with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and the generator are continually pre-trained upon a base model using unlabelled data. Furthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate library-oriented code generation. Experimental results demonstrate the impressive performance of CERT. For example, it surpasses the base model by an absolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is available at https://github.com/microsoft/PyCodeGPT.",translate error!,代码生成是一个长期的挑战，旨在根据自然语言描述生成代码段。通常，昂贵的文本编码配对数据对于培训代码生成模型至关重要。最近，由于培训预培训技术的成功，大型语言模型接受了大规模未标记的代码语料库的培训，并在代码生成方面表现良好。在本文中，我们调查了如何利用未标记的代码语料库来训练以图书馆为导向的代码生成的模型。由于对于程序员重复使用第三方库是一种普遍的做法，因此由于库数量大量，文本编码配对数据很难获得。我们观察到面向图书馆的代码片段更有可能共享类似的代码草图。因此，我们为证书提供了两个步骤：草图器生成草图，然后发电机填充了草图中的详细信息。Sketcher和Generator都使用未标记的数据在基本模型上不断预先训练。此外，我们制作了两个名为Pandaseval和NumpyeVal的基准，以评估面向图书馆的代码生成。实验结果证明了CERT的表现令人印象深刻。例如，它超过了基本模型，在pandaseval上的Pass@1方面，绝对提高了15.67％。我们的工作可在https://github.com/microsoft/pycodegpt上获得。,https://arxiv.org/abs/2206.06888,IJCAI,True,False,False,False
1643,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering.,"Zhenyun Deng, Yonghua Zhu, Yang Chen, Michael Witbrock, Patricia Riddle","Effective multi-hop question answering (QA) requires reasoning over multiple scattered paragraphs and providing explanations for answers. Most existing approaches cannot provide an interpretable reasoning process to illustrate how these models arrive at an answer. In this paper, we propose a Question Decomposition method based on Abstract Meaning Representation (QDAMR) for multi-hop QA, which achieves interpretable reasoning by decomposing a multi-hop question into simpler sub-questions and answering them in order. Since annotating the decomposition is expensive, we first delegate the complexity of understanding the multi-hop question to an AMR parser. We then achieve the decomposition of a multi-hop question via segmentation of the corresponding AMR graph based on the required reasoning type. Finally, we generate sub-questions using an AMR-to-Text generation model and answer them with an off-the-shelf QA model. Experimental results on HotpotQA demonstrate that our approach is competitive for interpretable reasoning and that the sub-questions generated by QDAMR are well-formed, outperforming existing question-decomposition-based multi-hop QA approaches.",可解释的基于AMR的问题分解用于多跳问题回答。,有效的多跳问答（QA）需要在多个分散的段落上进行推理，并提供答案的解释。大多数现有方法无法提供可解释的推理过程，以说明这些模型如何得出答案。在本文中，我们提出了一种基于多跳QA的抽象含义表示形式（QDAMR）的问题分解方法，该方法通过将多跳问题分解为更简单的子问题并按顺序回答它们来实现可解释的推理。由于注释分解很昂贵，因此我们首先将理解多跳问题的复杂性委托给AMR解析器。然后，我们通过基于所需的推理类型对相应的AMR图进行分割实现多跳问题的分解。最后，我们使用AMR到文本生成模型生成子问题，并使用现成的QA模型回答它们。HOTPOTQA的实验结果表明，我们的方法在可解释的推理方面具有竞争力，并且QDAMR产生的子问题是良好的，表现优于现有的基于问题分解的多跳质量质量检查方法。,https://arxiv.org/abs/2206.08486,IJCAI,True,False,False,False
1644,Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration.,"Di Wang, Jinyuan Liu, Xin Fan, Risheng Liu","Recent learning-based image fusion methods have marked numerous progress in pre-registered multi-modality data, but suffered serious ghosts dealing with misaligned multi-modality data, due to the spatial deformation and the difficulty narrowing cross-modality discrepancy. To overcome the obstacles, in this paper, we present a robust cross-modality generation-registration paradigm for unsupervised misaligned infrared and visible image fusion (IVIF). Specifically, we propose a Cross-modality Perceptual Style Transfer Network (CPSTN) to generate a pseudo infrared image taking a visible image as input. Benefiting from the favorable geometry preservation ability of the CPSTN, the generated pseudo infrared image embraces a sharp structure, which is more conducive to transforming cross-modality image alignment into mono-modality registration coupled with the structure-sensitive of the infrared image. In this case, we introduce a Multi-level Refinement Registration Network (MRRN) to predict the displacement vector field between distorted and pseudo infrared images and reconstruct registered infrared image under the mono-modality setting. Moreover, to better fuse the registered infrared images and visible images, we present a feature Interaction Fusion Module (IFM) to adaptively select more meaningful features for fusion in the Dual-path Interaction Fusion Network (DIFN). Extensive experimental results suggest that the proposed method performs superior capability on misaligned cross-modality image fusion.",通过跨模式的产生和注册，无监督的未对准红外线和可见的图像融合。,最近基于学习的图像融合方法标志着预注册的多模式数据的许多进展，但由于空间变形和难以狭窄的交叉模式差异，遭受了严重的幽灵，涉及涉及未对准多模式数据的严重幽灵。为了克服障碍，在本文中，我们提出了一个强大的交叉模式生成注册范式，用于无监督的未对准的红外和可见图像融合（IVIF）。具体而言，我们提出了一个交叉模式感知样式转移网络（CPSTN），以生成以可见图像为输入的伪红外图像。受益于CPSTN的有利几何保存能力，生成的伪红外图像具有锋利的结构，这更有利于将交叉模式图像对准变成单模式登记，并与红外图像的结构敏感。在这种情况下，我们引入了一个多级改进登记网络（MRRN），以预测扭曲和伪红外图像和重建单模式设置下注册的红外图像之间的位移矢量场。此外，为了更好地融合注册的红外图像和可见图像，我们提出了一个特征交互融合模块（IFM），以适应在双路线相互作用融合网络（DIFN）中自适应选择更有意义的特征。广泛的实验结果表明，所提出的方法对未对准的交叉模式图像融合表现出色。,https://arxiv.org/abs/2205.11876,IJCAI,True,False,False,False
1645,Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering.,"Min Peng, Chongyang Wang, Yuan Gao, Yu Shi, Xiang-Dong Zhou","Video question answering (VideoQA) is challenging given its multimodal combination of visual understanding and natural language processing. While most existing approaches ignore the visual appearance-motion information at different temporal scales, it is unknown how to incorporate the multilevel processing capacity of a deep learning model with such multiscale information. Targeting these issues, this paper proposes a novel Multilevel Hierarchical Network (MHN) with multiscale sampling for VideoQA. MHN comprises two modules, namely Recurrent Multimodal Interaction (RMI) and Parallel Visual Reasoning (PVR). With a multiscale sampling, RMI iterates the interaction of appearance-motion information at each scale and the question embeddings to build the multilevel question-guided visual representations. Thereon, with a shared transformer encoder, PVR infers the visual cues at each level in parallel to fit with answering different question types that may rely on the visual information at relevant levels. Through extensive experiments on three VideoQA datasets, we demonstrate improved performances than previous state-of-the-arts and justify the effectiveness of each part of our method.",多级分层网络，具有多尺度抽样，用于视频问答。,鉴于其视觉理解和自然语言处理的多模式组合，视频问题回答（VideoQA）具有挑战性。尽管大多数现有方法忽略了不同时间尺度上的视觉外观运动信息，但未知如何将深度学习模型的多级处理能力与此类多尺度信息结合在一起。针对这些问题，本文提出了一个新型的多级分层网络（MHN），并为VideoQA进行多尺度采样。MHN包括两个模块，即复发多模式相互作用（RMI）和平行视觉推理（PVR）。通过多尺度采样，RMI迭代了每个量表上的外观运动信息的相互作用以及构建多级问题引导的视觉表示的问题。因此，使用共享的变压器编码器，PVR并行地渗透到每个级别的视觉提示，以符合回答可能依赖相关级别的视觉信息的不同问题类型。通过在三个VideoQA数据集上进行的大量实验，我们证明了比以前的最先进的表现改进的性能，并证明了我们方法的每个部分的有效性。,https://arxiv.org/abs/2205.04061,IJCAI,True,False,False,False
1646,Manipulating Elections by Changing Voter Perceptions.,"Junlin Wu, Andrew Estornell, Lecheng Kong, Yevgeniy Vorobeychik","The integrity of elections is central to democratic systems. However, a myriad of malicious actors aspire to influence election outcomes for financial or political benefit. A common means to such ends is by manipulating perceptions of the voting public about select candidates, for example, through misinformation. We present a formal model of the impact of perception manipulation on election outcomes in the framework of spatial voting theory, in which the preferences of voters over candidates are generated based on their relative distance in the space of issues. We show that controlling elections in this model is, in general, NP-hard, whether issues are binary or real-valued. However, we demonstrate that critical to intractability is the diversity of opinions on issues exhibited by the voting public. When voter views lack diversity, and we can instead group them into a small number of categories -- for example, as a result of political polarization -- the election control problem can be solved in polynomial time in the number of issues and candidates for arbitrary scoring rules.",通过改变选民的看法来操纵选举。,选举的完整性对于民主制度至关重要。但是，无数的恶意行为者渴望影响选举成果以获得财务或政治利益。这种目的的一种常见手段是通过操纵对投票公众对某些候选人的看法，例如通过错误信息。我们提出了一种正式的模型，即在空间投票理论框架内，感知操纵对选举结果的影响，其中，选民对候选人的偏好是根据他们在问题空间中的相对距离产生的。我们表明，无论问题是二进制还是真实价值，该模型中的控制选举通常都是NP-HARD。但是，我们证明，至关重要的是，至关重要的是对投票公众展示的问题的多样性。评分规则。,https://arxiv.org/abs/2205.00102,IJCAI,True,False,False,False
1647,Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition.,"Shuhui Wu, Yongliang Shen, Zeqi Tan, Weiming Lu","Nested named entity recognition (nested NER) is a fundamental task in natural language processing. Various span-based methods have been proposed to detect nested entities with span representations. However, span-based methods do not consider the relationship between a span and other entities or phrases, which is helpful in the NER task. Besides, span-based methods have trouble predicting long entities due to limited span enumeration length. To mitigate these issues, we present the Propose-and-Refine Network (PnRNet), a two-stage set prediction network for nested NER. In the propose stage, we use a span-based predictor to generate some coarse entity predictions as entity proposals. In the refine stage, proposals interact with each other, and richer contextual information is incorporated into the proposal representations. The refined proposal representations are used to re-predict entity boundaries and classes. In this way, errors in coarse proposals can be eliminated, and the boundary prediction is no longer constrained by the span enumeration length limitation. Additionally, we build multi-scale sentence representations, which better model the hierarchical structure of sentences and provide richer contextual information than token-level representations. Experiments show that PnRNet achieves state-of-the-art performance on four nested NER datasets and one flat NER dataset.",提出反复：嵌套命名实体识别的两阶段预测网络。,嵌套命名实体识别（Nested Ner）是自然语言处理中的基本任务。已经提出了各种基于跨度的方法来检测具有跨度表示的嵌套实体。但是，基于跨度的方法不考虑跨度与其他实体或短语之间的关系，这对NER任务很有帮助。此外，由于跨度枚举长度有限，基于跨度的方法在预测长实体方面难以预测。为了减轻这些问题，我们介绍了提出的和refine网络（PNRNET），这是一个嵌套NER的两阶段集预测网络。在建议阶段，我们使用基于跨度的预测指标来生成一些粗糙的实体预测作为实体建议。在精炼阶段，建议相互互动，并将更丰富的上下文信息纳入建议表示。精致的建议表示形式用于重新预测实体边界和类。这样，可以消除粗略建议中的错误，并且边界预测不再受到跨度枚举长度限制的约束。此外，我们构建了多尺度句子表示，它可以更好地对句子的层次结构进行建模，并提供比令牌级表示更丰富的上下文信息。实验表明，PNRNET在四个嵌套的NER数据集和一个Flat NER数据集上实现了最先进的性能。,https://arxiv.org/abs/2204.12732,IJCAI,True,False,False,False
1648,PAnDR: Fast Adaptation to New Environments from Offline Experiences via Decoupling Policy and Environment Representations.,"Tong Sang, Hongyao Tang, Yi Ma, Jianye Hao, Yan Zheng, Zhaopeng Meng, Boyan Li, Zhen Wang","Deep Reinforcement Learning (DRL) has been a promising solution to many complex decision-making problems. Nevertheless, the notorious weakness in generalization among environments prevent widespread application of DRL agents in real-world scenarios. Although advances have been made recently, most prior works assume sufficient online interaction on training environments, which can be costly in practical cases. To this end, we focus on an \textit{offline-training-online-adaptation} setting, in which the agent first learns from offline experiences collected in environments with different dynamics and then performs online policy adaptation in environments with new dynamics. In this paper, we propose Policy Adaptation with Decoupled Representations (PAnDR) for fast policy adaptation. In offline training phase, the environment representation and policy representation are learned through contrastive learning and policy recovery, respectively. The representations are further refined by mutual information optimization to make them more decoupled and complete. With learned representations, a Policy-Dynamics Value Function (PDVF) (Raileanu et al., 2020) network is trained to approximate the values for different combinations of policies and environments. In online adaptation phase, with the environment context inferred from few experiences collected in new environments, the policy is optimized by gradient ascent with respect to the PDVF. Our experiments show that PAnDR outperforms existing algorithms in several representative policy adaptation problems.",PANDR：通过解耦政策和环境表示，快速适应了离线体验的新环境。,深入的强化学习（DRL）一直是解决许多复杂决策问题的有前途的解决方案。然而，环境之间臭名昭著的弱点阻止了DRL代理在现实世界中的广泛应用。尽管最近取得了进步，但大多数先前的作品都在培训环境上采用足够的在线互动，在实际情况下，这可能是昂贵的。为此，我们专注于\ textIt {离线训练 - 在线适应}设置，其中代理首先从具有不同动态的环境中收集的离线体验中学习，然后在具有新动力学的环境中执行在线策略适应。在本文中，我们提出了使用脱钩表示（PANDR）进行快速政策适应的政策适应。在离线培训阶段，环境代表和政策表示分别是通过对比度学习和政策恢复来学习的。通过相互信息优化进一步完善表示形式，以使它们更加脱钩和完整。通过学习的表示，对策略 - 动力学价值函数（PDVF）（Reareanu等，2020）网络进行了训练，以近似政策和环境的不同组合的值。在在线适应阶段，通过在新环境中收集的几乎没有经验推断出的环境环境，该策略通过相对于PDVF的梯度上升来优化。我们的实验表明，在几种代表性的政策适应问题中，PANDR优于现有算法。,https://arxiv.org/abs/2204.02877,IJCAI,True,False,False,False
1649,Exploring the Benefits of Teams in Multiagent Learning.,"David Radke, Kate Larson, Tim Brecht","For problems requiring cooperation, many multiagent systems implement solutions among either individual agents or across an entire population towards a common goal. Multiagent teams are primarily studied when in conflict; however, organizational psychology (OP) highlights the benefits of teams among human populations for learning how to coordinate and cooperate. In this paper, we propose a new model of multiagent teams for reinforcement learning (RL) agents inspired by OP and early work on teams in artificial intelligence. We validate our model using complex social dilemmas that are popular in recent multiagent RL and find that agents divided into teams develop cooperative pro-social policies despite incentives to not cooperate. Furthermore, agents are better able to coordinate and learn emergent roles within their teams and achieve higher rewards compared to when the interests of all agents are aligned.",探索团队在多种学习中的好处。,对于需要合作的问题，许多多基因系统在各个代理商之间或整个人群之间实施解决方案，以实现一个共同的目标。冲突时，主要研究了多方面的团队。但是，组织心理学（OP）强调了人群中团队的好处，以学习如何协调和合作。在本文中，我们提出了一个新的多项式团队的增强学习团队（RL）代理模型，该模型受人工智能团队的启发和早期工作的启发。我们使用复杂的社会困境来验证我们的模型，这些困境在最近的多种RL中很受欢迎，发现尽管激励不合作，但分为团队制定合作的亲社会政策。此外，与所有代理人的利益相比，代理人能够更好地协调和学习新兴的角色并获得更高的奖励。,https://arxiv.org/abs/2205.02328,IJCAI,True,False,False,False
1650,Boundary-Guided Camouflaged Object Detection.,"Yujia Sun, Shuo Wang, Chenglizhao Chen, Tian-Zhu Xiang","Camouflaged object detection (COD), segmenting objects that are elegantly blended into their surroundings, is a valuable yet challenging task. Existing deep-learning methods often fall into the difficulty of accurately identifying the camouflaged object with complete and fine object structure. To this end, in this paper, we propose a novel boundary-guided network (BGNet) for camouflaged object detection. Our method explores valuable and extra object-related edge semantics to guide representation learning of COD, which forces the model to generate features that highlight object structure, thereby promoting camouflaged object detection of accurate boundary localization. Extensive experiments on three challenging benchmark datasets demonstrate that our BGNet significantly outperforms the existing 18 state-of-the-art methods under four widely-used evaluation metrics. Our code is publicly available at: https://github.com/thograce/BGNet.",边界引导的伪装对象检测。,伪装的对象检测（COD），将其优雅地融合到周围环境中的对象是一项有价值但充满挑战的任务。现有的深度学习方法通常陷入具有完整和精细的对象结构准确识别伪装对象的困难。为此，在本文中，我们提出了一个新颖的边界引导网络（BGNET），以用于伪装对象检测。我们的方法探索了有价值的和额外的对象相关的边缘语义，以指导COD的表示形式，这迫使模型生成突出对象结构的特征，从而促进了精确边界定位的伪装对象检测。在三个具有挑战性的基准数据集上进行的广泛实验表明，我们的BGNET在四个广泛使用的评估指标下显着优于现有的18种最新方法。我们的代码可在以下网址公开获取：https：//github.com/thograce/bgnet。,https://arxiv.org/abs/2207.00794,IJCAI,True,False,False,False
1651,Boosting Multi-Label Image Classification with Complementary Parallel Self-Distillation.,"Jiazhi Xu, Sheng Huang, Fengtao Zhou, Luwen Huangfu, Daniel Zeng, Bo Liu","Multi-Label Image Classification (MLIC) approaches usually exploit label correlations to achieve good performance. However, emphasizing correlation like co-occurrence may overlook discriminative features of the target itself and lead to model overfitting, thus undermining the performance. In this study, we propose a generic framework named Parallel Self-Distillation (PSD) for boosting MLIC models. PSD decomposes the original MLIC task into several simpler MLIC sub-tasks via two elaborated complementary task decomposition strategies named Co-occurrence Graph Partition (CGP) and Dis-occurrence Graph Partition (DGP). Then, the MLIC models of fewer categories are trained with these sub-tasks in parallel for respectively learning the joint patterns and the category-specific patterns of labels. Finally, knowledge distillation is leveraged to learn a compact global ensemble of full categories with these learned patterns for reconciling the label correlation exploitation and model overfitting. Extensive results on MS-COCO and NUS-WIDE datasets demonstrate that our framework can be easily plugged into many MLIC approaches and improve performances of recent state-of-the-art approaches. The explainable visual study also further validates that our method is able to learn both the category-specific and co-occurring features. The source code is released at https://github.com/Robbie-Xu/CPSD.",通过互补的平行自我鉴定来增强多标签图像分类。,多标签图像分类（MIC）方法通常利用标签相关性以实现良好的性能。但是，强调相关性等相关性可能会忽略目标本身的判别特征，并导致模型过度拟合，从而破坏了性能。在这项研究中，我们提出了一个通用框架，用于增强MIC模型的名为平行自我验证（PSD）。PSD通过两个详细的互补任务分解策略将原始MLIC任务分解为几个更简单的MIC子任务，称为共发生图形分区（CGP）和Dis-Eccurrence Graph Graph分区（DGP）。然后，较少类别的MLIC模型并行训练这些子任务，以分别学习关节模式和标签类别模式。最后，利用知识蒸馏来学习完整类别的紧凑全球合奏，并通过这些学习的模式来调和标签相关性利用和模型过度拟合。MS-Coco和NUS范围内数据集的广泛结果表明，我们的框架可以轻松地插入许多MICAD方法中，并改善最近最新方法的性能。可解释的视觉研究还进一步验证了我们的方法能够学习特定类别和共同出现的特征。源代码在https://github.com/robbie-xu/cpsd上发布。,https://arxiv.org/abs/2205.10986,IJCAI,True,False,False,False
1652,Trading Hard Negatives and True Negatives: A Debiased Contrastive Collaborative Filtering Approach.,"Chenxiao Yang, Qitian Wu, Jipeng Jin, Xiaofeng Gao, Junwei Pan, Guihai Chen","Collaborative filtering (CF), as a standard method for recommendation with implicit feedback, tackles a semi-supervised learning problem where most interaction data are unobserved. Such a nature makes existing approaches highly rely on mining negatives for providing correct training signals. However, mining proper negatives is not a free lunch, encountering with a tricky trade-off between mining informative hard negatives and avoiding false ones. We devise a new approach named as Hardness-Aware Debiased Contrastive Collaborative Filtering (HDCCF) to resolve the dilemma. It could sufficiently explore hard negatives from two-fold aspects: 1) adaptively sharpening the gradients of harder instances through a set-wise objective, and 2) implicitly leveraging item/user frequency information with a new sampling strategy. To circumvent false negatives, we develop a principled approach to improve the reliability of negative instances and prove that the objective is an unbiased estimation of sampling from the true negative distribution. Extensive experiments demonstrate the superiority of the proposed model over existing CF models and hard negative mining methods.",交易艰苦的负面因素和真实负面因素：一种偏见的对比协作过滤方法。,协作过滤（CF）作为一种标准的建议，并通过隐式反馈来解决半监督的学习问题，其中大多数交互数据都无法观察到。这种性质使现有方法高度依赖于采矿负面因素来提供正确的培训信号。但是，采矿适当的负面因素不是免费的午餐，在挖掘内容丰富的艰苦负面底片和避免虚假的底片之间遇到了棘手的权衡。我们设计了一种新的方法，称为“硬度意识”的对比协作过滤（HDCCF）来解决困境。它可以从两个方面充分探索艰难的负面因素：1）通过设定目标自适应地锐化了更艰难实例的梯度，以及2）使用新的采样策略隐含地利用项目/用户频率信息。为了避免虚假负面因素，我们开发了一种有原则的方法来提高负面实例的可靠性，并证明该目标是对真正负面分布采样的无偏估计。广泛的实验证明了所提出的模型比现有CF模型和硬采矿方法的优越性。,https://arxiv.org/abs/2204.11752,IJCAI,True,False,False,False
1653,Anti-Forgery: Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations.,"Run Wang, Ziheng Huang, Zhikai Chen, Li Liu, Jing Chen, Lina Wang","DeepFake is becoming a real risk to society and brings potential threats to both individual privacy and political security due to the DeepFaked multimedia are realistic and convincing. However, the popular DeepFake passive detection is an ex-post forensics countermeasure and failed in blocking the disinformation spreading in advance. To address this limitation, researchers study the proactive defense techniques by adding adversarial noises into the source data to disrupt the DeepFake manipulation. However, the existing studies on proactive DeepFake defense via injecting adversarial noises are not robust, which could be easily bypassed by employing simple image reconstruction revealed in a recent study MagDR.   In this paper, we investigate the vulnerability of the existing forgery techniques and propose a novel \emph{anti-forgery} technique that helps users protect the shared facial images from attackers who are capable of applying the popular forgery techniques. Our proposed method generates perceptual-aware perturbations in an incessant manner which is vastly different from the prior studies by adding adversarial noises that is sparse. Experimental results reveal that our perceptual-aware perturbations are robust to diverse image transformations, especially the competitive evasion technique, MagDR via image reconstruction. Our findings potentially open up a new research direction towards thorough understanding and investigation of perceptual-aware adversarial attack for protecting facial images against DeepFakes in a proactive and robust manner. We open-source our tool to foster future research. Code is available at https://github.com/AbstractTeen/AntiForgery/.",反虐待：通过对抗性感知性扰动进行隐形而强大的深层破坏攻击。,Deepfake正在成为社会的真正风险，由于深深的多媒体而对个人隐私和政治安全构成潜在威胁，这是现实和令人信服的。然而，流行的深击被动检测是一个前法医的对策，未能预先阻止虚假信息扩散。为了解决这一局限性，研究人员通过将对抗性噪声添加到源数据中以破坏深层操作来研究主动的防御技术。但是，现有的关于通过注入对抗噪声的主动深层防御的研究并不强大，可以通过在最近的一项研究MAGDR中揭示的简单图像重建可以轻松绕开。在本文中，我们研究了现有的伪造技术的脆弱性，并提出了一种新颖的\ Emph {Antifergery}技术，该技术可帮助用户保护能够应用流行伪造技术的攻击者保护共享的面部图像。我们提出的方法以不断的方式产生感知意识的扰动，这与先前的研究差异很大，通过添加稀疏的对抗性噪声。实验结果表明，我们感知意识的扰动对各种图像变换具有鲁棒性，尤其是竞争性逃避技术，通过图像重建。我们的发现有可能为对感知意识的对抗性攻击的彻底理解和调查打开新的研究方向，以积极而健壮的方式保护面部图像免受深层影响。我们开源工具以促进未来的研究。代码可从https://github.com/abstractteen/antiforgery/获得。,https://arxiv.org/abs/2206.00477,IJCAI,True,False,False,False
1654,Comparison Knowledge Translation for Generalizable Image Classification.,"Zunlei Feng, Tian Qiu, Sai Wu, Xiaotuan Jin, Zengliang He, Mingli Song, Huiqiong Wang","Deep learning has recently achieved remarkable performance in image classification tasks, which depends heavily on massive annotation. However, the classification mechanism of existing deep learning models seems to contrast to humans' recognition mechanism. With only a glance at an image of the object even unknown type, humans can quickly and precisely find other same category objects from massive images, which benefits from daily recognition of various objects. In this paper, we attempt to build a generalizable framework that emulates the humans' recognition mechanism in the image classification task, hoping to improve the classification performance on unseen categories with the support of annotations of other categories. Specifically, we investigate a new task termed Comparison Knowledge Translation (CKT). Given a set of fully labeled categories, CKT aims to translate the comparison knowledge learned from the labeled categories to a set of novel categories. To this end, we put forward a Comparison Classification Translation Network (CCT-Net), which comprises a comparison classifier and a matching discriminator. The comparison classifier is devised to classify whether two images belong to the same category or not, while the matching discriminator works together in an adversarial manner to ensure whether classified results match the truth. Exhaustive experiments show that CCT-Net achieves surprising generalization ability on unseen categories and SOTA performance on target categories.",可概括图像分类的比较知识翻译。,深度学习最近在图像分类任务中取得了显着的性能，这在很大程度上取决于大量注释。但是，现有深度学习模型的分类机制似乎与人类的识别机制形成鲜明对比。只需浏览一下对象的图像即使是未知类型的图像，人类就可以快速而精确地从大型图像中找到其他相同的类别对象，这些对象受益于各种对象的日常识别。在本文中，我们试图建立一个可概括的框架，该框架在图像分类任务中模仿人类的识别机制，希望在其他类别的注释的支持下改善看不见类别的分类性能。具体而言，我们研究了一项称为比较知识翻译（CKT）的新任务。考虑到一组完全标记的类别，CKT旨在将从标记类别学到的比较知识转化为一组新型类别。为此，我们提出了一个比较分类翻译网络（CCT-NET），该网络包括一个比较分类器和匹配歧视器。设计比较分类器是为了对两个图像是否属于同一类别进行分类，而匹配的歧视器以对抗性方式一起工作以确保分类结果是否与真相相匹配。详尽的实验表明，CCT-NET在目标类别上对看不见的类别和SOTA性能具有令人惊讶的概括能力。,https://arxiv.org/abs/2205.03633,IJCAI,True,False,False,False
1655,Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning.,"Jun Luo, Shandong Wu","The goal of conventional federated learning (FL) is to train a global model for a federation of clients with decentralized data, reducing the systemic privacy risk of centralized training. The distribution shift across non-IID datasets, also known as the data heterogeneity, often poses a challenge for this one-global-model-fits-all solution. In this work, we propose APPLE, a personalized cross-silo FL framework that adaptively learns how much each client can benefit from other clients' models. We also introduce a method to flexibly control the focus of training APPLE between global and local objectives. We empirically evaluate our method's convergence and generalization behavior and performed extensive experiments on two benchmark datasets and two medical imaging datasets under two non-IID settings. The results show that the proposed personalized FL framework, APPLE, achieves state-of-the-art performance compared to several other personalized FL approaches in the literature.",适应适应：跨核心联合学习的学习个性化。,常规联合学习（FL）的目标是培训一个全球模型，为客户联合会使用分散数据，从而降低了集中式培训的系统性隐私风险。跨非IID数据集的分布变化（也称为数据异质性）通常会对这种单全球模型拟合所有解决方案构成挑战。在这项工作中，我们提出了一个个性化的跨核FL框架Apple，该框架可以自适应地了解每个客户可以从其他客户的模型中受益多少。我们还引入了一种方法，以灵活控制全球和本地目标之间培训苹果的重点。我们经验评估了我们的方法的收敛性和泛化行为，并在两个非IID设置下在两个基准数据集和两个医学成像数据集上进行了广泛的实验。结果表明，与文献中其他几种个性化的FL方法相比，提议的个性化FL框架Apple苹果实现了最先进的性能。,https://arxiv.org/abs/2110.08394,IJCAI,True,False,False,False
1656,Cumulative Stay-time Representation for Electronic Health Records in Medical Event Time Prediction.,"Takayuki Katsuki, Kohei Miyaguchi, Akira Koseki, Toshiya Iwamori, Ryosuke Yanagiya, Atsushi Suzuki","We address the problem of predicting when a disease will develop, i.e., medical event time (MET), from a patient's electronic health record (EHR). The MET of non-communicable diseases like diabetes is highly correlated to cumulative health conditions, more specifically, how much time the patient spent with specific health conditions in the past. The common time-series representation is indirect in extracting such information from EHR because it focuses on detailed dependencies between values in successive observations, not cumulative information. We propose a novel data representation for EHR called cumulative stay-time representation (CTR), which directly models such cumulative health conditions. We derive a trainable construction of CTR based on neural networks that has the flexibility to fit the target data and scalability to handle high-dimensional EHR. Numerical experiments using synthetic and real-world datasets demonstrate that CTR alone achieves a high prediction performance, and it enhances the performance of existing models when combined with them.",医疗活动时间预测中电子健康记录的累积停留时间代表。,我们解决了从患者的电子健康记录（EHR）中预测何时出现疾病（即医疗事件时间（MET））的问题。糖尿病等非传染性疾病大会与累积健康状况高度相关，更具体地说，患者过去花费了多少时间。在从EHR中提取此类信息时，常见的时间序列表示是间接的，因为它重点是连续观测值（而不是累积信息）之间的详细依赖关系。我们为EHR提出了一个新的数据表示，称为累积停留时间表示（CTR），该数据直接模拟了此类累积健康状况。我们基于神经网络得出可训练的CTR构造，该构建具有符合目标数据和可扩展性以处理高维EHR的灵活性。使用合成和现实世界数据集的数值实验表明，单独使用CTR可以实现高预测性能，并且在与它们结合使用时会增强现有模型的性能。,https://arxiv.org/abs/2204.13451,IJCAI,True,False,False,False
1657,"Domain-Adaptive Text Classification with Structured Knowledge from
  Unlabeled Data","Tian Li, Xiang Chen, Zhen Dong, Weijiang Yu, Yijun Yan, Kurt Keutzer, Shanghang Zhang","Domain adaptive text classification is a challenging problem for the large-scale pretrained language models because they often require expensive additional labeled data to adapt to new domains. Existing works usually fails to leverage the implicit relationships among words across domains. In this paper, we propose a novel method, called Domain Adaptation with Structured Knowledge (DASK), to enhance domain adaptation by exploiting word-level semantic relationships. DASK first builds a knowledge graph to capture the relationship between pivot terms (domain-independent words) and non-pivot terms in the target domain. Then during training, DASK injects pivot-related knowledge graph information into source domain texts. For the downstream task, these knowledge-injected texts are fed into a BERT variant capable of processing knowledge-injected textual data. Thanks to the knowledge injection, our model learns domain-invariant features for non-pivots according to their relationships with pivots. DASK ensures the pivots to have domain-invariant behaviors by dynamically inferring via the polarity scores of candidate pivots during training with pseudo-labels. We validate DASK on a wide range of cross-domain sentiment classification tasks and observe up to 2.9% absolute performance improvement over baselines for 20 different domain pairs. Code will be made available at https://github.com/hikaru-nara/DASK.","域自适应文本分类，具有结构化知识
  未标记的数据",域自适应文本分类对于大规模预处理的语言模型来说是一个具有挑战性的问题，因为它们通常需要昂贵的额外标记数据来适应新域。现有作品通常无法利用跨域单词之间的隐式关系。在本文中，我们提出了一种新的方法，称为结构化知识（DASK）的域适应性，以通过利用单词级别的语义关系来增强域的适应性。Dask首先构建知识图，以捕获目标域中的枢轴项（独立域单词）和非居式项之间的关系。然后在训练期间，DASK注入与源域文本的枢轴相关知识图信息。对于下游任务，这些注入知识的文本被馈入能够处理知识注入文本数据的BERT变体。多亏了知识注入，我们的模型根据与枢轴的关系学习了非客者的域不变特征。DASK通过在使用伪标签训练期间通过候选枢轴的极性得分动态推断出具有域不变行为的枢轴。我们在各种跨域情绪分类任务上验证了DASK，并观察到20种不同领域对的基准的绝对性能提高了2.9％。代码将在https://github.com/hikaru-nara/dask上提供。,https://arxiv.org/abs/2206.09591,IJCAI,True,False,False,False
1658,A Solver + Gradient Descent Training Algorithm for Deep Neural Networks.,"Dhananjay Ashok, Vineel Nagisetty, Christopher Srinivasa, Vijay Ganesh","We present a novel hybrid algorithm for training Deep Neural Networks that combines the state-of-the-art Gradient Descent (GD) method with a Mixed Integer Linear Programming (MILP) solver, outperforming GD and variants in terms of accuracy, as well as resource and data efficiency for both regression and classification tasks. Our GD+Solver hybrid algorithm, called GDSolver, works as follows: given a DNN $D$ as input, GDSolver invokes GD to partially train $D$ until it gets stuck in a local minima, at which point GDSolver invokes an MILP solver to exhaustively search a region of the loss landscape around the weight assignments of $D$'s final layer parameters with the goal of tunnelling through and escaping the local minima. The process is repeated until desired accuracy is achieved. In our experiments, we find that GDSolver not only scales well to additional data and very large model sizes, but also outperforms all other competing methods in terms of rates of convergence and data efficiency. For regression tasks, GDSolver produced models that, on average, had 31.5% lower MSE in 48% less time, and for classification tasks on MNIST and CIFAR10, GDSolver was able to achieve the highest accuracy over all competing methods, using only 50% of the training data that GD baselines required.",深层神经网络的求解器 +梯度下降训练算法。,我们提出了一种用于训练深神经网络的新型混合算法，该算法将最先进的梯度下降（GD）方法与混合整数线性编程（MILP）求解器相结合，以准确性以及变体的差异以及变体，以及回归和分类任务的资源和数据效率。我们的GD+求解器混合算法称为GDSolver，工作如下：给定DNN $ d $作为输入，GDSolver召集GD派出部分训练$ d $，直到卡在当地的最小值中，这一点GDSOLVER将Milp Solver召集到一定程度上详尽地搜索损失景观的区域，围绕$ d $的最后一层参数的重量分配，目的是贯穿并逃脱本地的最小值。重复该过程，直到达到所需的准确性。在我们的实验中，我们发现GDSolver不仅可以很好地扩展到其他数据和非常大的模型大小，而且还优于收敛和数据效率率的所有其他竞争方法。对于回归任务，GDOLVER生产的模型平均在48％的时间内降低了31.5％，并且对于MNIST和CIFAR10的分类任务，GDSOLVER仅使用所有竞争方法就能达到最高精度，仅使用50％GD基准需要的培训数据。,https://arxiv.org/abs/2207.03264,IJCAI,True,False,False,False
1659,SVTR: Scene Text Recognition with a Single Visual Model.,"Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, Yu-Gang Jiang","Dominant scene text recognition models commonly contain two building blocks, a visual model for feature extraction and a sequence model for text transcription. This hybrid architecture, although accurate, is complex and less efficient. In this study, we propose a Single Visual model for Scene Text recognition within the patch-wise image tokenization framework, which dispenses with the sequential modeling entirely. The method, termed SVTR, firstly decomposes an image text into small patches named character components. Afterward, hierarchical stages are recurrently carried out by component-level mixing, merging and/or combining. Global and local mixing blocks are devised to perceive the inter-character and intra-character patterns, leading to a multi-grained character component perception. Thus, characters are recognized by a simple linear prediction. Experimental results on both English and Chinese scene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L (Large) achieves highly competitive accuracy in English and outperforms existing methods by a large margin in Chinese, while running faster. In addition, SVTR-T (Tiny) is an effective and much smaller model, which shows appealing speed at inference. The code is publicly available at https://github.com/PaddlePaddle/PaddleOCR.",SVTR：具有单个视觉模型的场景文本识别。,主观场景文本识别模型通常包含两个构建块，一个用于特征提取的视觉模型以及用于文本转录的序列模型。这种混合体系结构虽然准确，但很复杂且效率较低。在这项研究中，我们提出了一个单一的视觉模型，用于在贴片图像令牌框架内进行场景文本识别，该模型完全分配了顺序建模。该方法称为SVTR，首先将图像文本分解为名为“字符成分”的小贴片。之后，通过组件级混合，合并和/或组合，将分层阶段反复进行。设计了全局和局部混合块，以感知字符间和字符内模式，从而导致多透明的字符成分感知。因此，字符通过简单的线性预测识别。英语和中文场景文本识别任务的实验结果证明了SVTR的有效性。SVTR-L（大）在英语中实现了高度竞争性的精度，并且在运行速度更快的同时，中文的优于现有方法的优于现有方法。此外，SVTR-T（TININE）是一个有效且较小的模型，它显示了推理时具有吸引力的速度。该代码可在https://github.com/paddlepaddle/paddleocr上公开获得。,https://arxiv.org/abs/2205.00159,IJCAI,True,False,False,False
1660,SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification.,"Haocong Rao, Chunyan Miao","Recent advances in skeleton-based person re-identification (re-ID) obtain impressive performance via either hand-crafted skeleton descriptors or skeleton representation learning with deep learning paradigms. However, they typically require skeletal pre-modeling and label information for training, which leads to limited applicability of these methods. In this paper, we focus on unsupervised skeleton-based person re-ID, and present a generic Simple Masked Contrastive learning (SimMC) framework to learn effective representations from unlabeled 3D skeletons for person re-ID. Specifically, to fully exploit skeleton features within each skeleton sequence, we first devise a masked prototype contrastive learning (MPC) scheme to cluster the most typical skeleton features (skeleton prototypes) from different subsequences randomly masked from raw sequences, and contrast the inherent similarity between skeleton features and different prototypes to learn discriminative skeleton representations without using any label. Then, considering that different subsequences within the same sequence usually enjoy strong correlations due to the nature of motion continuity, we propose the masked intra-sequence contrastive learning (MIC) to capture intra-sequence pattern consistency between subsequences, so as to encourage learning more effective skeleton representations for person re-ID. Extensive experiments validate that the proposed SimMC outperforms most state-of-the-art skeleton-based methods. We further show its scalability and efficiency in enhancing the performance of existing models. Our codes are available at https://github.com/Kali-Hac/SimMC.",SIMMC：简单的掩盖对比度学习无监督人员重新识别的骨骼表示。,基于骨架的人重新识别（RE-ID）的最新进展通过手工制作的骨骼描述符或骨骼表示，并具有深度学习范式的学习。但是，它们通常需要骨骼预制和标签信息进行培训，这导致这些方法的适用性有限。在本文中，我们专注于无监督的基于骨架的人重新ID，并提出一个通用的简单掩盖对比度学习（SIMMC）框架，以从未标记的3D骨架中学习有效的表示人。具体而言，要在每个骨架序列中充分利用骨骼特征，我们首先设计了一个掩盖的原型对比度学习（MPC）方案，以聚集最典型的骨骼特征（骨架原型），从从原始序列随机掩盖的不同子序列中进行，并与原始序列之间的固有相似性之间的相似性。骨骼特征和不同的原型，可学习判别骨骼表示，而无需使用任何标签。然后，考虑到同一顺序中不同的子序列通常由于运动连续性的性质而具有很强的相关性，我们提出了掩盖的序列内对比度学习（MIC）以捕获子序列之间的序列模式一致性，以鼓励学习更多人重新ID的有效骨骼表示。广泛的实验验证了所提出的SIMMC优于大多数基于最先进的骨架方法。我们进一步显示了其在增强现有模型性能方面的可扩展性和效率。我们的代码可在https://github.com/kali-hac/simmc上找到。,https://arxiv.org/abs/2204.09826,IJCAI,True,False,False,False
1661,BandMaxSAT: A Local Search MaxSAT Solver with Multi-armed Bandit.,"Jiongzhi Zheng, Kun He, Jianrong Zhou, Yan Jin, Chu-min Li, Felip Manya","We address Partial MaxSAT (PMS) and Weighted PMS (WPMS), two practical generalizations of the MaxSAT problem, and propose a local search algorithm called BandMaxSAT, that applies a multi-armed bandit to guide the search direction, for these problems. The bandit in our method is associated with all the soft clauses in the input (W)PMS instance. Each arm corresponds to a soft clause. The bandit model can help BandMaxSAT to select a good direction to escape from local optima by selecting a soft clause to be satisfied in the current step, that is, selecting an arm to be pulled. We further propose an initialization method for (W)PMS that prioritizes both unit and binary clauses when producing the initial solutions. Extensive experiments demonstrate that BandMaxSAT significantly outperforms the state-of-the-art (W)PMS local search algorithm SATLike3.0. Specifically, the number of instances in which BandMaxSAT obtains better results is about twice that obtained by SATLike3.0. We further combine BandMaxSAT with the complete solver TT-Open-WBO-Inc. The resulting solver BandMaxSAT-c also outperforms some of the best state-of-the-art complete (W)PMS solvers, including SATLike-c, Loandra and TT-Open-WBO-Inc.",BandMaxSat：具有多臂强盗的本地搜索MaxSat求解器。,我们解决了部分MaxSAT（PMS）和加权PMS（WPM），这是MaxSat问题的两个实际概括，并提出了一种称为BandMaxSat的局部搜索算法，该算法应用多臂强盗来指导搜索方向，以解决这些问题。我们方法中的匪徒与输入（w）pms实例中的所有软子句相关联。每个手臂对应于软子句。Bandit模型可以通过选择要在当前步骤中满足的软子句，即选择要拉的臂来帮助BandmaxSat选择一个良好的方向以逃脱本地Optima。我们进一步提出了一种初始化方法（w）PMS，在生产初始解决方案时优先考虑单元和二进制条款。广泛的实验表明，BandMaxSat显着优于最先进的（W）PMS本地搜索算法SATLIKE3.0。具体而言，BandMaxSat获得更好结果的实例数量大约是Satlike3.0获得的两倍。我们将BandMaxSat与完整的求解器TT-OPEN-WBO-INC结合在一起。最终的求解器bandmaxsat-c还胜过一些最好的最新完整（W）PMS求解器，包括satlike-c，loandra和tt-open-wbo-inc。,https://arxiv.org/abs/2201.05544,IJCAI,True,False,False,False
1662,RAPQ: Rescuing Accuracy for Power-of-Two Low-bit Post-training Quantization.,"Hongyi Yao, Pu Li, Jian Cao, Xiangcheng Liu, Chenying Xie, Bingzhang Wang","We introduce a Power-of-Two post-training quantization( PTQ) method for deep neural network that meets hardware requirements and does not call for long-time retraining. PTQ requires a small set of calibration data and is easier for deployment, but results in lower accuracy than Quantization-Aware Training( QAT). Power-of-Two quantization can convert the multiplication introduced by quantization and dequantization to bit-shift that is adopted by many efficient accelerators. However, the Power-of-Two scale has fewer candidate values, which leads to more rounding or clipping errors. We propose a novel Power-of-Two PTQ framework, dubbed RAPQ, which dynamically adjusts the Power-of-Two scales of the whole network instead of statically determining them layer by layer. It can theoretically trade off the rounding error and clipping error of the whole network. Meanwhile, the reconstruction method in RAPQ is based on the BN information of every unit. Extensive experiments on ImageNet prove the excellent performance of our proposed method. Without bells and whistles, RAPQ can reach accuracy of 65% and 48% on ResNet-18 and MobileNetV2 respectively with weight INT2 activation INT4. We are the first to propose PTQ for the more constrained but hardware-friendly Power-of-Two quantization and prove that it can achieve nearly the same accuracy as SOTA PTQ method. The code will be released.",RAPQ：两次低位训练后培训量化的挽救准确性。,我们引入了针对深神经网络的两次训练后培训量化（PTQ）方法，该方法符合硬件要求，并且不需要长期重新训练。PTQ需要一小部分校准数据，并且更容易进行部署，但是与量化感知训练（QAT）相比，精度较低。两次量化的能力可以将通过量化和去除化引入的乘法转换为许多有效加速器采用的位移位。但是，两者量表的候选值较少，这会导致更多的舍入或剪辑错误。我们提出了一种新型的两个PTQ框架，称为RAPQ，该框架被动态调整了整个网络的两个尺度，而不是静态地确定它们一层。从理论上讲，它可以权衡整个网络的舍入错误和剪辑错误。同时，RAPQ中的重建方法基于每个单元的BN信息。对Imagenet的广泛实验证明了我们提出的方法的出色性能。没有铃铛和哨声，REPQ在RESNET-18和MOBILENETV2上的准确度可以达到65％和48％，分别具有INT2激活INT4的精度。我们是第一个提出PTQ的人，以实现更受限制但对硬件友好的两次量化功率，并证明它可以达到与SOTA PTQ方法几乎相同的准确性。代码将发布。,https://arxiv.org/abs/2204.12322,IJCAI,True,False,False,False
1663,Geometric Transformer for End-to-End Molecule Properties Prediction.,"Yoni Choukroun, Lior Wolf","Transformers have become methods of choice in many applications thanks to their ability to represent complex interaction between elements. However, extending the Transformer architecture to non-sequential data such as molecules and enabling its training on small datasets remain a challenge. In this work, we introduce a Transformer-based architecture for molecule property prediction, which is able to capture the geometry of the molecule. We modify the classical positional encoder by an initial encoding of the molecule geometry, as well as a learned gated self-attention mechanism. We further suggest an augmentation scheme for molecular data capable of avoiding the overfitting induced by the overparameterized architecture. The proposed framework outperforms the state-of-the-art methods while being based on pure machine learning solely, i.e. the method does not incorporate domain knowledge from quantum chemistry and does not use extended geometric inputs beside the pairwise atomic distances.",端到端分子特性预测的几何变压器。,由于它们能够代表元素之间的复杂相互作用，因此在许多应用中，变形金刚已成为选择方法。但是，将变压器体系结构扩展到非序列数据，例如分子并在小型数据集上进行培训仍然是一个挑战。在这项工作中，我们介绍了一个基于变压器的分子性能预测的结构，该结构能够捕获分子的几何形状。我们通过对分子几何形状的初始编码以及学习的门控自我注意机制来修改经典的位置编码器。我们进一步提出了一种扩大方案，用于避免过度参数化架构引起的过度拟合的分子数据。所提出的框架在仅基于纯机器学习的同时优于最先进的方法，即该方法不包括量子化学的域知识，并且不使用成对原子距离旁边的扩展几何输入。,https://arxiv.org/abs/2110.13721,IJCAI,True,False,False,False
1664,GL-RG: Global-Local Representation Granularity for Video Captioning.,"Liqi Yan, Qifan Wang, Yiming Cui, Fuli Feng, Xiaojun Quan, Xiangyu Zhang, Dongfang Liu","Video captioning is a challenging task as it needs to accurately transform visual understanding into natural language description. To date, state-of-the-art methods inadequately model global-local representation across video frames for caption generation, leaving plenty of room for improvement. In this work, we approach the video captioning task from a new perspective and propose a GL-RG framework for video captioning, namely a \textbf{G}lobal-\textbf{L}ocal \textbf{R}epresentation \textbf{G}ranularity. Our GL-RG demonstrates three advantages over the prior efforts: 1) we explicitly exploit extensive visual representations from different video ranges to improve linguistic expression; 2) we devise a novel global-local encoder to produce rich semantic vocabulary to obtain a descriptive granularity of video contents across frames; 3) we develop an incremental training strategy which organizes model learning in an incremental fashion to incur an optimal captioning behavior. Experimental results on the challenging MSR-VTT and MSVD datasets show that our DL-RG outperforms recent state-of-the-art methods by a significant margin. Code is available at \url{https://github.com/ylqi/GL-RG}.",GL-RG：视频字幕的全球本地表示粒度。,视频字幕是一项艰巨的任务，因为它需要将视觉理解准确地转换为自然语言描述。迄今为止，最新的方法不足地建模了整个视频框架的全球本地表示形式，以产生字幕生成，留出了很大的改进空间。在这项工作中，我们从新的角度了解视频字幕任务，并为视频字幕提出了一个GL-RG框架，即\ textbf {g} lobal- \ textbf {l} ocal \ textbf {r textbf {r}} ranularity。我们的GL-RG在先前的努力中展示了三个优点：1）我们明确利用不同视频范围的广泛视觉表示以改善语言表达；2）我们设计了一种新型的全球本地编码器，以产生丰富的语义词汇，以获得跨帧中视频内容的描述性粒度；3）我们制定了一种增量培训策略，该策略以渐进的方式组织模型学习，以产生最佳字幕行为。关于挑战性的MSR-VTT和MSVD数据集的实验结果表明，我们的DL-RG优于最近的最新方法。代码可在\ url {https://github.com/ylqi/gl-rg}上获得。,https://arxiv.org/abs/2205.10706,IJCAI,True,False,False,False
1665,Offline Time-Independent Multi-Agent Path Planning.,"Keisuke Okumura, François Bonnet, Yasumasa Tamura, Xavier Défago","This paper studies a novel planning problem for multiple agents moving on graphs that we call offline time-independent multi-agent path planning (OTIMAPP). The motivation is to overcome time uncertainties in multi-agent scenarios where we cannot expect agents to act perfectly following timed plans, e.g., executions with mobile robots. For this purpose, OTIMAPP abandons all timing assumptions; it is offline planning that assumes event-driven executions without or less run-time effort. The problem is finding plans to be terminated correctly in any action orders of agents, i.e., guaranteeing that all agents eventually reach their destinations. We address a bunch of questions for this problem: required conditions for feasible solutions, computational complexity, comparison with well-known other multi-agent problems, construction of solvers, effective relaxation of a solution concept, and how to implement the plans by actual robots. Throughout the paper, we establish the foundation of OTIMAPP and demonstrate its utility. A video is available at https://kei18.github.io/otimapp.",离线时间无关的多代理路径计划。,本文研究了一个新的计划问题，用于在图形上移动的多个代理，我们称之为离线时间无关的多代理路径计划（OTIMAPP）。动机是在多代理场景中克服时间不确定性，在这种情况下，我们不能指望代理商按时计划（例如使用移动机器人执行执行）表现出色。为此，Otimapp放弃了所有定时假设；正是脱机计划，假定事件驱动的执行，而无需或更少的运行时间努力。问题是找到计划在任何代理的行动命令中正确终止的计划，即确保所有代理人最终到达目的地。我们解决了有关此问题的许多问题：可行解决方案的必需条件，计算复杂性，与其他众所周知的其他多代理问题进行比较，解决方案的构建，有效的解决方案概念的放松以及如何通过实际机器人实施计划。在整篇文章中，我们建立了Otimapp的基础，并展示了其实用性。可以在https://kei18.github.io/otimapp上获得视频。,https://arxiv.org/abs/2105.07132,IJCAI,True,False,False,False
1666,Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge.,"Jiangnan Li, Fandong Meng, Zheng Lin, Rui Liu, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou","Conversational Causal Emotion Entailment aims to detect causal utterances for a non-neutral targeted utterance from a conversation. In this work, we build conversations as graphs to overcome implicit contextual modelling of the original entailment style. Following the previous work, we further introduce the emotion information into graphs. Emotion information can markedly promote the detection of causal utterances whose emotion is the same as the targeted utterance. However, it is still hard to detect causal utterances with different emotions, especially neutral ones. The reason is that models are limited in reasoning causal clues and passing them between utterances. To alleviate this problem, we introduce social commonsense knowledge (CSK) and propose a Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two utterances. As not all CSK is emotionally suitable for utterances, we therefore propose a sentiment-realized knowledge selecting strategy to filter CSK. To process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph networks. Experimental results show that our method outperforms baselines and infers more causes with different emotions from the targeted utterance.",中立的话语也是原因：增强对话性因果情绪与社会常识知识相关。,会话因果情绪的旨在检测来自对话的非中性针对性话语的因果话语。在这项工作中，我们构建对话作为图表，以克服原始组成风格的隐式上下文建模。在先前的工作之后，我们将情感信息进一步介绍给图形。情感信息可以显着促进对情感与目标话语相同的因果话语的发现。但是，仍然很难检测出不同情绪，尤其是中立情绪的因果话语。原因是模型在原因因果线索中受到限制，并将其传递给在话语之间。为了减轻这个问题，我们介绍了社交常识性知识（CSK），并提出知识增强的对话图（KEC）。KEC在两种话语之间传播了CSK。因此，由于并非所有的CSK在情感上都适合话语，因此我们提出了一个情感知识的知识选择策略来过滤CSK。为了处理KEC，我们进一步构建了知识增强的定向无环图网络。实验结果表明，我们的方法优于基准，并以与目标话语不同的情绪不同的原因引起了更多的原因。,https://arxiv.org/abs/2205.00759,IJCAI,True,False,False,False
1667,Gromov-Wasserstein Discrepancy with Local Differential Privacy for Distributed Structural Graphs.,"Hongwei Jin, Xun Chen","Learning the similarity between structured data, especially the graphs, is one of the essential problems. Besides the approach like graph kernels, Gromov-Wasserstein (GW) distance recently draws big attention due to its flexibility to capture both topological and feature characteristics, as well as handling the permutation invariance. However, structured data are widely distributed for different data mining and machine learning applications. With privacy concerns, accessing the decentralized data is limited to either individual clients or different silos. To tackle these issues, we propose a privacy-preserving framework to analyze the GW discrepancy of node embedding learned locally from graph neural networks in a federated flavor, and then explicitly place local differential privacy (LDP) based on Multi-bit Encoder to protect sensitive information. Our experiments show that, with strong privacy protections guaranteed by the $\varepsilon$-LDP algorithm, the proposed framework not only preserves privacy in graph learning but also presents a noised structural metric under GW distance, resulting in comparable and even better performance in classification and clustering tasks. Moreover, we reason the rationale behind the LDP-based GW distance analytically and empirically.",Gromov-Wasserstein差异，具有分布式结构图的局部微分隐私。,学习结构化数据（尤其是图形）之间的相似性是基本问题之一。除了图形内核之外，Gromov-Wasserstein（GW）距离最近由于其灵活地捕获拓扑特性和特征特征以及处理置换不变性而引起了很大的关注。但是，对于不同的数据挖掘和机器学习应用程序，结构化数据被广泛分布。出于隐私问题，访问分散数据仅限于单个客户或其他孤岛。为了解决这些问题，我们提出了一个隐私保护框架，以分析从联合风味中从图形神经网络中学到的节点嵌入的GW差异，然后明确地将基于多位数编码器的当地差异隐私（LDP）放置，以保护敏感的编码器信息。我们的实验表明，通过$ \ varepsilon $ -LDP算法保证了强大的隐私保护，拟议的框架不仅保留了图形学习中的隐私，而且在GW距离下呈现了噪声结构指标，从而在分类中提供了可比性的性能，甚至更好和聚类任务。此外，我们通过分析和经验上基于LDP的GW距离背后的基本原理。,https://arxiv.org/abs/2202.00808,IJCAI,True,False,False,False
1668,Learning to Hash Naturally Sorts,"Yuming Shen, Jiaguo Yu, Haofeng Zhang, Philip H. S. Torr, Menghan Wang","Locality sensitive hashing pictures a list-wise sorting problem. Its testing metrics, e.g., mean-average precision, count on a sorted candidate list ordered by pair-wise code similarity. However, scarcely does one train a deep hashing model with the sorted results end-to-end because of the non-differentiable nature of the sorting operation. This inconsistency in the objectives of training and test may lead to sub-optimal performance since the training loss often fails to reflect the actual retrieval metric. In this paper, we tackle this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming distances of samples' hash codes and accordingly gather their latent representations for self-supervised training. Thanks to the recent advances in differentiable sorting approximations, the hash head receives gradients from the sorter so that the hash encoder can be optimized along with the training procedure. Additionally, we describe a novel Sorted Noise-Contrastive Estimation (SortedNCE) loss that selectively picks positive and negative samples for contrastive learning, which allows NSH to mine data semantic relations during training in an unsupervised manner. Our extensive experiments show the proposed NSH model significantly outperforms the existing unsupervised hashing methods on three benchmarked datasets.",学会自然排列,局部敏感的散列图片是列表分类问题。它的测试指标，例如，平均水平的精度，指望按配对代码相似性排序的分类候选列表。但是，由于分类操作的非差异性质，几乎没有人会以端到端的方式训练一个深层的散列模型。在培训和测试目标中的这种不一致可能会导致次优性能，因为训练损失通常无法反映实际的检索度量。在本文中，我们通过引入自然排序的哈希（NSH）来解决这个问题。我们对样品的哈希守则的锤距距离进行排序，并因此收集其潜在表现以进行自我监督培训。由于最新的可区分排序近似值的进展，哈希头从分层器中接收梯度，因此可以与培训过程一起优化哈希编码器。此外，我们描述了一种新型的分类噪声对焦估计（分类）损失，该损失有选择地选择正面和负面样本进行对比学习，这使NSH可以在培训期间以无监督的方式挖掘数据语义关系。我们的广泛实验表明，所提出的NSH模型大大优于三个基准数据集上现有的无监督哈希方法。,https://arxiv.org/abs/2201.13322,IJCAI,True,False,False,False
1669,AdMix: A Mixed Sample Data Augmentation Method for Neural Machine Translation.,"Chang Jin, Shigui Qiu, Nini Xiao, Hao Jia","In Neural Machine Translation (NMT), data augmentation methods such as back-translation have proven their effectiveness in improving translation performance. In this paper, we propose a novel data augmentation approach for NMT, which is independent of any additional training data. Our approach, AdMix, consists of two parts: 1) introduce faint discrete noise (word replacement, word dropping, word swapping) into the original sentence pairs to form augmented samples; 2) generate new synthetic training data by softly mixing the augmented samples with their original samples in training corpus. Experiments on three translation datasets of different scales show that AdMix achieves signifi cant improvements (1.0 to 2.7 BLEU points) over strong Transformer baseline. When combined with other data augmentation techniques (e.g., back-translation), our approach can obtain further improvements.",AMDIX：一种用于神经机器翻译的混合样品数据增强方法。,在神经机器翻译（NMT）中，诸如反向翻译之类的数据增强方法证明了它们在改善翻译性能方面的有效性。在本文中，我们为NMT提出了一种新型的数据增强方法，该方法与任何其他培训数据无关。我们的方法，Ambix包括两个部分：1）将微弱的离散噪声（单词更换，单词删除，单词交换）对原始句子对组成，以形成增强样品；2）通过将增强样品与训练语料库中的原始样品轻轻混合在一起来生成新的合成训练数据。在三个不同量表的三个翻译数据集上进行的实验表明，与强的变压器基线相比，Ambix实现了显着改进（1.0至2.7 BLEU点）。当与其他数据增强技术（例如，反向翻译）结合使用时，我们的方法可以获得进一步的改进。,https://arxiv.org/abs/2205.04686,IJCAI,True,False,False,False
1670,Variational Learning for Unsupervised Knowledge Grounded Dialogs.,"Mayank Mishra, Dhiraj Madan, Gaurav Pandey, Danish Contractor","Recent methods for knowledge grounded dialogs generate responses by incorporating information from an external textual document. These methods do not require the exact document to be known during training and rely on the use of a retrieval system to fetch relevant documents from a large index. The documents used to generate the responses are modeled as latent variables whose prior probabilities need to be estimated. Models such as RAG , marginalize the document probabilities over the documents retrieved from the index to define the log likelihood loss function which is optimized end-to-end.   In this paper, we develop a variational approach to the above technique wherein, we instead maximize the Evidence Lower bound (ELBO). Using a collection of three publicly available open-conversation datasets, we demonstrate how the posterior distribution, that has information from the ground-truth response, allows for a better approximation of the objective function during training. To overcome the challenges associated with sampling over a large knowledge collection, we develop an efficient approach to approximate the ELBO. To the best of our knowledge we are the first to apply variational training for open-scale unsupervised knowledge grounded dialog systems.",无监督知识接地对话的变分学习。,知识接地对话框的最新方法通过合并外部文本文档中的信息来产生响应。这些方法不需要在培训期间知道确切的文档，并且依靠使用检索系统从大型索引中获取相关文档。用于生成响应的文档将建模为需要估算先前概率的潜在变量。诸如抹布之类的模型，将文档概率边缘化在从索引检索到的文档上，以定义对数可能性损失函数的定义，该函数被优化。在本文中，我们开发了上述技术的变分方法，在其中，我们相反，最大化证据下限（ELBO）。使用三个公开可用的开放式数据集的集合，我们演示了如何从地面响应中获得信息的后验分布，可以在训练过程中更好地近似目标函数。为了克服与大型知识收集相关的挑战，我们开发了一种有效的方法来近似Elbo。据我们所知，我们是第一个对开放式无监督知识接地对话系统进行各种培训的人。,https://arxiv.org/abs/2112.00653,IJCAI,True,False,False,False
1671,Accelerated Multiplicative Weights Update Avoids Saddle Points Almost Always.,"Yi Feng, Ioannis Panageas, Xiao Wang","We consider non-convex optimization problems with constraint that is a product of simplices. A commonly used algorithm in solving this type of problem is the Multiplicative Weights Update (MWU), an algorithm that is widely used in game theory, machine learning and multi-agent systems. Despite it has been known that MWU avoids saddle points, there is a question that remains unaddressed:""Is there an accelerated version of MWU that avoids saddle points provably?"" In this paper we provide a positive answer to above question. We provide an accelerated MWU based on Riemannian Accelerated Gradient Descent, and prove that the Riemannian Accelerated Gradient Descent, thus the accelerated MWU, almost always avoid saddle points.",加速的倍增权重更新几乎总是避免鞍点。,我们认为非凸优化问题具有约束，这是简单的产物。解决此类问题的一种常用算法是乘法权重更新（MWU），这是一种在游戏理论，机器学习和多代理系统中广泛使用的算法。尽管众所周知，MWU避免了鞍点，但仍有一个未解决的问题：“ MWU是否有加速版本可以避免鞍点？”在本文中，我们为上述问题提供了积极的答案。我们提供了基于Riemannian加速梯度下降的加速MWU，并证明了Riemannian加速梯度下降，因此加速MWU，几乎总是避免鞍点。,https://arxiv.org/abs/2204.11407,IJCAI,True,False,False,False
1672,Neuro-Symbolic Verification of Deep Neural Networks.,"Xuan Xie, Kristian Kersting, Daniel Neider","Formal verification has emerged as a powerful approach to ensure the safety and reliability of deep neural networks. However, current verification tools are limited to only a handful of properties that can be expressed as first-order constraints over the inputs and output of a network. While adversarial robustness and fairness fall under this category, many real-world properties (e.g., ""an autonomous vehicle has to stop in front of a stop sign"") remain outside the scope of existing verification technology. To mitigate this severe practical restriction, we introduce a novel framework for verifying neural networks, named neuro-symbolic verification. The key idea is to use neural networks as part of the otherwise logical specification, enabling the verification of a wide variety of complex, real-world properties, including the one above. Moreover, we demonstrate how neuro-symbolic verification can be implemented on top of existing verification infrastructure for neural networks, making our framework easily accessible to researchers and practitioners alike.",深神经网络的神经符号验证。,正式验证已成为确保深神经网络的安全性和可靠性的强大方法。但是，当前的验证工具仅限于少数属性，这些属性可以作为网络输入和输出的一阶约束表示。尽管对抗性的鲁棒性和公平性属于这一类，但许多现实世界的属性（例如，“自动驾驶汽车必须停在停车标志面前”）仍然超出了现有验证技术的范围。为了减轻这种严重的实际限制，我们引入了一个新的框架，用于验证神经网络，名为Neuro-Symbolic验证。关键思想是将神经网络用作原本逻辑规范的一部分，以验证各种复杂的现实世界属性，包括上述属性。此外，我们证明了如何在神经网络的现有验证基础架构之上实施神经符号验证，使研究人员和从业人员都可以轻松访问我们的框架。,https://arxiv.org/abs/2203.00938,IJCAI,True,False,False,False
1673,A Unified Framework for Adversarial Attack and Defense in Constrained Feature Space.,"Thibault Simonetto, Salijona Dyrmishi, Salah Ghamizi, Maxime Cordy, Yves Le Traon","The generation of feasible adversarial examples is necessary for properly assessing models that work on constrained feature space. However, it remains a challenging task to enforce constraints into attacks that were designed for computer vision. We propose a unified framework to generate feasible adversarial examples that satisfy given domain constraints. Our framework supports the use cases reported in the literature and can handle both linear and non-linear constraints. We instantiate our framework into two algorithms: a gradient-based attack that introduces constraints in the loss function to maximize, and a multi-objective search algorithm that aims for misclassification, perturbation minimization, and constraint satisfaction. We show that our approach is effective on two datasets from different domains, with a success rate of up to 100%, where state-of-the-art attacks fail to generate a single feasible example. In addition to adversarial retraining, we propose to introduce engineered non-convex constraints to improve model adversarial robustness. We demonstrate that this new defense is as effective as adversarial retraining. Our framework forms the starting point for research on constrained adversarial attacks and provides relevant baselines and datasets that future research can exploit.",在受约束的特征空间中进行对抗攻击和防御的统一框架。,对于正确评估在受约束特征空间上工作的模型的正确评估模型是必要的。但是，将限制强制为专门为计算机视觉设计的攻击仍然是一项具有挑战性的任务。我们提出了一个统一的框架，以生成满足给定领域约束的可行逆流示例。我们的框架支持文献中报告的用例，并且可以处理线性和非线性约束。我们将框架实例化为两种算法：基于梯度的攻击，它引入了损失函数中的约束以最大化，以及一种旨在用于错误分类，扰动最小化和约束满意度的多目标搜索算法。我们表明，我们的方法在来自不同域的两个数据集上有效，其成功率高达100％，其中最先进的攻击无法生成一个可行的示例。除了对抗性训练外，我们还建议引入工程的非凸限制，以提高模型对抗性鲁棒性。我们证明，这种新防御与对抗性训练一样有效。我们的框架构成了研究受限的对抗攻击的起点，并提供了相关的基线和数据集，未来研究可以利用这些基线和数据集。,https://arxiv.org/abs/2112.01156,IJCAI,True,False,False,False
1674,A Strengthened Branch and Bound Algorithm for the Maximum Common (Connected) Subgraph Problem.,"Jianrong Zhou, Kun He, Jiongzhi Zheng, Chu-Min Li, Yanli Liu","We propose a new and strengthened Branch-and-Bound (BnB) algorithm for the maximum common (connected) induced subgraph problem based on two new operators, Long-Short Memory (LSM) and Leaf vertex Union Match (LUM). Given two graphs for which we search for the maximum common (connected) induced subgraph, the first operator of LSM maintains a score for the branching node using the short-term reward of each vertex of the first graph and the long-term reward of each vertex pair of the two graphs. In this way, the BnB process learns to reduce the search tree size significantly and improve the algorithm performance. The second operator of LUM further improves the performance by simultaneously matching the leaf vertices connected to the current matched vertices, and allows the algorithm to match multiple vertex pairs without affecting the solution optimality. We incorporate the two operators into the state-of-the-art BnB algorithm McSplit, and denote the resulting algorithm as McSplit+LL. Experiments show that McSplit+LL outperforms McSplit+RL, a more recent variant of McSplit using reinforcement learning that is superior than McSplit.",最大共同（连接）子图问题的加强分支和结合算法。,我们提出了一种基于两个新的操作员，长短记忆（LSM）和Leaf Vertex Union Match（LUM）的最大常见（连接）诱导子图问题，提出了一种新的且增强的分支和结合（BNB）算法。给定两个图表，我们搜索了最大共同（连接）诱导子图，LSM的第一个操作员使用第一个图的每个顶点的短期奖励维持分支节点的分数，并且每个图的长期奖励两个图的顶点对。通过这种方式，BNB过程学会了大大减少搜索树的大小并改善算法性能。Lum的第二个操作员通过同时匹配连接到当前匹配的顶点的叶顶点来进一步提高性能，并允许该算法匹配多个顶点对，而不会影响解决方案最佳性。我们将两个操作员纳入最先进的BNB算法MCSPLIT，并将结果算法表示为MCSPLIT+LL。实验表明，MCSPLIT+LL优于MCSPLIT+RL，这是MCSPLIT的最新变体，使用了比MCSPLIT优越的增强学习。,https://arxiv.org/abs/2201.06252,IJCAI,True,False,False,False
1675,Neural Network Pruning by Cooperative Coevolution.,"Haopu Shang, Jia-Liang Wu, Wenjing Hong, Chao Qian","Neural network pruning is a popular model compression method which can significantly reduce the computing cost with negligible loss of accuracy. Recently, filters are often pruned directly by designing proper criteria or using auxiliary modules to measure their importance, which, however, requires expertise and trial-and-error. Due to the advantage of automation, pruning by evolutionary algorithms (EAs) has attracted much attention, but the performance is limited for deep neural networks as the search space can be quite large. In this paper, we propose a new filter pruning algorithm CCEP by cooperative coevolution, which prunes the filters in each layer by EAs separately. That is, CCEP reduces the pruning space by a divide-and-conquer strategy. The experiments show that CCEP can achieve a competitive performance with the state-of-the-art pruning methods, e.g., prune ResNet56 for $63.42\%$ FLOPs on CIFAR10 with $-0.24\%$ accuracy drop, and ResNet50 for $44.56\%$ FLOPs on ImageNet with $0.07\%$ accuracy drop.",合作进化的神经网络修剪。,神经网络修剪是一种流行的模型压缩方法，可以显着降低计算成本，而准确性丧失。最近，过滤器通常通过设计适当的标准或使用辅助模块来衡量其重要性来直接修剪过滤器，但是，这需要专业知识和反复试验。由于自动化的优势，进化算法（EAS）的修剪引起了很多关注，但是由于搜索空间可能很大，因此对深神经网络的性能受到限制。在本文中，我们提出了一种合作进化的新滤波器修剪算法CCEP，该算法通过EAS分别将过滤器修剪到每一层中的过滤器。也就是说，CCEP通过划分和诱使策略降低了修剪空间。该实验表明，CCEP可以通过最先进的修剪方法实现竞争性能，例如，Prune Resnet56的价格为$ 63.42 \％$ $ $ $ $ $ $ $ $ $ $ $ -0.24 \％\％\％$ $ $ $准确性下降，并以$ 44.56 \％\％的价格重新确定ImageNet上的$ flops，$ 0.07 \％$ $精度下降。,https://arxiv.org/abs/2204.05639,IJCAI,True,False,False,False
1676,Model-Based Offline Planning with Trajectory Pruning.,"Xianyuan Zhan, Xiangyu Zhu, Haoran Xu","Offline reinforcement learning (RL) enables learning policies using pre-collected datasets without environment interaction, which provides a promising direction to make RL useable in real-world systems. Although recent offline RL studies have achieved much progress, existing methods still face many practical challenges in real-world system control tasks, such as computational restriction during agent training and the requirement of extra control flexibility. Model-based planning framework provides an attractive solution for such tasks. However, most model-based planning algorithms are not designed for offline settings. Simply combining the ingredients of offline RL with existing methods either provides over-restrictive planning or leads to inferior performance. We propose a new light-weighted model-based offline planning framework, namely MOPP, which tackles the dilemma between the restrictions of offline learning and high-performance planning. MOPP encourages more aggressive trajectory rollout guided by the behavior policy learned from data, and prunes out problematic trajectories to avoid potential out-of-distribution samples. Experimental results show that MOPP provides competitive performance compared with existing model-based offline planning and RL approaches, and allows easy adaptation to varying objectives and extra constraints.",基于型号的离线计划，轨迹修剪。,离线增强学习（RL）可以使用未经环境互动的预采用数据集进行学习政策，这提供了一个有希望的方向，使RL可在现实世界中使用。尽管最近的离线RL研究取得了很大的进步，但现有方法仍然面临现实世界中系统控制任务的许多实际挑战，例如代理培训期间的计算限制和对额外控制灵活性的要求。基于模型的计划框架为此类任务提供了一个有吸引力的解决方案。但是，大多数基于模型的计划算法不是为离线设置而设计的。只需将离线RL的成分与现有方法相结合，或者可以提供过度限制性的计划或导致劣等性能。我们提出了一个新的基于轻度模型的离线计划框架，即MOPP，该框架解决了离线学习限制和高性能计划之间的困境。MOPP鼓励以从数据中学到的行为政策为指导的指导性的更积极的轨迹推出，并修剪出有问题的轨迹，以避免潜在的分发样本。实验结果表明，与现有的基于模型的离线计划和RL方法相比，MOPP提供了竞争性能，并可以轻松适应各种目标和额外的约束。,https://arxiv.org/abs/2105.07351,IJCAI,True,False,False,False
1677,Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation.,"Chunbo Lang, Binfei Tu, Gong Cheng, Junwei Han","Few-shot segmentation, which aims to segment unseen-class objects given only a handful of densely labeled samples, has received widespread attention from the community. Existing approaches typically follow the prototype learning paradigm to perform meta-inference, which fails to fully exploit the underlying information from support image-mask pairs, resulting in various segmentation failures, e.g., incomplete objects, ambiguous boundaries, and distractor activation. To this end, we propose a simple yet versatile framework in the spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is first implemented on the annotated support image, and then the coarse segmentation mask is divided into multiple regions with different properties. Leveraging effective masked average pooling operations, a series of support-induced proxies are thus derived, each playing a specific role in conquering the above challenges. Moreover, we devise a unique parallel decoder structure that integrates proxies with similar attributes to boost the discrimination power. Our proposed approach, named divide-and-conquer proxies (DCP), allows for the development of appropriate and reliable information as a guide at the ""episode"" level, not just about the object cues themselves. Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of DCP over conventional prototype-based approaches (up to 5~10% on average), which also establishes a new state-of-the-art. Code is available at github.com/chunbolang/DCP.",超越原型：分隔段的划分和拼接代理。,几乎没有射击细分，旨在仅给出少数标记的样品的看不见的对象，从而受到社区的广泛关注。现有方法通常遵循原型学习范式执行元推导，该元推导无法完全利用支持图像掩码对的基础信息，从而导致各种细分失败，例如，不完整的对象，模棱两可的界限，歧义性界限和分散分散器的激活。为此，我们本着分裂和构造精神提出了一个简单而多功能的框架。具体而言，首先在注释的支持图像上实现了一种新颖的自我调查方案，然后将粗分割掩码分为具有不同属性的多个区域。因此，利用有效的掩盖平均池操作，得出了一系列支持引起的代理，每个代理都在征服上述挑战方面发挥了特定的作用。此外，我们设计了一个独特的并行解码器结构，该结构将代理集成具有相似属性以提高歧视能力。我们所提出的方法，名为“分裂和争议代理”（DCP），允许开发适当可靠的信息作为“情节”级别的指南，而不仅仅是对象提示本身。关于Pascal-5i和Coco-20i的广泛实验表明，DCP优于基于常规原型的方法（平均最高5〜10％），这也建立了新的最新技术。代码可在github.com/chunbolang/dcp上找到。,https://arxiv.org/abs/2204.09903,IJCAI,True,False,False,False
1678,DeepExtrema: A Deep Learning Approach for Forecasting Block Maxima in Time Series Data.,"Asadullah Hill Galib, Andrew McDonald, Tyler Wilson, Lifeng Luo, Pang-Ning Tan","Accurate forecasting of extreme values in time series is critical due to the significant impact of extreme events on human and natural systems. This paper presents DeepExtrema, a novel framework that combines a deep neural network (DNN) with generalized extreme value (GEV) distribution to forecast the block maximum value of a time series. Implementing such a network is a challenge as the framework must preserve the inter-dependent constraints among the GEV model parameters even when the DNN is initialized. We describe our approach to address this challenge and present an architecture that enables both conditional mean and quantile prediction of the block maxima. The extensive experiments performed on both real-world and synthetic data demonstrated the superiority of DeepExtrema compared to other baseline methods.",DeepExtrema：预测时间序列数据中最大值的深度学习方法。,translate error!,https://arxiv.org/abs/2205.02441,IJCAI,True,False,True,False
1679,Online Hybrid Lightweight Representations Learning: Its Application to Visual Tracking.,"Ilchae Jung, Minji Kim, Eunhyeok Park, Bohyung Han","This paper presents a novel hybrid representation learning framework for streaming data, where an image frame in a video is modeled by an ensemble of two distinct deep neural networks; one is a low-bit quantized network and the other is a lightweight full-precision network. The former learns coarse primary information with low cost while the latter conveys residual information for high fidelity to original representations. The proposed parallel architecture is effective to maintain complementary information since fixed-point arithmetic can be utilized in the quantized network and the lightweight model provides precise representations given by a compact channel-pruned network. We incorporate the hybrid representation technique into an online visual tracking task, where deep neural networks need to handle temporal variations of target appearances in real-time. Compared to the state-of-the-art real-time trackers based on conventional deep neural networks, our tracking algorithm demonstrates competitive accuracy on the standard benchmarks with a small fraction of computational cost and memory footprint.",在线混合轻量级表示学习：其应用于视觉跟踪。,本文提出了一个新型的混合表示学习框架，用于流数据，其中视频中的图像框架由两个不同的深神经网络组成。一个是一个低位量化的网络，另一个是轻质的全精度网络。前者以低成本学习粗略的主要信息，而后者则传达了剩余信息，以高保真对原始表示形式。所提出的并行体系结构可有效地维护互补信息，因为可以在量化网络中使用固定点算术，并且轻量级模型提供了紧凑的通道键入网络给出的精确表示。我们将混合表示技术纳入在线视觉跟踪任务中，其中深层神经网络需要实时处理目标外观的时间变化。与基于常规深神经网络的最先进的实时跟踪器相比，我们的跟踪算法在标准基准测试基准上表现出竞争精度，其计算成本和内存足迹的一小部分。,https://arxiv.org/abs/2205.11179,IJCAI,True,False,False,False
1680,Efficient Neural Neighborhood Search for Pickup and Delivery Problems.,"Yining Ma, Jingwen Li, Zhiguang Cao, Wen Song, Hongliang Guo, Yuejiao Gong, Yeow Meng Chee","We present an efficient Neural Neighborhood Search (N2S) approach for pickup and delivery problems (PDPs). In specific, we design a powerful Synthesis Attention that allows the vanilla self-attention to synthesize various types of features regarding a route solution. We also exploit two customized decoders that automatically learn to perform removal and reinsertion of a pickup-delivery node pair to tackle the precedence constraint. Additionally, a diversity enhancement scheme is leveraged to further ameliorate the performance. Our N2S is generic, and extensive experiments on two canonical PDP variants show that it can produce state-of-the-art results among existing neural methods. Moreover, it even outstrips the well-known LKH3 solver on the more constrained PDP variant. Our implementation for N2S is available online.",有效的神经社区搜索接送和送货问题。,translate error!,https://arxiv.org/abs/2204.11399,IJCAI,True,False,False,False
1681,Using Constraint Programming and Graph Representation Learning for Generating Interpretable Cloud Security Policies.,"Mikhail Kazdagli, Mohit Tiwari, Akshat Kumar","Modern software systems rely on mining insights from business sensitive data stored in public clouds. A data breach usually incurs significant (monetary) loss for a commercial organization. Conceptually, cloud security heavily relies on Identity Access Management (IAM) policies that IT admins need to properly configure and periodically update. Security negligence and human errors often lead to misconfiguring IAM policies which may open a backdoor for attackers. To address these challenges, first, we develop a novel framework that encodes generating optimal IAM policies using constraint programming (CP). We identify reducing dark permissions of cloud users as an optimality criterion, which intuitively implies minimizing unnecessary datastore access permissions. Second, to make IAM policies interpretable, we use graph representation learning applied to historical access patterns of users to augment our CP model with similarity constraints: similar users should be grouped together and share common IAM policies. Third, we describe multiple attack models and show that our optimized IAM policies significantly reduce the impact of security attacks using real data from 8 commercial organizations, and synthetic instances.",使用约束编程和图形表示学习来生成可解释的云安全策略。,现代软件系统依赖于存储在公共云中的业务敏感数据的采矿见解。数据泄露通常会给商业组织带来重大（货币）损失。从概念上讲，云安全性在很大程度上依赖于身份访问管理（IAM）策略，其策略需要正确配置和定期更新。安全的疏忽和人为错误通常会导致IAM政策错误配置，这可能为攻击者打开后门。为了解决这些挑战，首先，我们开发了一个新颖的框架，该框架使用约束编程（CP）编码生成最佳IAM策略。我们将减少云用户的黑暗权限确定为最佳标准，这直觉上意味着最大程度地减少了不必要的数据存储访问权限。其次，为了使IAM策略可解释，我们使用应用于用户的历史访问模式的图表学习来增强我们的CP模型具有相似性约束：相似的用户应分组在一起并共享常见的IAM策略。第三，我们描述了多个攻击模型，并表明我们优化的IAM政策可大大减少使用来自8个商业组织和合成实例的真实数据的安全攻击的影响。,https://arxiv.org/abs/2205.01240,IJCAI,True,False,False,False
1682,Strategy Proof Mechanisms for Facility Location with Capacity Limits,Toby Walsh,"An important feature of many real world facility location problems are capacity limits on the facilities. We show here how capacity constraints make it harder to design strategy proof mechanisms for facility location, but counter-intuitively can improve the guarantees on how well we can approximate the optimal solution.",设施位置具有容量限制的策略证明机制,许多现实世界设施位置问题的一个重要特征是设施的容量限制。我们在这里展示了容量限制如何使为设施位置设计策略证明机制变得更加困难，但是违反直觉可以改善我们能够近似最佳解决方案的保证。,https://arxiv.org/abs/2009.07986,IJCAI,True,False,False,False
1683,MotionMixer: MLP-based 3D Human Body Pose Forecasting.,"Arij Bouazizi, Adrian Holzbock, Ulrich Kressel, Klaus Dietmayer, Vasileios Belagiannis","In this work, we present MotionMixer, an efficient 3D human body pose forecasting model based solely on multi-layer perceptrons (MLPs). MotionMixer learns the spatial-temporal 3D body pose dependencies by sequentially mixing both modalities. Given a stacked sequence of 3D body poses, a spatial-MLP extracts fine grained spatial dependencies of the body joints. The interaction of the body joints over time is then modelled by a temporal MLP. The spatial-temporal mixed features are finally aggregated and decoded to obtain the future motion. To calibrate the influence of each time step in the pose sequence, we make use of squeeze-and-excitation (SE) blocks. We evaluate our approach on Human3.6M, AMASS, and 3DPW datasets using the standard evaluation protocols. For all evaluations, we demonstrate state-of-the-art performance, while having a model with a smaller number of parameters. Our code is available at: https://github.com/MotionMLP/MotionMixer",MotionMixer：基于MLP的3D人体姿势预测。,在这项工作中，我们提出了MotionMixer，这是一个有效的3D人体姿势预测模型，仅基于多层感知器（MLP）。MotionMixer通过顺序混合这两种方式来学习时空3D身体姿势依赖性。给定3D身体姿势的堆叠序列，空间MLP提取物是身体关节的细粒空间依赖性。然后，随着时间的推移，身体关节的相互作用由时间MLP建模。最终将时空混合特征汇总并解码以获得未来的运动。为了校准姿势序列中每个时间步的影响，我们利用挤压和兴奋（SE）块。我们使用标准评估协议评估了36M，Amass和3DPW数据集的方法。对于所有评估，我们展示了最先进的性能，同时具有具有较少参数的模型。我们的代码可在以下网址找到：https：//github.com/motionmlp/motionmixer,https://arxiv.org/abs/2207.00499,IJCAI,True,False,False,False
1684,Copy Motion From One to Another: Fake Motion Video Generation.,"Zhenguang Liu, Sifan Wu, Chejian Xu, Xiang Wang, Lei Zhu, Shuang Wu, Fuli Feng","One compelling application of artificial intelligence is to generate a video of a target person performing arbitrary desired motion (from a source person). While the state-of-the-art methods are able to synthesize a video demonstrating similar broad stroke motion details, they are generally lacking in texture details. A pertinent manifestation appears as distorted face, feet, and hands, and such flaws are very sensitively perceived by human observers. Furthermore, current methods typically employ GANs with a L2 loss to assess the authenticity of the generated videos, inherently requiring a large amount of training samples to learn the texture details for adequate video generation. In this work, we tackle these challenges from three aspects: 1) We disentangle each video frame into foreground (the person) and background, focusing on generating the foreground to reduce the underlying dimension of the network output. 2) We propose a theoretically motivated Gromov-Wasserstein loss that facilitates learning the mapping from a pose to a foreground image. 3) To enhance texture details, we encode facial features with geometric guidance and employ local GANs to refine the face, feet, and hands. Extensive experiments show that our method is able to generate realistic target person videos, faithfully copying complex motions from a source person. Our code and datasets are released at https://github.com/Sifann/FakeMotion",将运动从一个复制到另一个：假运动视频生成。,人工智能的一种令人信服的应用是生成一个目标人执行任意所需运动的视频（来自来源的人）。虽然最新的方法能够合成一个视频，展示了类似的宽带运动细节，但它们通常缺乏纹理细节。相关的表现出现为扭曲的脸，脚和手，这种缺陷是人类观察者对人的非常敏感的。此外，当前的方法通常采用L2损失的GAN来评估生成的视频的真实性，固有地需要大量的培训样品来学习纹理细节以进行足够的视频生成。在这项工作中，我们从三个方面应对这些挑战：1）我们将每个视频框架分解为前景（人）和背景，重点是生成前景，以减少网络输出的基本维度。2）我们提出了一种理论上动机的Gromov-Wasserstein损失，可促进从姿势到前景图像学习地图。3）为了增强纹理细节，我们用几何指导编码面部特征，并使用当地甘斯来完善面部，脚和手。广泛的实验表明，我们的方法能够生成现实的目标人视频，忠实地从源人员那里复制复杂的动作。我们的代码和数据集在https://github.com/sifann/fakemotion上发布,https://arxiv.org/abs/2205.01373,IJCAI,True,False,False,False
1685,Investigating and Explaining the Frequency Bias in Image Classification.,"ZhiYu Lin, YiFei Gao, JiTao Sang","CNNs exhibit many behaviors different from humans, one of which is the capability of employing high-frequency components. This paper discusses the frequency bias phenomenon in image classification tasks: the high-frequency components are actually much less exploited than the low- and mid-frequency components. We first investigate the frequency bias phenomenon by presenting two observations on feature discrimination and learning priority. Furthermore, we hypothesize that (i) the spectral density, (ii) class consistency directly affect the frequency bias. Specifically, our investigations verify that the spectral density of datasets mainly affects the learning priority, while the class consistency mainly affects the feature discrimination.",研究和解释图像分类的频率偏差。,CNN表现出与人类不同的许多行为，其中之一是采用高频组件的能力。本文讨论了图像分类任务中的频率偏差现象：高频组件实际上比低频和中频组件的利用要少得多。我们首先通过提出有关特征歧视和学习优先级的两个观察结果来研究频率偏差现象。此外，我们假设（i）光谱密度，（ii）类一致性直接影响频率偏差。具体而言，我们的研究验证数据集的光谱密度主要影响学习优先级，而课程一致性主要影响特征歧视。,https://arxiv.org/abs/2205.03154,IJCAI,True,False,False,False
1686,Self-supervised Graph Neural Networks for Multi-behavior Recommendation.,"Shuyun Gu, Xiao Wang, Chuan Shi, Ding Xiao","Traditional recommendation usually focuses on utilizing only one target user behavior (e.g., purchase) but ignoring other auxiliary behaviors (e.g., click, add to cart). Early efforts of multi-behavior recommendation often emphasize the differences between multiple behaviors, i.e., they aim to extract useful information by distinguishing different behaviors. However, the commonality between them, which reflects user’s common preference for items associated with different behaviors, is largely ignored. Meanwhile, the multi-behavior recommendation still severely suffers from limited supervision signal issue. In this paper, we propose a novel self-supervised graph collaborative filtering model for multi-behavior recommendation named S-MBRec. Specifically, for each behavior, we execute the GCNs to learn the user and item embeddings. Then we design a supervised task, distinguishing the importance of different behaviors, to capture the differences between embeddings. Meanwhile, we propose a star-style contrastive learning task to capture the embedding commonality between target and auxiliary behaviors, so as to alleviate the sparsity of supervision signal, reduce the redundancy among auxiliary behavior, and extract the most critical information. Finally, we jointly optimize the above two tasks. Extensive experiments, in comparison with state-of-the-arts, well demonstrate the effectiveness of S-MBRec, where the maximum improvement can reach to 20%.",自我监督的图形神经网络，用于多行为建议。,传统建议通常专注于仅利用一种目标用户行为（例如购买），但忽略了其他辅助行为（例如，单击，添加到购物车）。多行为建议的早期努力通常强调多种行为之间的差异，即，它们旨在通过区分不同的行为来提取有用的信息。但是，它们之间的共同点反映了用户对与不同行为相关的项目的共同偏好，这在很大程度上被忽略了。同时，多行为建议仍然严重遭受有限的监督信号问题。在本文中，我们提出了一种新型的自我监督图协作过滤模型，用于多个行为建议，名为S-MBREC。具体来说，对于每种行为，我们执行GCN来学习用户和项目嵌入。然后，我们设计了一个有监督的任务，区分不同行为的重要性，以捕获嵌入之间的差异。同时，我们提出了一项星式的对比学习任务，以捕获目标和辅助行为之间的嵌入共同点，以减轻监督信号的稀疏性，减少辅助行为之间的冗余，并提取最关键的信息。最后，我们共同优化上述两个任务。与最先进的实验相比，广泛的实验很好地证明了S-MBREC的有效性，其中最大改善可以达到20％。,http://shichuan.org/doc/134.pdf,IJCAI,True,False,False,False
1687,"Function-words Enhanced Attention Networks for Few-Shot Inverse Relation
  Classification","Chunliu Dou, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng, Kewen Wang","The relation classification is to identify semantic relations between two entities in a given text. While existing models perform well for classifying inverse relations with large datasets, their performance is significantly reduced for few-shot learning. In this paper, we propose a function words adaptively enhanced attention framework (FAEA) for few-shot inverse relation classification, in which a hybrid attention model is designed to attend class-related function words based on meta-learning. As the involvement of function words brings in significant intra-class redundancy, an adaptive message passing mechanism is introduced to capture and transfer inter-class differences.We mathematically analyze the negative impact of function words from dot-product measurement, which explains why message passing mechanism effectively reduces the impact. Our experimental results show that FAEA outperforms strong baselines, especially the inverse relation accuracy is improved by 14.33% under 1-shot setting in FewRel1.0.","功能字可以增强注意力网络，以造成几击逆关系
  分类",关系分类是确定给定文本中两个实体之间的语义关系。尽管现有模型在与大数据集的逆关系分类方面表现良好，但对于很少的学习，它们的性能大大降低。在本文中，我们提出了一个函数词，以适应增强的注意框架（FAEA），以进行几次逆关系分类，其中混合注意模型被设计为基于元学习的类别相关的函数词。随着功能单词的参与带来了重要的级内冗余，引入了自适应消息传递机制以捕获和转移阶层间的差异。我们数学上分析了点 - 产品测量的功能单词的负面影响，这解释了为什么消息传递信息传递的原因机制有效地减少了影响。我们的实验结果表明，在Ligrel1.0中，FAEA胜过强大的基准，尤其是在1次设置以下的相反关系精度上提高了14.33％。,https://arxiv.org/abs/2204.12111,IJCAI,True,False,False,False
1688,CAT: Customized Adversarial Training for Improved Robustness,"Cheng Minhao, Lei Qi, Chen Pin-Yu, Dhillon Inderjit, Hsieh Cho-Jui","Adversarial training has become one of the most effective methods for improving robustness of neural networks. However, it often suffers from poor generalization on both clean and perturbed data. In this paper, we propose a new algorithm, named Customized Adversarial Training (CAT), which adaptively customizes the perturbation level and the corresponding label for each training sample in adversarial training. We show that the proposed algorithm achieves better clean and robust accuracy than previous adversarial training methods through extensive experiments.",猫：定制的对抗训练，以改善鲁棒性,对抗性训练已成为改善神经网络鲁棒性的最有效方法之一。但是，它通常遭受清洁和干扰数据的概括不良。在本文中，我们提出了一种名为“定制对抗训练”（CAT）的新算法，该算法可自适应地自定义扰动水平和对抗培训中每个培训样本的相应标签。我们表明，与以前的对抗性训练方法相比，提出的算法可以通过广泛的实验实现更好和稳健的精度。,https://arxiv.org/abs/2002.06789,IJCAI,True,False,False,False
1689,Rethinking the Setting of Semi-supervised Learning on Graphs.,"Ziang Li, Ming Ding, Weikai Li, Zihan Wang, Ziyu Zeng, Yukuo Cen, Jie Tang","We argue that the present setting of semisupervised learning on graphs may result in unfair comparisons, due to its potential risk of over-tuning hyper-parameters for models. In this paper, we highlight the significant influence of tuning hyper-parameters, which leverages the label information in the validation set to improve the performance. To explore the limit of over-tuning hyperparameters, we propose ValidUtil, an approach to fully utilize the label information in the validation set through an extra group of hyper-parameters. With ValidUtil, even GCN can easily get high accuracy of 85.8% on Cora.   To avoid over-tuning, we merge the training set and the validation set and construct an i.i.d. graph benchmark (IGB) consisting of 4 datasets. Each dataset contains 100 i.i.d. graphs sampled from a large graph to reduce the evaluation variance. Our experiments suggest that IGB is a more stable benchmark than previous datasets for semisupervised learning on graphs.",重新考虑在图表上半监督学习的设置。,我们认为，由于模型过度调整超参数的潜在风险，因此在图上进行半佩里的学习的当前设置可能会导致不公平的比较。在本文中，我们强调了调整超参数的重要影响，该参数利用验证设置中的标签信息来提高性能。为了探索过度调整超参数的限制，我们提出了有效的方法，这是一种通过额外的超参数组在验证设置中充分利用标签信息的方法。有了有效性，即使是GCN，Cora也很容易获得85.8％的高精度。为了避免过度调整，我们合并培训集以及验证集并构建I.I.D.图形基准（IGB）由4个数据集组成。每个数据集包含100 i.i.d.从大图采样的图表以减少评估差异。我们的实验表明，IGB比以前的数据集更稳定，用于图形上的半佩斯学习。,https://arxiv.org/abs/2205.14403,IJCAI,True,False,False,False
1690,BiFSMN: Binary Neural Network for Keyword Spotting.,"Haotong Qin, Xudong Ma, Yifu Ding, Xiaoyang Li, Yang Zhang, Yao Tian, Zejun Ma, Jie Luo, Xianglong Liu","The deep neural networks, such as the Deep-FSMN, have been widely studied for keyword spotting (KWS) applications. However, computational resources for these networks are significantly constrained since they usually run on-call on edge devices. In this paper, we present BiFSMN, an accurate and extreme-efficient binary neural network for KWS. We first construct a High-frequency Enhancement Distillation scheme for the binarization-aware training, which emphasizes the high-frequency information from the full-precision network's representation that is more crucial for the optimization of the binarized network. Then, to allow the instant and adaptive accuracy-efficiency trade-offs at runtime, we also propose a Thinnable Binarization Architecture to further liberate the acceleration potential of the binarized network from the topology perspective. Moreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8 devices which fully utilizes registers and increases instruction throughput to push the limit of deployment efficiency. Extensive experiments show that BiFSMN outperforms existing binarization methods by convincing margins on various datasets and is even comparable with the full-precision counterpart (e.g., less than 3% drop on Speech Commands V1-12). We highlight that benefiting from the thinnable architecture and the optimized 1-bit implementation, BiFSMN can achieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge hardware.",BIFSMN：用于关键字发现的二进制神经网络。,深处神经网络（例如Deep-FSMN）已被广泛研究以用于关键字发现（KWS）应用。但是，这些网络的计算资源通常受到重大限制，因为它们通常在边缘设备上在通话中运行。在本文中，我们提出了BIFSMN，这是KWS的准确且极高的二元神经网络。我们首先为双向意识训练构建了高频增强蒸馏方案，该方案强调了全优先网络表示的高频信息，这对于对二进制网络的优化更为重要。然后，为了在运行时允许即时和自适应的准确性效率折衷，我们还提出了一个可稀薄的二进制架构，以从拓扑角度进一步解放二进制网络的加速潜力。此外，我们在ARMV8设备上为BIFSMN实施了快速的位计算内核，该内核充分利用了寄存器并增加了指令吞吐量以突破部署效率的极限。广泛的实验表明，BIFSMN通过说服各种数据集的利润率优于现有的二进制方法，甚至与全精度对应物相当（例如，语音命令v1-12下降少于3％）。我们强调的是，BIFSMN受益于稀薄的体系结构和优化的1位实现，可以在现实世界中的Edge硬件上实现令人印象深刻的22.3倍加速和15.5倍的存储空间。,https://arxiv.org/abs/2202.06483,IJCAI,True,False,False,False
1691,RoSA: A Robust Self-Aligned Framework for Node-Node Graph Contrastive Learning.,"Yun Zhu, Jianhao Guo, Fei Wu, Siliang Tang","Graph contrastive learning has gained significant progress recently. However, existing works have rarely explored non-aligned node-node contrasting. In this paper, we propose a novel graph contrastive learning method named RoSA that focuses on utilizing non-aligned augmented views for node-level representation learning. First, we leverage the earth mover's distance to model the minimum effort to transform the distribution of one view to the other as our contrastive objective, which does not require alignment between views. Then we introduce adversarial training as an auxiliary method to increase sampling diversity and enhance the robustness of our model. Experimental results show that RoSA outperforms a series of graph contrastive learning frameworks on homophilous, non-homophilous and dynamic graphs, which validates the effectiveness of our work. To the best of our awareness, RoSA is the first work focuses on the non-aligned node-node graph contrastive learning problem. Our codes are available at: \href{https://github.com/ZhuYun97/RoSA}{\texttt{https://github.com/ZhuYun97/RoSA}}",ROSA：用于节点节点图对比度学习的强大自我对准框架。,图对比度学习最近取得了重大进展。但是，现有作品很少探索非对准节点节点的对比。在本文中，我们提出了一种名为Rosa的新颖图形对比学习方法，该方法的重点是利用不结盟的增强视图来用于节点级表示学习。首先，我们利用地球推动者的距离来模拟最低限度的努力，以将一种视图的分布转换为另一种观点作为我们的对比目标，这不需要在视图之间对齐。然后，我们将对抗性训练作为一种辅助方法，以增加采样多样性并增强模型的鲁棒性。实验结果表明，ROSA的表现优于同质，非双重动态和动态图的一系列图形对比学习框架，这些框架验证了我们工作的有效性。为了我们的意识，罗莎（Rosa）是第一项工作，重点是非对准节点图形对比度学习问题。我们的代码可在：\ href {https://github.com/zhuyun97/rosa} {\ texttt {https://github.com/zhuyun97/rosa}}},https://arxiv.org/abs/2204.13846,IJCAI,True,False,False,False
1692,Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features.,"Xinyi Shang, Yang Lu, Gang Huang, Hanzi Wang","Federated learning (FL) provides a privacy-preserving solution for distributed machine learning tasks. One challenging problem that severely damages the performance of FL models is the co-occurrence of data heterogeneity and long-tail distribution, which frequently appears in real FL applications. In this paper, we reveal an intriguing fact that the biased classifier is the primary factor leading to the poor performance of the global model. Motivated by the above finding, we propose a novel and privacy-preserving FL method for heterogeneous and long-tailed data via Classifier Re-training with Federated Features (CReFF). The classifier re-trained on federated features can produce comparable performance as the one re-trained on real data in a privacy-preserving manner without information leakage of local data or class distribution. Experiments on several benchmark datasets show that the proposed CReFF is an effective solution to obtain a promising FL model under heterogeneous and long-tailed data. Comparative results with the state-of-the-art FL methods also validate the superiority of CReFF. Our code is available at https://github.com/shangxinyi/CReFF-FL.",通过与联合功能进行分类器重新培训对异质和长尾数据进行联合学习。,联合学习（FL）为分布式机器学习任务提供了隐私的解决方案。严重损害FL模型的性能的一个具有挑战性的问题是数据异质性和长尾分布的同时存在，这些分布经常出现在实际的FL应用中。在本文中，我们揭示了一个有趣的事实，即有偏见的分类器是导致全球模型表现不佳的主要因素。在上述发现的激励下，我们提出了一种新颖和隐私的FL方法，用于通过与联合特征（Creff）进行分类器重新培训的分类器进行异质和长尾数据。在联合功能上重新训练的分类器可以与以隐私性的方式在实际数据上重新训练的性能相当，而无需信息泄漏本地数据或类别分布。几个基准数据集的实验表明，所提出的Creff是在异质和长尾数据下获得有希望的FL模型的有效解决方案。最先进的FL方法的比较结果也验证了Creff的优越性。我们的代码可在https://github.com/shangxinyi/creff-fl上找到。,https://arxiv.org/abs/2204.13399,IJCAI,True,False,False,False
1693,Neural Re-ranking in Multi-stage Recommender Systems: A Review.,"Weiwen Liu, Yunjia Xi, Jiarui Qin, Fei Sun, Bo Chen, Weinan Zhang, Rui Zhang, Ruiming Tang","As the final stage of the multi-stage recommender system (MRS), re-ranking directly affects user experience and satisfaction by rearranging the input ranking lists, and thereby plays a critical role in MRS. With the advances in deep learning, neural re-ranking has become a trending topic and been widely applied in industrial applications. This review aims at integrating re-ranking algorithms into a broader picture, and paving ways for more comprehensive solutions for future research. For this purpose, we first present a taxonomy of current methods on neural re-ranking. Then we give a description of these methods along with the historic development according to their objectives. The network structure, personalization, and complexity are also discussed and compared. Next, we provide benchmarks of the major neural re-ranking models and quantitatively analyze their re-ranking performance. Finally, the review concludes with a discussion on future prospects of this field. A list of papers discussed in this review, the benchmark datasets, our re-ranking library LibRerank, and detailed parameter settings are publicly available at https://github.com/LibRerank-Community/LibRerank.",多阶段推荐系统中的神经重新排列：评论。,作为多阶段推荐系统（MRS）的最后阶段，重新排名直接通过重新排列输入排名列表来直接影响用户体验和满意度，从而在MRS中起着至关重要的作用。随着深度学习的进步，神经重新排列已成为一个热门话题，并广泛应用于工业应用中。这篇评论旨在将重新排列算法整合到更广泛的情况下，并为未来研究的更全面的解决方案铺平方法。为此，我们首先介绍了有关神经重新排列的当前方法的分类法。然后，我们根据其目标对这些方法以及历史发展进行描述。还讨论和比较了网络结构，个性化和复杂性。接下来，我们提供主要神经重新排列模型的基准，并定量分析其重新排列的性能。最后，审查以讨论该领域的未来前景的讨论结束。本评论中讨论的论文列表，基准数据集，我们的重新排列库库以及详细的参数设置，请访问https://github.com/librerrank-community/librerank。,https://arxiv.org/abs/2202.06602,IJCAI,True,False,True,False
1694,Markov Abstractions for PAC Reinforcement Learning in Non-Markov Decision Processes.,"Alessandro Ronca, Gabriel Paludo Licks, Giuseppe De Giacomo","Our work aims at developing reinforcement learning algorithms that do not rely on the Markov assumption. We consider the class of Non-Markov Decision Processes where histories can be abstracted into a finite set of states while preserving the dynamics. We call it a Markov abstraction since it induces a Markov Decision Process over a set of states that encode the non-Markov dynamics. This phenomenon underlies the recently introduced Regular Decision Processes (as well as POMDPs where only a finite number of belief states is reachable). In all such kinds of decision process, an agent that uses a Markov abstraction can rely on the Markov property to achieve optimal behaviour. We show that Markov abstractions can be learned during reinforcement learning. For these two tasks, any algorithms satisfying some basic requirements can be employed. We show that our approach has PAC guarantees when the employed algorithms have PAC guarantees, and we also provide an experimental evaluation.",Markov在非马尔科夫决策过程中用于PAC增强学习的抽象。,我们的工作旨在开发不依赖马尔可夫假设的强化学习算法。我们考虑了非马尔科夫决策过程的类别，在保留动力学的同时，可以将历史抽象成有限状态。我们称其为马尔可夫抽象，因为它在编码非马科夫动力学的一组状态上诱导了马尔可夫决策过程。这种现象是最近引入的定期决策过程（以及只有有限数量的信念状态的POMDP）。在所有这样的决策过程中，使用马尔可夫抽象的代理可以依靠马尔可夫属性来实现最佳行为。我们表明，马尔可夫抽象可以在强化学习过程中学习。对于这两个任务，可以采用任何满足某些基本要求的算法。我们表明，当使用的算法获得PAC保证时，我们的方法可以保证PAC，并且我们还提供了实验评估。,https://arxiv.org/abs/2205.01053,IJCAI,True,False,False,False
1695,Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks.,"Yijun Tian, Chuxu Zhang, Zhichun Guo, Yihong Ma, Ronald Metoyer, Nitesh V. Chawla","Learning effective recipe representations is essential in food studies. Unlike what has been developed for image-based recipe retrieval or learning structural text embeddings, the combined effect of multi-modal information (i.e., recipe images, text, and relation data) receives less attention. In this paper, we formalize the problem of multi-modal recipe representation learning to integrate the visual, textual, and relational information into recipe embeddings. In particular, we first present Large-RG, a new recipe graph data with over half a million nodes, making it the largest recipe graph to date. We then propose Recipe2Vec, a novel graph neural network based recipe embedding model to capture multi-modal information. Additionally, we introduce an adversarial attack strategy to ensure stable learning and improve performance. Finally, we design a joint objective function of node classification and adversarial learning to optimize the model. Extensive experiments demonstrate that Recipe2Vec outperforms state-of-the-art baselines on two classic food study tasks, i.e., cuisine category classification and region prediction. Dataset and codes are available at https://github.com/meettyj/Recipe2Vec.",配方2VEC：使用图神经网络的多模式配方表示学习。,学习有效的配方表示在食品研究中至关重要。与基于图像的食谱检索或学习结构文本嵌入的开发的内容不同，多模式信息（即食谱图像，文本和关系数据）的组合效果受到较少的关注。在本文中，我们正式化了多模式配方表示学习的问题，以将视觉，文本和关系信息整合到食谱嵌入中。特别是，我们首先介绍了大型RG，这是一个具有超过半百万节点的新食谱图数据，使其成为迄今为止最大的配方图。然后，我们提出了配方2VEC，这是一种基于图形神经网络的新型食谱嵌入模型，以捕获多模式信息。此外，我们引入了一种对抗性攻击策略，以确保稳定学习并提高绩效。最后，我们设计了节点分类和对抗性学习的联合目标功能，以优化模型。广泛的实验表明，配方2VEC在两项经典食品研究任务（即美食类别分类和区域预测）上的最先进基线优于最先进的基线。数据集和代码可在https://github.com/meettyj/recipe2vec上找到。,https://arxiv.org/abs/2205.12396,IJCAI,True,False,True,False
1696,RAW-GNN: RAndom Walk Aggregation based Graph Neural Network.,"Di Jin, Rui Wang, Meng Ge, Dongxiao He, Xiang Li, Wei Lin, Weixiong Zhang","Graph-Convolution-based methods have been successfully applied to representation learning on homophily graphs where nodes with the same label or similar attributes tend to connect with one another. Due to the homophily assumption of Graph Convolutional Networks (GCNs) that these methods use, they are not suitable for heterophily graphs where nodes with different labels or dissimilar attributes tend to be adjacent. Several methods have attempted to address this heterophily problem, but they do not change the fundamental aggregation mechanism of GCNs because they rely on summation operators to aggregate information from neighboring nodes, which is implicitly subject to the homophily assumption. Here, we introduce a novel aggregation mechanism and develop a RAndom Walk Aggregation-based Graph Neural Network (called RAW-GNN) method. The proposed approach integrates the random walk strategy with graph neural networks. The new method utilizes breadth-first random walk search to capture homophily information and depth-first search to collect heterophily information. It replaces the conventional neighborhoods with path-based neighborhoods and introduces a new path-based aggregator based on Recurrent Neural Networks. These designs make RAW-GNN suitable for both homophily and heterophily graphs. Extensive experimental results showed that the new method achieved state-of-the-art performance on a variety of homophily and heterophily graphs.",RAW-GNN：基于随机步行聚合的图形神经网络。,基于图形卷积的方法已成功地应用于同质图上的表示学习，其中具有相同标签或相似属性的节点倾向于相互连接。由于这些方法使用的图形卷积网络（GCN）的同义假设，它们不适合异质图，其中具有不同标记或不同属性的节点往往相邻。几种方法试图解决这个异质问题，但是它们没有改变GCN的基本聚合机制，因为它们依靠求和操作员来汇总邻近节点的信息，这隐含地遵守同质假设。在这里，我们介绍了一种新颖的聚合机制，并开发了基于随机步行聚集的图形神经网络（称为RAW-GNN）方法。提出的方法将随机步行策略与图神经网络集成在一起。新方法利用广度优先的随机步行搜索来捕获同质信息和深度优先搜索以收集异性信息。它用基于路径的社区取代了传统社区，并基于经常性神经网络引入了新的基于路径的聚合器。这些设计使RAW-GNN适用于同质图和异质图。广泛的实验结果表明，新方法在各种同质图和异质图上实现了最先进的性能。,https://arxiv.org/abs/2206.13953,IJCAI,True,False,True,False
1697,Enhancing Sequential Recommendation with Graph Contrastive Learning.,"Yixin Zhang, Yong Liu, Yonghui Xu, Hao Xiong, Chenyi Lei, Wei He, Lizhen Cui, Chunyan Miao","The sequential recommendation systems capture users' dynamic behavior patterns to predict their next interaction behaviors. Most existing sequential recommendation methods only exploit the local context information of an individual interaction sequence and learn model parameters solely based on the item prediction loss. Thus, they usually fail to learn appropriate sequence representations. This paper proposes a novel recommendation framework, namely Graph Contrastive Learning for Sequential Recommendation (GCL4SR). Specifically, GCL4SR employs a Weighted Item Transition Graph (WITG), built based on interaction sequences of all users, to provide global context information for each interaction and weaken the noise information in the sequence data. Moreover, GCL4SR uses subgraphs of WITG to augment the representation of each interaction sequence. Two auxiliary learning objectives have also been proposed to maximize the consistency between augmented representations induced by the same interaction sequence on WITG, and minimize the difference between the representations augmented by the global context on WITG and the local representation of the original sequence. Extensive experiments on real-world datasets demonstrate that GCL4SR consistently outperforms state-of-the-art sequential recommendation methods.",通过图形对比度学习增强顺序建议。,顺序推荐系统捕获用户的动态行为模式，以预测其下一个交互行为。大多数现有的顺序推荐方法仅利用单个交互序列的本地上下文信息，并仅根据项目预测损失来学习模型参数。因此，他们通常无法学习适当的顺序表示。本文提出了一个新颖的推荐框架，即顺序推荐的对比度学习（GCL4SR）。具体而言，GCL4SR采用基于所有用户的交互序列构建的加权项目过渡图（WITG），以提供每次交互的全局上下文信息，并削弱序列数据中的噪声信息。此外，GCL4SR使用WITG的子图来增强每个相互作用序列的表示。还提出了两个辅助学习目标，以最大程度地提高由WITG上相同相互作用序列引起的增强表示之间的一致性，并最大程度地减少由全局上下文对WITG和原始序列的局部表示形式增强的差异。对现实世界数据集的广泛实验表明，GCL4SR始终优于最先进的顺序推荐方法。,https://arxiv.org/abs/2205.14837,IJCAI,True,False,False,False
1698,Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training.,"Peide Huang, Mengdi Xu, Fei Fang, Ding Zhao","Robust Reinforcement Learning (RL) focuses on improving performances under model errors or adversarial attacks, which facilitates the real-life deployment of RL agents. Robust Adversarial Reinforcement Learning (RARL) is one of the most popular frameworks for robust RL. However, most of the existing literature models RARL as a zero-sum simultaneous game with Nash equilibrium as the solution concept, which could overlook the sequential nature of RL deployments, produce overly conservative agents, and induce training instability. In this paper, we introduce a novel hierarchical formulation of robust RL - a general-sum Stackelberg game model called RRL-Stack - to formalize the sequential nature and provide extra flexibility for robust training. We develop the Stackelberg Policy Gradient algorithm to solve RRL-Stack, leveraging the Stackelberg learning dynamics by considering the adversary's response. Our method generates challenging yet solvable adversarial environments which benefit RL agents' robust learning. Our algorithm demonstrates better training stability and robustness against different testing conditions in the single-agent robotics control and multi-agent highway merging tasks.",通过自适应的对抗性训练，强大的强化学习是一款Stackelberg游戏。,强大的强化学习（RL）着重于改善模型错误或对抗性攻击下的性能，这有助于RL代理的现实部署。强大的对抗强化学习（RARL）是RL最受欢迎的框架之一。但是，大多数现有的文献模型都以NASH均衡为解决方案概念的零和同时游戏，可以忽略RL部署的顺序性质，产生过度保守的代理，并引起训练不稳定。在本文中，我们介绍了一种新颖的RL RL的新型分层配方，即一种名为RRL -Stack的通用stackelberg游戏模型 - 以形式化顺序性质，并为健壮的训练提供了额外的灵活性。我们开发了Stackelberg策略梯度算法来解决RRL堆栈，通过考虑对手的反应来利用Stackelberg学习动态。我们的方法产生了有挑战性但可解决的对抗环境，这些环境使RL代理的强大学习受益。我们的算法表明，在单权机器人控制和多机科公路合并任务中，针对不同测试条件的训练稳定性和鲁棒性。,https://arxiv.org/abs/2202.09514,IJCAI,True,False,False,False
1699,CGMN: A Contrastive Graph Matching Network for Self-Supervised Graph Similarity Learning.,"Di Jin, Luzhi Wang, Yizhen Zheng, Xiang Li, Fei Jiang, Wei Lin, Shirui Pan","Graph similarity learning refers to calculating the similarity score between two graphs, which is required in many realistic applications, such as visual tracking, graph classification, and collaborative filtering. As most of the existing graph neural networks yield effective graph representations of a single graph, little effort has been made for jointly learning two graph representations and calculating their similarity score. In addition, existing unsupervised graph similarity learning methods are mainly clustering-based, which ignores the valuable information embodied in graph pairs. To this end, we propose a contrastive graph matching network (CGMN) for self-supervised graph similarity learning in order to calculate the similarity between any two input graph objects. Specifically, we generate two augmented views for each graph in a pair respectively. Then, we employ two strategies, namely cross-view interaction and cross-graph interaction, for effective node representation learning. The former is resorted to strengthen the consistency of node representations in two views. The latter is utilized to identify node differences between different graphs. Finally, we transform node representations into graph-level representations via pooling operations for graph similarity computation. We have evaluated CGMN on eight real-world datasets, and the experiment results show that the proposed new approach is superior to the state-of-the-art methods in graph similarity learning downstream tasks.",CGMN：一种用于自我监督的图形相似性学习的对比图匹配网络。,图形相似性学习是指计算两个图之间的相似性得分，这在许多现实的应用程序（例如视觉跟踪，图形分类和协作过滤）中需要。由于大多数现有的图形神经网络产生了单个图的有效图表，因此几乎没有努力共同学习两个图表并计算其相似性得分。此外，现有的无监督图相似性学习方法主要基于聚类，它忽略了图对中体现的有价值的信息。为此，我们提出了一个对比度图匹配网络（CGMN），以进行自我监督的图形相似性学习，以计算任何两个输入图对象之间的相似性。具体而言，我们分别在一对中为每个图生成两个增强视图。然后，我们采用两种策略，即跨视图相互作用和跨刻画相互作用，以实现有效的节点表示学习。前者求助于两种观点中节点表示的一致性。后者用于识别不同图之间的节点差异。最后，我们通过汇总操作进行图形相似性计算将节点表示形式转换为图形表示。我们已经在八个现实世界数据集上评估了CGMN，实验结果表明，所提出的新方法优于图形相似性学习下游任务的最新方法。,https://arxiv.org/abs/2205.15083,IJCAI,True,False,False,False
1700,Penalized Proximal Policy Optimization for Safe Reinforcement Learning.,"Linrui zhang, Li Shen, Long Yang, Shixiang Chen, Bo Yuan, Xueqian Wang, Dacheng Tao","Safe reinforcement learning aims to learn the optimal policy while satisfying safety constraints, which is essential in real-world applications. However, current algorithms still struggle for efficient policy updates with hard constraint satisfaction. In this paper, we propose Penalized Proximal Policy Optimization (P3O), which solves the cumbersome constrained policy iteration via a single minimization of an equivalent unconstrained problem. Specifically, P3O utilizes a simple-yet-effective penalty function to eliminate cost constraints and removes the trust-region constraint by the clipped surrogate objective. We theoretically prove the exactness of the proposed method with a finite penalty factor and provide a worst-case analysis for approximate error when evaluated on sample trajectories. Moreover, we extend P3O to more challenging multi-constraint and multi-agent scenarios which are less studied in previous work. Extensive experiments show that P3O outperforms state-of-the-art algorithms with respect to both reward improvement and constraint satisfaction on a set of constrained locomotive tasks.",对安全加强学习的惩罚近端政策优化。,安全的强化学习旨在学习最佳政策，同时满足安全限制，这在现实世界中至关重要。但是，当前的算法仍在为有效的政策更新而努力，并具有严格的约束满意度。在本文中，我们提出了受惩罚的近端政策优化（P3O），该政策优化（P3O）通过单一的最小化等效不受约束的问题来解决麻烦的受约束政策迭代。具体而言，P3O利用了简单的罚款功能来消除成本限制，并通过剪裁的替代目标消除了信任区域的约束。从理论上讲，我们用有限的惩罚因素证明了所提出的方法的精确性，并在对样品轨迹进行评估时提供了最坏情况分析，以实现近似误差。此外，我们将P3O扩展到更具挑战性的多构造和多代理方案，这些方案在以前的工作中所研究的情况较少。广泛的实验表明，在一组受限的机车任务上，P3O优于奖励改进和约束满意度的最先进算法。,https://arxiv.org/abs/2205.11814,IJCAI,True,False,False,False
1701,Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk.,"Chengyang Ying, Xinning Zhou, Hang Su, Dong Yan, Ning Chen, Jun Zhu","Though deep reinforcement learning (DRL) has obtained substantial success, it may encounter catastrophic failures due to the intrinsic uncertainty of both transition and observation. Most of the existing methods for safe reinforcement learning can only handle transition disturbance or observation disturbance since these two kinds of disturbance affect different parts of the agent; besides, the popular worst-case return may lead to overly pessimistic policies. To address these issues, we first theoretically prove that the performance degradation under transition disturbance and observation disturbance depends on a novel metric of Value Function Range (VFR), which corresponds to the gap in the value function between the best state and the worst state. Based on the analysis, we adopt conditional value-at-risk (CVaR) as an assessment of risk and propose a novel reinforcement learning algorithm of CVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitive constrained optimization problem by keeping its CVaR under a given threshold. Experimental results show that CPPO achieves a higher cumulative reward and is more robust against both observation and transition disturbances on a series of continuous control tasks in MuJoCo.",通过限制条件价值的风险进行安全加强学习。,尽管深度强化学习（DRL）取得了巨大的成功，但由于过渡和观察的内在不确定性，它可能遇到灾难性的失败。大多数现有的安全加固学习方法只能处理过渡干扰或观察障碍，因为这两种干扰影响了代理的不同部分。此外，受欢迎的最坏情况可能会导致过度悲观的政策。为了解决这些问题，我们首先从理论上证明了在过渡干扰和观察障碍下的性能降解取决于一个新颖的价值函数范围（VFR），这与最佳状态和最坏状态之间的价值函数的间隙相对应。基于分析，我们采用有条件的价值风险（CVAR）作为对风险的评估，并提出了一种新颖的强化学习算法的CVAR-Proximal-Policy-oftimization（CPPO），该算法通过保持风险敏感的约束优化问题形式化。它的CVAR在给定的阈值下。实验结果表明，CPPO获得了更高的累积奖励，并且在Mujoco中一系列连续控制任务上的观察和过渡干扰更加强大。,https://arxiv.org/abs/2206.04436,IJCAI,True,False,False,False
1702,Language Models as Knowledge Embeddings.,"Xintao Wang, Qianyu He, Jiaqing Liang, Yanghua Xiao","Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental results show that LMKE achieves state-of-the-art performance on KE benchmarks of link prediction and triple classification, especially for long-tail entities.",语言模型作为知识嵌入。,知识嵌入（KE）通过将实体和关系嵌入连续的向量空间来表示知识图（kg）。现有方法主要基于结构或基于描述。基于结构的方法学习保留KGS固有结构的表示。它们不能很好地代表具有有限结构信息的现实世界中的丰富长尾实体。基于描述的方法利用文本信息和语言模型。朝这个方向迈出的先前方法几乎不能胜过基于结构的结构，并且遇到了昂贵的负面抽样和限制性描述需求等问题。在本文中，我们提出了LMKE，该LMKE采用语言模型来得出知识嵌入，旨在既富集了长尾实体的表示形式又旨在解决先前的基于描述的方法的问题。我们通过对比度学习框架制定基于描述的KE学习，以提高培训和评估的效率。实验结果表明，LMKE在链接预测和三重分类的KE基准上实现了最先进的性能，尤其是对于长尾实体。,https://arxiv.org/abs/2206.12617,IJCAI,True,False,True,False
1703,Deep Video Harmonization With Color Mapping Consistency.,"Xinyuan Lu, Shengyuan Huang, Li Niu, Wenyan Cong, Liqing Zhang","Video harmonization aims to adjust the foreground of a composite video to make it compatible with the background. So far, video harmonization has only received limited attention and there is no public dataset for video harmonization. In this work, we construct a new video harmonization dataset HYouTube by adjusting the foreground of real videos to create synthetic composite videos. Moreover, we consider the temporal consistency in video harmonization task. Unlike previous works which establish the spatial correspondence, we design a novel framework based on the assumption of color mapping consistency, which leverages the color mapping of neighboring frames to refine the current frame. Extensive experiments on our HYouTube dataset prove the effectiveness of our proposed framework. Our dataset and code are available at https://github.com/bcmi/Video-Harmonization-Dataset-HYouTube.",深度视频与颜色映射一致性进行协调。,视频协调旨在调整复合视频的前景，以使其与背景兼容。到目前为止，视频协调仅受到有限的关注，并且没有公共数据集用于视频协调。在这项工作中，我们通过调整真实视频的前景以创建合成复合视频来构建一个新的视频协调数据集Hyoutube。此外，我们考虑视频协调任务中的时间一致性。与以前建立空间对应关系的作品不同，我们根据颜色映射一致性的假设设计了一个新颖的框架，该框架利用相邻帧的颜色映射来完善当前帧。在我们的Houtube数据集上进行的大量实验证明了我们提出的框架的有效性。我们的数据集和代码可在https://github.com/bcmi/video-harmonization-dataset-hyoutube上找到。,https://arxiv.org/abs/2205.00687,IJCAI,True,False,False,False
1704,Long-term Spatio-Temporal Forecasting via Dynamic Multiple-Graph Attention.,"Wei Shao, Zhiling Jin, Shuo Wang, Yufan Kang, Xiao Xiao, Hamid Menouar, Zhaofeng Zhang, Junshan Zhang, Flora Salim","Many real-world ubiquitous applications, such as parking recommendations and air pollution monitoring, benefit significantly from accurate long-term spatio-temporal forecasting (LSTF). LSTF makes use of long-term dependency between spatial and temporal domains, contextual information, and inherent pattern in the data. Recent studies have revealed the potential of multi-graph neural networks (MGNNs) to improve prediction performance. However, existing MGNN methods cannot be directly applied to LSTF due to several issues: the low level of generality, insufficient use of contextual information, and the imbalanced graph fusion approach. To address these issues, we construct new graph models to represent the contextual information of each node and the long-term spatio-temporal data dependency structure. To fuse the information across multiple graphs, we propose a new dynamic multi-graph fusion module to characterize the correlations of nodes within a graph and the nodes across graphs via the spatial attention and graph attention mechanisms. Furthermore, we introduce a trainable weight tensor to indicate the importance of each node in different graphs. Extensive experiments on two large-scale datasets demonstrate that our proposed approaches significantly improve the performance of existing graph neural network models in LSTF prediction tasks.",长期时空预测通过动态多圈的注意。,许多现实世界中普遍存在的应用程序，例如停车建议和空气污染监测，都能从准确的长期时空预测（LSTF）中受益匪浅。LSTF利用了空间和时间域，上下文信息和数据中固有模式之间的长期依赖性。最近的研究揭示了多画望神经网络（MGNN）提高预测性能的潜力。但是，由于几个问题，现有的MGNN方法不能直接应用于LSTF：一般性低，不充分使用上下文信息以及不平衡的图形融合方法。为了解决这些问题，我们构建了新的图形模型，以表示每个节点的上下文信息和长期时空数据依赖性结构。为了融合跨多个图形的信息，我们提出了一个新的动态多绘图融合模块，以通过空间注意力和图形注意机制来表征图中节点和跨图的节点的相关性。此外，我们引入了可训练的重量张量，以指示不同图中每个节点的重要性。在两个大规模数据集上进行的广泛实验表明，我们提出的方法显着改善了LSTF预测任务中现有图形神经网络模型的性能。,https://arxiv.org/abs/2204.11008,IJCAI,True,False,False,False
1705,GOCPT: Generalized Online Canonical Polyadic Tensor Factorization and Completion.,"Chaoqi Yang, Cheng Qian, Jimeng Sun","Low-rank tensor factorization or completion is well-studied and applied in various online settings, such as online tensor factorization (where the temporal mode grows) and online tensor completion (where incomplete slices arrive gradually). However, in many real-world settings, tensors may have more complex evolving patterns: (i) one or more modes can grow; (ii) missing entries may be filled; (iii) existing tensor elements can change. Existing methods cannot support such complex scenarios. To fill the gap, this paper proposes a Generalized Online Canonical Polyadic (CP) Tensor factorization and completion framework (named GOCPT) for this general setting, where we maintain the CP structure of such dynamic tensors during the evolution. We show that existing online tensor factorization and completion setups can be unified under the GOCPT framework. Furthermore, we propose a variant, named GOCPTE, to deal with cases where historical tensor elements are unavailable (e.g., privacy protection), which achieves similar fitness as GOCPT but with much less computational cost. Experimental results demonstrate that our GOCPT can improve fitness by up to 2:8% on the JHU Covid data and 9:2% on a proprietary patient claim dataset over baselines. Our variant GOCPTE shows up to 1:2% and 5:5% fitness improvement on two datasets with about 20% speedup compared to the best model.",GOCPT：广义在线规范的多核张量分解和完成。,低量张量分解或完成已进行了充分研究和应用于各种在线设置，例如在线张量分解（时间模式增长）和在线张量完成（其中不完整的切片逐渐到达）。但是，在许多现实世界中，张量可能具有更复杂的发展模式：（i）一种或多种模式可以增长；（ii）可能会填补缺失的条目；（iii）现有的张量元件可能会改变。现有方法不能支持这种复杂的方案。为了填补空白，本文提出了在这种一般环境中进行广义的在线规范多核（CP）张量分解和完成框架（名为GOCPT），在此过程中，我们在演变过程中维护了这种动态张量的CP结构。我们表明，在GOCPT框架下可以统一现有的在线张量分解和完成设置。此外，我们提出了一个名为Gocpte的变体，以应对不可用的历史张量元素（例如，隐私保护）的情况，该元件的适应性与Gocpt相似，但计算成本却降低了。实验结果表明，我们的GOCPT可以在JHU COVID数据上提高适应性多达2：8％，而专有患者的索赔数据集则可以提高9：2％。与最佳型号相比，我们的变体Gocpte在两个数据集上最多可提高1：2％和5：5％的健身性。,https://arxiv.org/abs/2205.03749,IJCAI,True,False,False,False
1706,DDDM: A Brain-Inspired Framework for Robust Classification.,"Xiyuan Chen, Xingyu Li, Yi Zhou, Tianming Yang","Despite their outstanding performance in a broad spectrum of real-world tasks, deep artificial neural networks are sensitive to input noises, particularly adversarial perturbations. On the contrary, human and animal brains are much less vulnerable. In contrast to the one-shot inference performed by most deep neural networks, the brain often solves decision-making with an evidence accumulation mechanism that may trade time for accuracy when facing noisy inputs. The mechanism is well described by the Drift-Diffusion Model (DDM). In the DDM, decision-making is modeled as a process in which noisy evidence is accumulated toward a threshold. Drawing inspiration from the DDM, we propose the Dropout-based Drift-Diffusion Model (DDDM) that combines test-phase dropout and the DDM for improving the robustness for arbitrary neural networks. The dropouts create temporally uncorrelated noises in the network that counter perturbations, while the evidence accumulation mechanism guarantees a reasonable decision accuracy. Neural networks enhanced with the DDDM tested in image, speech, and text classification tasks all significantly outperform their native counterparts, demonstrating the DDDM as a task-agnostic defense against adversarial attacks.",DDDM：用于智能分类的脑启发框架。,尽管在广泛的现实世界任务中表现出色，但深人造神经网络对输入噪声敏感，尤其是对抗性扰动。相反，人类和动物的大脑易受伤害。与大多数深层神经网络进行的单次推理相反，大脑经常通过证据积累机制解决决策，在面对嘈杂的输入时可能会以准确性交易。该机制由漂移扩散模型（DDM）很好地描述。在DDM中，决策被建模为一个过程，在该过程中，嘈杂的证据被积累到阈值。从DDM中汲取灵感，我们提出了基于辍学的漂移扩散模型（DDDM），该模型结合了测试相辍学和DDM，以改善任意神经网络的鲁棒性。辍学在网络中造成了反对扰动的时间无关的声音，而证据积累机制则保证了合理的决策准确性。神经网络通过在图像，语音和文本分类任务中测试的DDDM增强，这均大大优于其本地对应物，证明了DDDM是针对对抗攻击的任务无关紧要的防御。,https://arxiv.org/abs/2205.10117,IJCAI,True,False,False,False
1707,On Tracking Dialogue State by Inheriting Slot Values in Mentioned Slot Pools.,"Zhoujian Sun, Zhengxing Huang, Nai Ding","Dialogue state tracking (DST) is a component of the task-oriented dialogue system. It is responsible for extracting and managing slot values according to dialogue utterances, where each slot represents an essential part of the information to accomplish a task, and slot value is updated recurrently in each dialogue turn. However, many DST models cannot update slot values appropriately. These models may repeatedly inherit wrong slot values extracted in previous turns, resulting in the fail of the entire DST task.They cannot update indirectly mentioned slots well, either. This study designed a model with a mentioned slot pool (MSP) to tackle the update problem. The MSP is a slot-specific memory that records all mentioned slot values that may be inherited, and our model updates slot values according to the MSP and the dialogue context. Our model rejects inheriting the previous slot value when it predicates the value is wrong. Then, it re-extracts the slot value from the current dialogue context. As the contextual information accumulates with the dialogue progress, the new value is more likely to be correct. It also can track the indirectly mentioned slot by picking a value from the MSP. Experimental results showed our model reached state-of-the-art DST performance on MultiWOZ 2.1 and 2.2 datasets.",通过在上述插槽库中继承插槽值来跟踪对话状态。,对话状态跟踪（DST）是面向任务对话系统的组成部分。它负责根据对话说法提取和管理插槽值，在对话说法中，每个插槽代表完成任务的重要组成部分，并且在每个对话转弯时会反复更新插槽值。但是，许多DST模型无法适当更新插槽值。这些模型可能反复继承了上一轮中提取的错误的插槽值，从而导致整个DST任务的失败。它们也无法间接更新插槽。这项研究设计了一个具有上述老虎机（MSP）的模型，以解决更新问题。MSP是特定于插槽的内存，记录所有可能继承的插槽值，我们的模型根据MSP和对话上下文更新插槽值。我们的模型拒绝以前的插槽值谓词值是错误的。然后，它从当前的对话上下文中重新提取插槽值。随着上下文信息随着对话的进度积累，新值更可能是正确的。它还可以通过从MSP选择值来跟踪间接提到的插槽。实验结果表明，我们的模型达到了多沃兹2.1和2.2数据集的最先进的DST性能。,https://arxiv.org/abs/2202.07156,IJCAI,True,False,True,False
1708,Multi-Player Multi-Armed Bandits with Finite Shareable Resources Arms: Learning Algorithms & Applications.,"Xuchuang Wang, Hong Xie, John C. S. Lui","Multi-player multi-armed bandits (MMAB) study how decentralized players cooperatively play the same multi-armed bandit so as to maximize their total cumulative rewards. Existing MMAB models mostly assume when more than one player pulls the same arm, they either have a collision and obtain zero rewards, or have no collision and gain independent rewards, both of which are usually too restrictive in practical scenarios. In this paper, we propose an MMAB with shareable resources as an extension to the collision and non-collision settings. Each shareable arm has finite shareable resources and a ""per-load"" reward random variable, both of which are unknown to players. The reward from a shareable arm is equal to the ""per-load"" reward multiplied by the minimum between the number of players pulling the arm and the arm's maximal shareable resources. We consider two types of feedback: sharing demand information (SDI) and sharing demand awareness (SDA), each of which provides different signals of resource sharing. We design the DPE-SDI and SIC-SDA algorithms to address the shareable arm problem under these two cases of feedback respectively and prove that both algorithms have logarithmic regrets that are tight in the number of rounds. We conduct simulations to validate both algorithms' performance and show their utilities in wireless networking and edge computing.",具有有限共享资源武器的多武器匪徒：学习算法和应用程序。,多人多军强盗（MMAB）研究分散的玩家如何合作地演奏相同的多臂强盗，以最大程度地提高其总累积奖励。现有的MMAB模型主要假设当多个玩家拉上同一手臂时，他们要么有碰撞并获得零奖励，要么没有碰撞并获得独立的奖励，而在实际情况下，这两者通常都过于限制。在本文中，我们提出了一个具有可共享资源的MMAB，以扩展碰撞和非碰撞设置。每个可共享的臂都有有限共享资源和“每载”奖励随机变量，玩家都不知道。可共享的部门的奖励等于“每载”奖励乘以拉动手臂的球员数量和手臂最大共享资源之间的最小值。我们考虑两种反馈类型：共享需求信息（SDI）和共享需求意识（SDA），每种都提供了不同的资源共享信号。我们设计了DPE-SDI和SIC-SDA算法，分别在这两种反馈案例下解决了可共享的ARM问题，并证明这两种算法都具有对数的遗憾，这在巡回赛中很紧张。我们进行仿真以验证算法的性能并显示其在无线网络和边缘计算中的实用程序。,https://arxiv.org/abs/2204.13502,IJCAI,True,False,False,False
1709,Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation.,"Qun Li, Ziyi Zhang, Fu Xiao, Feng Zhang, Bir Bhanu","A high-resolution network exhibits remarkable capability in extracting multi-scale features for human pose estimation, but fails to capture long-range interactions between joints and has high computational complexity. To address these problems, we present a Dynamic lightweight High-Resolution Network (Dite-HRNet), which can efficiently extract multi-scale contextual information and model long-range spatial dependency for human pose estimation. Specifically, we propose two methods, dynamic split convolution and adaptive context modeling, and embed them into two novel lightweight blocks, which are named dynamic multi-scale context block and dynamic global context block. These two blocks, as the basic component units of our Dite-HRNet, are specially designed for the high-resolution networks to make full use of the parallel multi-resolution architecture. Experimental results show that the proposed network achieves superior performance on both COCO and MPII human pose estimation datasets, surpassing the state-of-the-art lightweight networks. Code is available at: \url{https://github.com/ZiyiZhang27/Dite-HRNet}.",Dite-HRNET：人体姿势估计的动态轻型高分辨率网络。,高分辨率网络在提取人姿势估计的多尺度特征方面具有显着的能力，但无法捕获关节之间的长距离相互作用，并且具有很高的计算复杂性。为了解决这些问题，我们提出了一个动态的轻质高分辨率网络（Dite-HRNET），该网络可以有效地提取多规模上下文信息，并模拟人体姿势估计的长距离空间依赖性。具体而言，我们提出了两种方法，即动态拆分卷积和自适应上下文建模，并将它们嵌入两个新颖的轻质块中，它们被命名为动态多尺度上下文块和动态全局上下文块。作为我们的Dite-HRNET的基本组件单元，这两个块是专门为高分辨率网络设计的，以充分利用并行的多分辨率体系结构。实验结果表明，所提出的网络在可可和MPII人类姿势估计数据集上均达到了卓越的性能，超过了最先进的轻量级网络。代码可在：\ url {https://github.com/ziyizhang27/dite-hrnet}中获得。,https://arxiv.org/abs/2204.10762,IJCAI,True,False,False,False
1710,Evolutionary Approach to Security Games with Signaling.,"Adam Żychowski, Jacek Mańdziuk, Elizabeth Bondi, Aravind Venugopal, Milind Tambe, Balaraman Ravindran","Green Security Games have become a popular way to model scenarios involving the protection of natural resources, such as wildlife. Sensors (e.g. drones equipped with cameras) have also begun to play a role in these scenarios by providing real-time information. Incorporating both human and sensor defender resources strategically is the subject of recent work on Security Games with Signaling (SGS). However, current methods to solve SGS do not scale well in terms of time or memory. We therefore propose a novel approach to SGS, which, for the first time in this domain, employs an Evolutionary Computation paradigm: EASGS. EASGS effectively searches the huge SGS solution space via suitable solution encoding in a chromosome and a specially-designed set of operators. The operators include three types of mutations, each focusing on a particular aspect of the SGS solution, optimized crossover and a local coverage improvement scheme (a memetic aspect of EASGS). We also introduce a new set of benchmark games, based on dense or locally-dense graphs that reflect real-world SGS settings. In the majority of 342 test game instances, EASGS outperforms state-of-the-art methods, including a reinforcement learning method, in terms of time scalability, nearly constant memory utilization, and quality of the returned defender's strategies (expected payoffs).",通过信号传导的安全游戏的进化方法。,绿色安全游戏已成为模拟涉及保护自然资源（例如野生动植物）的风景的流行方式。传感器（例如配备相机的无人机）也已经通过提供实时信息在这些情况下发挥作用。战略性地将人类和传感器辩护人资源纳入了信号（SGS）的最新安全游戏的主题。但是，解决SGS的当前方法在时间或内存方面不能很好地扩展。因此，我们提出了一种新颖的SGS方法，该方法在该领域首次采用了进化计算范式：EASGS。EASGS通过在染色体和专门设计的一组操作员中编码的合适解决方案有效地搜索了巨大的SGS解决方案空间。操作员包括三种类型的突变，每种突变都集中在SGS解决方案的特定方面，优化的交叉和局部覆盖改进方案（EASG的模因方面）。我们还基于反映现实世界中SGS设置的密集或本地密集的图表，推出了一套新的基准游戏。在大多数342个测试游戏实例中，EASGS优于最先进的方法，包括增强学习方法，在时间的可扩展性，几乎持续的内存利用以及返回的Defender策略的质量方面（预期的回报）。,https://arxiv.org/abs/2204.14173,IJCAI,True,False,False,False
1711,Anytime Capacity Expansion in Medical Residency Match by Monte Carlo Tree Search.,"Kenshi Abe, Junpei Komiyama, Atsushi Iwasaki","This paper considers the capacity expansion problem in two-sided matchings, where the policymaker is allowed to allocate some extra seats as well as the standard seats. In medical residency match, each hospital accepts a limited number of doctors. Such capacity constraints are typically given in advance. However, such exogenous constraints can compromise the welfare of the doctors; some popular hospitals inevitably dismiss some of their favorite doctors. Meanwhile, it is often the case that the hospitals are also benefited to accept a few extra doctors. To tackle the problem, we propose an anytime method that the upper confidence tree searches the space of capacity expansions, each of which has a resident-optimal stable assignment that the deferred acceptance method finds. Constructing a good search tree representation significantly boosts the performance of the proposed method. Our simulation shows that the proposed method identifies an almost optimal capacity expansion with a significantly smaller computational budget than exact methods based on mixed-integer programming.",任何时候通过Monte Carlo Tree Search在医疗居住地匹配中的容量扩展。,本文考虑了在双面比赛中的容量扩展问题，允许决策者分配一些额外的座位以及标准座位。在医疗居住比赛中，每家医院接受数量有限的医生。这种容量限制通常是事先给出的。但是，这种外源性约束会损害医生的福利。一些受欢迎的医院不可避免地会驳斥他们最喜欢的医生。同时，通常情况下，医院也受益于接受一些额外的医生。为了解决该问题，我们提出了一种任何时间置信树搜索容量扩展的空间，每个方法都有延期接受方法可以找到的居民最佳稳定分配。构建良好的搜索树表示可以显着提高所提出方法的性能。我们的仿真表明，所提出的方法比基于混合企业编程的精确方法确定了几乎最佳的能力扩展，其计算预算明显较小。,https://arxiv.org/abs/2202.06570,IJCAI,True,False,False,False
1712,"""Think Before You Speak"": Improving Multi-Action Dialog Policy by Planning Single-Action Dialogs.","Shuo Zhang, Junzhou Zhao, Pinghui Wang, Yu Li, Yi Huang, Junlan Feng","Multi-action dialog policy (MADP), which generates multiple atomic dialog actions per turn, has been widely applied in task-oriented dialog systems to provide expressive and efficient system responses. Existing MADP models usually imitate action combinations from the labeled multi-action dialog samples. Due to data limitations, they generalize poorly toward unseen dialog flows. While interactive learning and reinforcement learning algorithms can be applied to incorporate external data sources of real users and user simulators, they take significant manual effort to build and suffer from instability. To address these issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel multi-task learning framework that learns single-action dialog dynamics to enhance multi-action prediction. Our PEDP method employs model-based planning for conceiving what to express before deciding the current response through simulating single-action dialogs. Experimental results on the MultiWOZ dataset demonstrate that our fully supervised learning-based method achieves a solid task success rate of 90.6%, improving 3% compared to the state-of-the-art methods.",“在说话之前思考”：通过计划单项对话框来改善多进攻对话策略。,每回合生成多个原子对话框操作的多动作对话框策略（MADP）已被广泛应用于以任务为导向的对话框系统，以提供表现力和高效的系统响应。现有的MADP模型通常模仿标记的多动对话框样本中的动作组合。由于数据限制，它们概括为看不见的对话框流动。虽然可以应用交互式学习和强化学习算法来合并真实用户和用户模拟器的外部数据源，但它们会付出大量的手动努力来构建和遭受不稳定性的折磨。为了解决这些问题，我们提出了计划增强对话策略（PEDP），这是一个新颖的多任务学习框架，可以学习单项对话框动力学以增强多进攻预测。我们的PEDP方法采用基于模型的计划来自我想象在通过模拟单一操作对话框决定当前响应之前表达的内容。多WOZ数据集的实验结果表明，我们完全监督的基于学习的方法达到了90.6％的稳定任务成功率，与最先进的方法相比提高了3％。,https://arxiv.org/abs/2204.11481,IJCAI,True,False,False,False
1713,Test-time Fourier Style Calibration for Domain Generalization.,"Xingchen Zhao, Chang Liu, Anthony Sicilia, Seong Jae Hwang, Yun Fu","The topic of generalizing machine learning models learned on a collection of source domains to unknown target domains is challenging. While many domain generalization (DG) methods have achieved promising results, they primarily rely on the source domains at train-time without manipulating the target domains at test-time. Thus, it is still possible that those methods can overfit to source domains and perform poorly on target domains. Driven by the observation that domains are strongly related to styles, we argue that reducing the gap between source and target styles can boost models' generalizability. To solve the dilemma of having no access to the target domain during training, we introduce Test-time Fourier Style Calibration (TF-Cal) for calibrating the target domain style on the fly during testing. To access styles, we utilize Fourier transformation to decompose features into amplitude (style) features and phase (semantic) features. Furthermore, we present an effective technique to Augment Amplitude Features (AAF) to complement TF-Cal. Extensive experiments on several popular DG benchmarks and a segmentation dataset for medical images demonstrate that our method outperforms state-of-the-art methods.",测试时间傅立叶样式校准用于域概括。,从源域收集到未知目标域的概括机器学习模型的主题是具有挑战性的。尽管许多域的概括（DG）方法已经取得了令人鼓舞的结果，但它们主要依赖于火车时间的源域，而无需在测试时操作目标域。因此，这些方法仍然有可能过度拟合到源域并在目标域上表现不佳。在观察到域与样式密切相关的观察过程中，我们认为减少源和目标样式之间的差距可以提高模型的通用性。为了解决训练期间无法访问目标域的困境，我们引入了测试时间傅立叶样式校准（TF-CAL），以在测试过程中校准目标域样式。为了访问样式，我们利用傅立叶变换将功能分解为振幅（样式）功能和相位（语义）功能。此外，我们提出了一种有效的技术来增强幅度特征（AAF）以补充TF-CAL。对几个流行的DG基准和医学图像的分割数据集进行了广泛的实验，这表明我们的方法表现优于最先进的方法。,https://arxiv.org/abs/2205.06427,IJCAI,True,False,False,False
1714,D-DPCC: Deep Dynamic Point Cloud Compression via 3D Motion Prediction.,"Tingyu Fan, Linyao Gao, Yiling Xu, Zhu Li, Dong Wang","The non-uniformly distributed nature of the 3D dynamic point cloud (DPC) brings significant challenges to its high-efficient inter-frame compression. This paper proposes a novel 3D sparse convolution-based Deep Dynamic Point Cloud Compression (D-DPCC) network to compensate and compress the DPC geometry with 3D motion estimation and motion compensation in the feature space. In the proposed D-DPCC network, we design a {\it Multi-scale Motion Fusion} (MMF) module to accurately estimate the 3D optical flow between the feature representations of adjacent point cloud frames. Specifically, we utilize a 3D sparse convolution-based encoder to obtain the latent representation for motion estimation in the feature space and introduce the proposed MMF module for fused 3D motion embedding. Besides, for motion compensation, we propose a 3D {\it Adaptively Weighted Interpolation} (3DAWI) algorithm with a penalty coefficient to adaptively decrease the impact of distant neighbors. We compress the motion embedding and the residual with a lossy autoencoder-based network. To our knowledge, this paper is the first work proposing an end-to-end deep dynamic point cloud compression framework. The experimental result shows that the proposed D-DPCC framework achieves an average 76\% BD-Rate (Bjontegaard Delta Rate) gains against state-of-the-art Video-based Point Cloud Compression (V-PCC) v13 in inter mode.",D-DPCC：通过3D运动预测进行深度动态点云压缩。,3D动态点云（DPC）的非均匀分布性质为其高效框架间压缩带来了重大挑战。本文提出了一个新型的3D稀疏卷积深度动态点云压缩（D-DPCC）网络，以补偿和压缩DPC几何形状，并在特征空间中使用3D运动估计和运动补偿。在提出的D-DPCC网络中，我们设计了一个{\ it多尺度运动融合}（MMF）模块，以准确估计相邻点云帧的特征表示之间的3D光流。具体而言，我们利用一个基于3D稀疏卷积的编码器来获取特征空间中运动估计的潜在表示，并引入了Fused 3D运动嵌入的建议的MMF模块。此外，对于运动补偿，我们提出了一个3D {\它自适应加权的插值}（3DAWI）算法，并具有惩罚系数，以适应降低遥远邻居的影响。我们通过基于自动编码器的网络来压缩运动嵌入和残差。据我们所知，本文是提出端到端深度动态点云压缩框架的第一本作品。实验结果表明，所提出的D-DPCC框架在以跨模式下的基于最新的基于视频的点云压缩（V-PCC）V13实现了平均76 \％BD率（Bjontegaard Delta速率）。,https://arxiv.org/abs/2205.01135,IJCAI,True,False,False,False
1715,An EF2X Allocation Protocol for Restricted Additive Valuations.,"Hannaneh Akrami, Rojin Rezvan, Masoud Seddighin","We study the problem of fairly allocating a set of $m$ indivisible goods to a set of $n$ agents. Envy-freeness up to any good (EFX) criteria -- which requires that no agent prefers the bundle of another agent after removal of any single good -- is known to be a remarkable analogous of envy-freeness when the resource is a set of indivisible goods. In this paper, we investigate EFX notion for the restricted additive valuations, that is, every good has some non-negative value, and every agent is interested in only some of the goods.   We introduce a natural relaxation of EFX called EFkX which requires that no agent envies another agent after removal of any $k$ goods. Our main contribution is an algorithm that finds a complete (i.e., no good is discarded) EF2X allocation for the restricted additive valuations. In our algorithm we devise new concepts, namely ""configuration"" and ""envy-elimination"" that might be of independent interest.   We also use our new tools to find an EFX allocation for restricted additive valuations that discards at most $\lfloor n/2 \rfloor -1$ goods. This improves the state of the art for the restricted additive valuations by a factor of $2$.",EF2X分配协议，用于限制添加剂估值。,我们研究了将一组$ M $不可分割的商品分配给一组$ n $代理商的问题。嫉妒的柔软性达到任何好处（EFX）标准 - 要求没有代理在删除任何单一商品后更喜欢另一个代理商的束 - 当资源是一组时，却是一个显着的嫉妒性。不可分割的商品。在本文中，我们调查了EFX概念的限制添加剂估值，即，每种商品都有一些非负值，每个代理商仅对某些商品感兴趣。我们引入了一种称为EFKX的EFX的自然放松，它要求没有代理在删除任何$ k $商品后羡慕其他代理商。我们的主要贡献是一种算法，该算法发现对受限添加估值的完整（即没有好处）EF2X分配。在我们的算法中，我们设计了新概念，即可能具有独立利益的“配置”和“嫉妒 - 淘汰”。我们还使用新工具来查找EFX分配，以限制添加估值，最多可丢弃$ \ lfloor n/2 \ rfloor -1 $商品。这将有限的添加剂估值提高了最高的$ 2 $。,https://arxiv.org/abs/2202.13676,IJCAI,True,False,False,False
1716,Non-Euclidean Self-Organizing Maps,Dorota Celińska-Kopczyńska Eryk Kopczyński,"Self-Organizing Maps (SOMs, Kohonen networks) belong to neural network models of the unsupervised class. In this paper, we present the generalized setup for non-Euclidean SOMs. Most data analysts take it for granted to use some subregions of a flat space as their data model; however, by the assumption that the underlying geometry is non-Euclidean we obtain a new degree of freedom for the techniques that translate the similarities into spatial neighborhood relationships. We improve the traditional SOM algorithm by introducing topology-related extensions. Our proposition can be successfully applied to dimension reduction, clustering or finding similarities in big data (both hierarchical and non-hierarchical).",非欧几里德自组织地图,自组织地图（SOMS，Kohonen网络）属于无监督类的神经网络模型。在本文中，我们介绍了非欧国人SOM的广义设置。大多数数据分析师都将其视为将平面空间的某些子区域作为其数据模型。但是，通过假设潜在的几何形状是非欧几里得人，我们获得了将相似性转化为空间邻里关系的技术的新自由度。我们通过引入与拓扑相关的扩展来改善传统的SOM算法。我们的命题可以成功地应用于缩小，聚类或在大数据（分层和非层次结构）中找到相似性。,https://arxiv.org/abs/2109.11769,IJCAI,True,False,False,False
1717,Voting in Two-Crossing Elections.,"Andrei Constantinescu, Roger Wattenhofer","We introduce two-crossing elections as a generalization of single-crossing elections, showing a number of new results. First, we show that two-crossing elections can be recognized in polynomial time, by reduction to the well-studied consecutive ones problem. We also conjecture that recognizing $k$-crossing elections is NP-complete in general, providing evidence by relating to a problem similar to consecutive ones proven to be hard in the literature. Single-crossing elections exhibit a transitive majority relation, from which many important results follow. On the other hand, we show that the classical Debord-McGarvey theorem can still be proven two-crossing, implying that any weighted majority tournament is inducible by a two-crossing election. This shows that many voting rules are NP-hard under two-crossing elections, including Kemeny and Slater. This is in contrast to the single-crossing case and outlines an important complexity boundary between single- and two-crossing. Subsequently, we show that for two-crossing elections the Young scores of all candidates can be computed in polynomial time, by formulating a totally unimodular linear program. Finally, we consider the Chamberlin-Courant rule with arbitrary disutilities and show that a winning committee can be computed in polynomial time, using an approach based on dynamic programming.",在两次交战的选举中投票。,我们引入了两次交叉选举，作为单跨选举的概括，显示了许多新的结果。首先，我们表明，通过减少了经过深入研究的连续问题，可以在多项式时间内认识到两次交叉选举。我们还猜想，认识到$ k $  - 交叉选举通常是NP的完整选举，通过与与连续的问题相似的问题相关的证据，这在文献中被证明是困难的。单跨选举表现出多数关系，从中得出许多重要的结果。另一方面，我们表明，古典的Debord-McGarvey定理仍然可以被证明是两面的，这意味着任何加权多数锦标赛都是通过两次交战的选举诱发的。这表明许多投票规则在包括凯门尼和斯莱特在内的两次选举下都是NP-Hard。这与单跨案例相反，并概述了单跨和两跨之间的重要复杂性边界。随后，我们表明，对于两次交叉选举，可以通过制定一个完全单模型的线性程序来在多项式时间内计算所有候选人的年轻分数。最后，我们考虑具有任意疾病的Chamberlin-Courant规则，并表明可以使用基于动态编程的方法在多项式时间内计算获胜委员会。,https://arxiv.org/abs/2205.00474,IJCAI,True,False,False,False
1718,Exchangeability-Aware Sum-Product Networks.,"Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt","Sum-Product Networks (SPNs) are expressive probabilistic models that provide exact, tractable inference. They achieve this efficiency by making used of local independence. On the other hand, mixtures of exchangeable variable models (MEVMs) are a class of tractable probabilistic models that make use of exchangeability of random variables to render inference tractable. Exchangeability, which arises naturally in systems consisting of multiple, interrelated entities, has not been considered for efficient representation and inference in SPNs yet. The contribution of this paper is a novel probabilistic model which we call Exchangeability-Aware Sum-Product Networks (XSPNs). It contains both SPNs and MEVMs as special cases, and combines the ability of SPNs to efficiently learn deep probabilistic models with the ability of MEVMs to efficiently handle exchangeable random variables. We also introduce a structure learning algorithm for XSPNs and empirically show that they can be more accurate and efficient than conventional SPNs when the data contains repeated, interchangeable parts.",交换性意识到的总产品网络。,总产品网络（SPN）是提供精确，可拖延推理的表达概率模型。他们通过使用当地独立性来实现这种效率。另一方面，可交换变量模型（MEVM）的混合物是一类可进行的概率模型，可利用随机变量的交换性来渲染推理。尚未考虑在由多个相互关联的实体组成的系统中产生的交换性，尚未被视为有效的SPN表示和推断。本文的贡献是一种新型的概率模型，我们称之为交换性意识到的求和产物网络（XSPNS）。它同时包含SPN和MEVM作为特殊情况，并结合了SPN有效地学习深层概率模型的能力以及MEVM有效处理可交换的随机变量的能力。我们还为XSPN引入了一种结构学习算法，并从经验上表明，当数据包含重复的，可互换的部分时，它们比常规SPN更准确，更有效。,https://arxiv.org/abs/2110.05165,IJCAI,True,False,False,False
1719,Dynamic Car Dispatching and Pricing: Revenue and Fairness for Ridesharing Platforms.,"Zishuo Zhao, Xi Chen, Xuefeng Zhang, Yuan Zhou","A major challenge for ridesharing platforms is to guarantee profit and fairness simultaneously, especially in the presence of misaligned incentives of drivers and riders. We focus on the dispatching-pricing problem to maximize the total revenue while keeping both drivers and riders satisfied. We study the computational complexity of the problem, provide a novel two-phased pricing solution with revenue and fairness guarantees, extend it to stochastic settings and develop a dynamic (a.k.a., learning-while-doing) algorithm that actively collects data to learn the demand distribution during the scheduling process. We also conduct extensive experiments to demonstrate the effectiveness of our algorithms.",动态汽车调度和定价：乘车共享平台的收入和公平性。,乘车平台的一个主要挑战是同时保证利润和公平，尤其是在驾驶员和骑手的激励措施不一致的情况下。我们专注于调度定价问题，以最大程度地利用总收入，同时使驾驶员和骑手都满意。我们研究了问题的计算复杂性，提供了一种新颖的两步定价解决方案，并保证了收入和公平性，将其扩展到随机设置，并开发动态（又称学习 - 学习过程）算法，以积极地收集数据以学习需求，以学习需求在计划过程中分发。我们还进行了广泛的实验，以证明算法的有效性。,https://arxiv.org/abs/2207.06318,IJCAI,True,False,False,False
1720,Single-Peaked Opinion Updates.,"Robert Bredereck, Anne-Marie George, Jonas Israel, Leon Kellerhals","We consider opinion diffusion for undirected networks with sequential updates when the opinions of the agents are single-peaked preference rankings. Our starting point is the study of preserving single-peakedness. We identify voting rules that, when given a single-peaked profile, output at least one ranking that is single peaked w.r.t. a single-peaked axis of the input. For such voting rules we show convergence to a stable state of the diffusion process that uses the voting rule as the agents' update rule. Further, we establish an efficient algorithm that maximises the spread of extreme opinions.",单声明的意见更新。,当代理人的意见是单峰首选项排名时，我们考虑对无向网络进行连续更新的意见扩散。我们的起点是维护单峰的研究。我们确定了投票规则，当给出一个单台面的配置文件时，输出至少一个排名是单个峰值W.R.T.输入的单轴轴。对于此类投票规则，我们显示了将投票规则用作代理的更新规则的扩散过程的稳定状态。此外，我们建立了一种有效的算法，可最大程度地提高极端意见的传播。,https://arxiv.org/abs/2204.14094,IJCAI,True,False,False,False
1721,Taylor-Lagrange Neural Ordinary Differential Equations: Toward Fast Training and Evaluation of Neural ODEs.,"Franck Djeumou, Cyrus Neary, Eric Goubault, Sylvie Putot, Ufuk Topcu","Neural ordinary differential equations (NODEs) -- parametrizations of differential equations using neural networks -- have shown tremendous promise in learning models of unknown continuous-time dynamical systems from data. However, every forward evaluation of a NODE requires numerical integration of the neural network used to capture the system dynamics, making their training prohibitively expensive. Existing works rely on off-the-shelf adaptive step-size numerical integration schemes, which often require an excessive number of evaluations of the underlying dynamics network to obtain sufficient accuracy for training. By contrast, we accelerate the evaluation and the training of NODEs by proposing a data-driven approach to their numerical integration. The proposed Taylor-Lagrange NODEs (TL-NODEs) use a fixed-order Taylor expansion for numerical integration, while also learning to estimate the expansion's approximation error. As a result, the proposed approach achieves the same accuracy as adaptive step-size schemes while employing only low-order Taylor expansions, thus greatly reducing the computational cost necessary to integrate the NODE. A suite of numerical experiments, including modeling dynamical systems, image classification, and density estimation, demonstrate that TL-NODEs can be trained more than an order of magnitude faster than state-of-the-art approaches, without any loss in performance.",泰勒 - 拉格朗日神经普通微分方程：朝着快速训练和评估神经odes。,神经普通微分方程（节点） - 使用神经网络的微分方程的参数化 - 在从数据的未知连续时间动态系统的学习模型中显示出巨大的希望。但是，对节点的每个正向评估都需要用于捕获系统动态的神经网络的数值集成，从而使其训练过高。现有作品依赖于现成的自适应阶梯尺寸的数值集成方案，该方案通常需要对基础动态网络进行过多的评估，以获得足够的培训准确性。相比之下，我们通过提出数据驱动的数值集成方法来加速节点的评估和培训。所提出的泰勒 - 拉格朗日节点（TL节点）使用固定阶的泰勒扩展进行数值集成，同时还学习估计扩展的近似误差。结果，所提出的方法在仅采用低阶泰勒膨胀的同时，达到了与自适应阶梯尺寸方案相同的准确性，从而大大降低了整合节点所需的计算成本。一组数值实验，包括建模动态系统，图像分类和密度估计，表明TL节点可以比最先进的方法更快地训练超过一个数量级，而不会出现任何性能。,https://arxiv.org/abs/2201.05715,IJCAI,True,False,False,False
1722,Understanding Distance Measures Among Elections.,"Niclas Boehmer, Piotr Faliszewski, Rolf Niedermeier, Stanisław Szufa, Tomasz Wąs","Motivated by putting empirical work based on (synthetic) election data on a more solid mathematical basis, we analyze six distances among elections, including, e.g., the challenging-to-compute but very precise swap distance and the distance used to form the so-called map of elections. Among the six, the latter seems to strike the best balance between its computational complexity and expressiveness.",了解选举之间的距离措施。,通过以更坚实的数学为基础，基于（综合）选举数据的经验工作的激励，我们分析了选举之间的六个距离，包括，例如，具有挑战性但非常精确的交换距离，以及用于形成所谓的距离称为选举地图。在这六个中，后者似乎在其计算复杂性和表现力之间取得了最佳平衡。,https://arxiv.org/abs/2205.00492,IJCAI,True,False,False,False
1723,I Will Have Order! Optimizing Orders for Fair Reviewer Assignment.,"Justin Payan, Yair Zick","Scientific advancement requires effective peer review. Papers should be reviewed by experts in the subject area, but it is equally important that reviewer quality is fairly distributed amongst papers. We model reviewer assignment as an instance of a fair allocation problem, presenting an extension of the classic round-robin mechanism, called Reviewer Round Robin (RRR). Round-robin mechanisms are a standard tool to ensure envy-free up to one item (EF1) allocations. However, fairness often comes at the cost of decreased efficiency. To overcome this challenge, we carefully select an approximately optimal round-robin order. Applying a relaxation of submodularity, $\gamma$-weak submodularity, we show that greedily inserting papers into an order yields a ${(1+\gamma^2)}$-approximation to the maximum welfare attainable by our round-robin mechanism under any order. Our approach outputs highly efficient EF1 allocations for three real conference datasets, outperforming several state-of-the-art paper assignment methods in fairness, efficiency and runtime.",我会订单！优化公平审阅者作业的订单。,科学进步需要有效的同行审查。论文应由该学科领域的专家进行审查，但同样重要的是要在论文中进行公平分发审稿人质量。我们将审阅者分配模型为公平分配问题的实例，并提出了经典的循环机制的扩展名，称为评论者循环roble Robin（RRR）。旋转机制是确保最多一项（EF1）分配的标准工具。但是，公平性通常以降低效率为代价。为了克服这一挑战，我们仔细选择了大约最佳的圆形旋转顺序。应用放松的子管道，$ \ gamma $  - 弱 - 弱体，我们表明，贪婪地将论文插入订单中会产生$ {（1+ \ gamma^2）} $  - 近似于我们的最大福利，我们可以通过我们的rougbin机制获得的最大福利根据任何命令。我们的方法为三个真实会议数据集输出高效的EF1分配，在公平，效率和运行时表现优于几种最先进的纸质分配方法。,https://arxiv.org/abs/2108.02126,IJCAI,True,False,False,False
1724,Monotone-Value Neural Networks: Exploiting Preference Monotonicity in Combinatorial Assignment.,"Jakob Weissteiner, Jakob Heiss, Julien Siems, Sven Seuken","Many important resource allocation problems involve the combinatorial assignment of items, e.g., auctions or course allocation. Because the bundle space grows exponentially in the number of items, preference elicitation is a key challenge in these domains. Recently, researchers have proposed ML-based mechanisms that outperform traditional mechanisms while reducing preference elicitation costs for agents. However, one major shortcoming of the ML algorithms that were used is their disregard of important prior knowledge about agents' preferences. To address this, we introduce monotone-value neural networks (MVNNs), which are designed to capture combinatorial valuations, while enforcing monotonicity and normality. On a technical level, we prove that our MVNNs are universal in the class of monotone and normalized value functions, and we provide a mixed-integer linear program (MILP) formulation to make solving MVNN-based winner determination problems (WDPs) practically feasible. We evaluate our MVNNs experimentally in spectrum auction domains. Our results show that MVNNs improve the prediction performance, they yield state-of-the-art allocative efficiency in the auction, and they also reduce the run-time of the WDPs. Our code is available on GitHub: https://github.com/marketdesignresearch/MVNN.",单调值神经网络：组合分配中的偏好单调性。,许多重要的资源分配问题涉及项目的组合分配，例如拍卖或课程分配。由于捆绑空间在项目数量中成倍增长，因此偏好启发是这些域中的关键挑战。最近，研究人员提出了基于ML的机制，以优于传统机制，同时降低代理商的偏好启发成本。但是，使用的ML算法的一个主要缺点是他们无视有关代理人偏好的重要先验知识。为了解决这个问题，我们引入了单调值神经网络（MVNN），旨在捕获组合估值，同时执行单调性和正常性。从技术层面上讲，我们证明我们的MVNN在单调和归一化值函数类别中是通用的，并且我们提供了一个混合成员线性程序（MILP）公式，以使解决MVNN的获胜者确定问题（WDPS）实际上可行。我们在频谱拍卖域中对MVNNS进行了实验评估。我们的结果表明，MVNNS改善了预测性能，它们在拍卖中产生了最先进的分配效率，并且还降低了WDP的运行时间。我们的代码可在github：https：//github.com/marketdesignresearch/mvnn上找到。,https://arxiv.org/abs/2109.15117,IJCAI,True,False,False,False
1725,FAITH: Few-Shot Graph Classification with Hierarchical Task Graphs.,"Song Wang, Yushun Dong, Xiao Huang, Chen Chen, Jundong Li","Few-shot graph classification aims at predicting classes for graphs, given limited labeled graphs for each class. To tackle the bottleneck of label scarcity, recent works propose to incorporate few-shot learning frameworks for fast adaptations to graph classes with limited labeled graphs. Specifically, these works propose to accumulate meta-knowledge across diverse meta-training tasks, and then generalize such meta-knowledge to the target task with a disjoint label set. However, existing methods generally ignore task correlations among meta-training tasks while treating them independently. Nevertheless, such task correlations can advance the model generalization to the target task for better classification performance. On the other hand, it remains non-trivial to utilize task correlations due to the complex components in a large number of meta-training tasks. To deal with this, we propose a novel few-shot learning framework FAITH that captures task correlations via constructing a hierarchical task graph at different granularities. Then we further design a loss-based sampling strategy to select tasks with more correlated classes. Moreover, a task-specific classifier is proposed to utilize the learned task correlations for few-shot classification. Extensive experiments on four prevalent few-shot graph classification datasets demonstrate the superiority of FAITH over other state-of-the-art baselines.",信仰：带有层次任务图的很少的图形分类。,几乎没有图形分类旨在预测图形的类，给定每个类标记的图形有限。为了应对标签稀缺性的瓶颈，最近的作品建议将几乎没有的学习框架结合起来，以快速适应具有有限标签图的图形类别。具体而言，这些作品建议在不同的元训练任务中累积元知识，然后使用不交战的标签集将这种元知识概括为目标任务。但是，现有方法通常忽略元训练任务之间的任务相关性，同时独立治疗。然而，这种任务相关性可以将模型概括提高到目标任务以获得更好的分类性能。另一方面，由于大量的元训练任务中的复杂组件，因此使用任务相关性仍然不足。为了解决这个问题，我们提出了一种新颖的几弹性学习框架信仰，该信仰通过在不同的粒度上构建层次结构任务图来捕获任务相关性。然后，我们进一步设计了一种基于损失的抽样策略，以选择具有更多相关类别的任务。此外，提出了特定于任务的分类器来利用学习的任务相关性来进行几次分类。在四个普遍的几示图分类数据集上进行了广泛的实验，证明了信仰比其他最先进的基线的优越性。,https://arxiv.org/abs/2205.02435,IJCAI,True,False,False,False
1726,FedCG: Leverage Conditional GAN for Protecting Privacy and Maintaining Competitive Performance in Federated Learning.,"Yuezhou Wu, Yan Kang, Jiahuan Luo, Yuanqin He, Qiang Yang","Federated learning (FL) aims to protect data privacy by enabling clients to collaboratively build machine learning models without sharing their private data. However, recent works demonstrate that FL is vulnerable to gradient-based data recovery attacks. Varieties of privacy-preserving technologies have been leveraged to further enhance the privacy of FL. Nonetheless, they either are computational or communication expensive (e.g., homomorphic encryption) or suffer from precision loss (e.g., differential privacy). In this work, we propose \textsc{FedCG}, a novel \underline{fed}erated learning method that leverages \underline{c}onditional \underline{g}enerative adversarial networks to achieve high-level privacy protection while still maintaining competitive model performance. More specifically, \textsc{FedCG} decomposes each client's local network into a private extractor and a public classifier and keeps the extractor local to protect privacy. Instead of exposing extractors which is the culprit of privacy leakage, \textsc{FedCG} shares clients' generators with the server for aggregating common knowledge aiming to enhance the performance of clients' local networks. Extensive experiments demonstrate that \textsc{FedCG} can achieve competitive model performance compared with baseline FL methods, and numerical privacy analysis shows that \textsc{FedCG} has high-level privacy-preserving capability.",FedCG：利用有条件的gan来保护隐私并保持联盟学习中的竞争性能。,联合学习（FL）旨在通过使客户在不共享私人数据的情况下协作建立机器学习模型来保护数据隐私。但是，最近的作品表明，FL容易受到基于梯度的数据恢复攻击的影响。已经利用了多种隐私技术的品种，以进一步增强佛罗里达州的隐私。尽管如此，它们要么是计算量昂贵的（例如同构加密），要么遭受精确损失（例如，差异隐私）的损失。在这项工作中，我们提出了\ textsc {fedcg}，一种小说\下划线{fed}的学习方法，该方法利用\ underline {c} onDitional \ onditional \ underline \ underline {g}能量对抗性网络，同时仍然保持高级隐私保护保护表现。更具体地说，\ textsc {fedCG}将每个客户的本地网络分解为私人提取器和公共分类器，并保留提取器本地以保护隐私。\ textsc {fedCG}与其与服务器共享客户的生成器，而不是曝光提取器，而不是泄露隐私泄漏的罪魁祸首，以汇总旨在提高客户本地网络性能的常识。广泛的实验表明，与基线FL方法相比，\ textsc {fedCG}可以实现竞争性模型性能，而数值隐私分析表明，\ textsc {fedcg}具有高级隐私性保护能力。,https://arxiv.org/abs/2111.08211,IJCAI,True,False,False,False
1727,Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs.,"Zhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang, Xiaoli Li, Ru Li, Jeff Z. Pan","Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly challenging problem as traditional subgraph matching methods are not capable to deal with noise and missing information. To address this problem, it has been recently introduced a promising approach based on jointly embedding logical queries and KGs into a low-dimensional space to identify answer entities. However, existing proposals ignore critical semantic knowledge inherently available in KGs, such as type information. To leverage type information, we propose a novel TypE-aware Message Passing (TEMP) model, which enhances the entity and relation representations in queries, and simultaneously improves generalization, deductive and inductive reasoning. Remarkably, TEMP is a plug-and-play model that can be easily incorporated into existing embedding-based models to improve their performance. Extensive experiments on three real-world datasets demonstrate TEMP's effectiveness.",多跳推理的类型感知嵌入在知识图上。,关于现实生活知识图（KGS）的多跳上推理是一个高度挑战的问题，因为传统的子图匹配方法无法处理噪音和缺失信息。为了解决这个问题，最近已经引入了一种有希望的方法，该方法基于将逻辑查询和kgs共同嵌入到一个低维空间中以识别答案实体。但是，现有的提案忽略了KGS中固有可用的关键语义知识，例如类型信息。为了利用类型信息，我们提出了一种新颖的类型感知消息传递（TEMP）模型，该模型可以增强查询中的实体和关系表示形式，并同时改善概括，演绎和归纳推理。值得注意的是，Temp是一种插件模型，可以轻松地将其纳入现有的基于嵌入的模型中以提高其性能。在三个现实世界数据集上进行了广泛的实验证明了温度的有效性。,https://arxiv.org/abs/2205.00782,IJCAI,True,False,False,False
1728,Data-Free Adversarial Knowledge Distillation for Graph Neural Networks.,"Yuanxin Zhuang, Lingjuan Lyu, Chuan Shi, Carl Yang, Lichao Sun","Graph neural networks (GNNs) have been widely used in modeling graph structured data, owing to its impressive performance in a wide range of practical applications. Recently, knowledge distillation (KD) for GNNs has enabled remarkable progress in graph model compression and knowledge transfer. However, most of the existing KD methods require a large volume of real data, which are not readily available in practice, and may preclude their applicability in scenarios where the teacher model is trained on rare or hard to acquire datasets. To address this problem, we propose the first end-to-end framework for data-free adversarial knowledge distillation on graph structured data (DFAD-GNN). To be specific, our DFAD-GNN employs a generative adversarial network, which mainly consists of three components: a pre-trained teacher model and a student model are regarded as two discriminators, and a generator is utilized for deriving training graphs to distill knowledge from the teacher model into the student model. Extensive experiments on various benchmark models and six representative datasets demonstrate that our DFAD-GNN significantly surpasses state-of-the-art data-free baselines in the graph classification task.",图形神经网络的无数据对抗知识蒸馏。,图形神经网络（GNN）已被广泛用于建模图形结构化数据，这是由于其在广泛的实用应用中令人印象深刻的性能。最近，GNNS的知识蒸馏（KD）在图形模型压缩和知识转移方面取得了显着进步。但是，大多数现有的KD方法都需要大量的真实数据，这些数据在实践中不容易获得，并且可能排除其在教师模型对稀有或难以获取数据集培训的情况下的适用性。为了解决这个问题，我们提出了第一个用于图形结构化数据（DFAD-GNN）的无数据对抗知识蒸馏的端到端框架。具体而言，我们的DFAD-GNN采用生成性对抗网络，主要由三个组成部分组成：预训练的教师模型和学生模型被视为两个歧视者，并利用生成器来衍生训练图来从教师模型进入学生模型。在各种基准模型和六个代表性数据集上进行的广泛实验表明，我们的DFAD-GNN在图形分类任务中显着超过了最新的无数据基线。,https://arxiv.org/abs/2205.03811,IJCAI,True,False,True,False
1729,Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation.,"Jun Xia, Ting Wang, Jieping Ding, Xian Wei, Mingsong Chen","Due to the prosperity of Artificial Intelligence (AI) techniques, more and more backdoors are designed by adversaries to attack Deep Neural Networks (DNNs).Although the state-of-the-art method Neural Attention Distillation (NAD) can effectively erase backdoor triggers from DNNs, it still suffers from non-negligible Attack Success Rate (ASR) together with lowered classification ACCuracy (ACC), since NAD focuses on backdoor defense using attention features (i.e., attention maps) of the same order. In this paper, we introduce a novel backdoor defense framework named Attention Relation Graph Distillation (ARGD), which fully explores the correlation among attention features with different orders using our proposed Attention Relation Graphs (ARGs). Based on the alignment of ARGs between both teacher and student models during knowledge distillation, ARGD can eradicate more backdoor triggers than NAD. Comprehensive experimental results show that, against six latest backdoor attacks, ARGD outperforms NAD by up to 94.85% reduction in ASR, while ACC can be improved by up to 3.23%.",使用注意力关系图蒸馏消除深层神经网络的后门触发器。,由于人工智能（AI）技术的繁荣，越来越多的后门是由对手攻击深度神经网络（DNNS）的设计。尽管最先进的方法神经注意力蒸馏（NAD）可以有效擦除后门触发器从DNNS来看，它仍然患有不可忽略的攻击成功率（ASR）以及分类准确性降低，因为NAD专注于使用相同顺序的注意特征（即注意力图）上的后门防御。在本文中，我们介绍了一个名为“注意力关系图”蒸馏（ARGD）的新型后门防御框架，该框架使用我们提出的注意力关系图（ARGS）充分探讨了注意力特征与不同订单之间的相关性。基于知识蒸馏过程中教师和学生模型之间的ARG对齐，ARGD可以消除比NAD更多的后门触发器。全面的实验结果表明，在最新的六次后门攻击中，ARGD的表现高达94.85％的ASR，而ACC可以提高高达3.23％。,https://arxiv.org/abs/2204.09975,IJCAI,True,False,False,False
1730,RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection.,"Jinpeng Li, Haibo Jin, Shengcai Liao, Ling Shao, Pheng-Ann Heng","This paper presents a Refinement Pyramid Transformer (RePFormer) for robust facial landmark detection. Most facial landmark detectors focus on learning representative image features. However, these CNN-based feature representations are not robust enough to handle complex real-world scenarios due to ignoring the internal structure of landmarks, as well as the relations between landmarks and context. In this work, we formulate the facial landmark detection task as refining landmark queries along pyramid memories. Specifically, a pyramid transformer head (PTH) is introduced to build both homologous relations among landmarks and heterologous relations between landmarks and cross-scale contexts. Besides, a dynamic landmark refinement (DLR) module is designed to decompose the landmark regression into an end-to-end refinement procedure, where the dynamically aggregated queries are transformed to residual coordinates predictions. Extensive experimental results on four facial landmark detection benchmarks and their various subsets demonstrate the superior performance and high robustness of our framework.",复制器：用于健壮面部标志性检测的改进金字塔变压器。,本文提出了一个改进金字塔变压器（复制器），以进行健壮的面部标志性检测。大多数面部地标探测器都专注于学习代表性图像特征。但是，这些基于CNN的功能表示不足以处理复杂的现实世界情景，因为忽略了地标的内部结构以及地标和环境之间的关系。在这项工作中，我们制定了面部标志性检测任务，作为沿金字塔记忆的提炼里程碑式的查询。具体而言，引入了金字塔变压器头（PTH），以在地标之间建立同源关系，以及地标和跨尺度环境之间的异源关系。此外，动态里程碑改进（DLR）模块旨在将地标回归分解为端到端的细化过程，其中动态聚合的查询被转换为残留坐标预测。对四个面部标志检测基准及其各种子集进行的广泛实验结果表明，我们的框架具有卓越的性能和较高的鲁棒性。,https://arxiv.org/abs/2207.03917,IJCAI,True,False,False,False
1731,Contrastive Multi-view Hyperbolic Hierarchical Clustering.,"Fangfei Lin, Bing Bai, Kun Bai, Yazhou Ren, Peng Zhao, Zenglin Xu","Hierarchical clustering recursively partitions data at an increasingly finer granularity. In real-world applications, multi-view data have become increasingly important. This raises a less investigated problem, i.e., multi-view hierarchical clustering, to better understand the hierarchical structure of multi-view data. To this end, we propose a novel neural network-based model, namely Contrastive Multi-view Hyperbolic Hierarchical Clustering (CMHHC). It consists of three components, i.e., multi-view alignment learning, aligned feature similarity learning, and continuous hyperbolic hierarchical clustering. First, we align sample-level representations across multiple views in a contrastive way to capture the view-invariance information. Next, we utilize both the manifold and Euclidean similarities to improve the metric property. Then, we embed the representations into a hyperbolic space and optimize the hyperbolic embeddings via a continuous relaxation of hierarchical clustering loss. Finally, a binary clustering tree is decoded from optimized hyperbolic embeddings. Experimental results on five real-world datasets demonstrate the effectiveness of the proposed method and its components.",对比度多视图双曲线分层聚类。,分层聚类递归地分隔了越来越细的粒度。在实际应用程序中，多视图数据变得越来越重要。这引发了一个较少研究的问题，即多视图分层聚类，以更好地了解多视图数据的层次结构。为此，我们提出了一个新型的基于神经网络的模型，即对比度多视线双曲线分层聚类（CMHHC）。它由三个组成部分，即多视图对齐学习，对齐功能相似性学习和连续双曲分层聚类。首先，我们以一种对比的方式将样本级表示跨多个视图对齐，以捕获视图不变信息。接下来，我们同时利用歧管和欧几里得相似性来改善公制属性。然后，我们将表示形式嵌入双曲线空间中，并通过连续放松分层聚类损失来优化双曲线嵌入。最后，通过优化的双曲线嵌入来解码二进制聚类树。五个现实世界数据集的实验结果证明了该方法及其组件的有效性。,https://arxiv.org/abs/2205.02618,IJCAI,True,False,False,False
1732,Multi-Graph Fusion Networks for Urban Region Embedding.,"Shangbin Wu, Xu Yan, Xiaoliang Fan, Shirui Pan, Shichao Zhu, Chuanpan Zheng, Ming Cheng, Cheng Wang","Learning the embeddings for urban regions from human mobility data can reveal the functionality of regions, and then enables the correlated but distinct tasks such as crime prediction. Human mobility data contains rich but abundant information, which yields to the comprehensive region embeddings for cross domain tasks. In this paper, we propose multi-graph fusion networks (MGFN) to enable the cross domain prediction tasks. First, we integrate the graphs with spatio-temporal similarity as mobility patterns through a mobility graph fusion module. Then, in the mobility pattern joint learning module, we design the multi-level cross-attention mechanism to learn the comprehensive embeddings from multiple mobility patterns based on intra-pattern and inter-pattern messages. Finally, we conduct extensive experiments on real-world urban datasets. Experimental results demonstrate that the proposed MGFN outperforms the state-of-the-art methods by up to 12.35% improvement.",城市地区嵌入的多段融合网络。,从人类流动数据中学习城市地区的嵌入可以揭示区域的功能，然后启用相关但不同的任务，例如犯罪预测。人类流动性数据包含丰富而丰富的信息，这些信息可用于跨域任务的综合区域嵌入。在本文中，我们提出了多段融合网络（MGFN）来实现跨域预测任务。首先，我们通过移动图融合模块将图形与时空相似性作为移动性模式集成在一起。然后，在移动性模式联合学习模块中，我们设计了多级交叉注意机制，以根据基于模式和模式间信息从多个移动性模式中学习综合嵌入。最后，我们对现实世界中的城市数据集进行了广泛的实验。实验结果表明，所提出的MGFN的表现优于最先进的方法，最大提高了12.35％。,https://arxiv.org/abs/2201.09760,IJCAI,True,False,False,False
1733,Neural Contextual Anomaly Detection for Time Series.,"Chris U. Carmona, François-Xavier Aubet, Valentin Flunkert, Jan Gasthaus","We introduce Neural Contextual Anomaly Detection (NCAD), a framework for anomaly detection on time series that scales seamlessly from the unsupervised to supervised setting, and is applicable to both univariate and multivariate time series. This is achieved by effectively combining recent developments in representation learning for multivariate time series, with techniques for deep anomaly detection originally developed for computer vision that we tailor to the time series setting. Our window-based approach facilitates learning the boundary between normal and anomalous classes by injecting generic synthetic anomalies into the available data. Moreover, our method can effectively take advantage of all the available information, be it as domain knowledge, or as training labels in the semi-supervised setting. We demonstrate empirically on standard benchmark datasets that our approach obtains a state-of-the-art performance in these settings.",时间序列的神经背景异常检测。,我们引入了神经情境异常检测（NCAD），这是时间序列的异常检测框架，该框架从无监督到监督的设置无缝缩放，并且适用于单变量和多元时间序列。这是通过有效地结合了多元时间序列的表示学习中的最新发展来实现的，最初针对计算机视觉开发的深度异常检测技术，我们根据时间序列设置量身定制。我们的基于窗口的方法通过将通用合成异常注入可用数据来促进学习正常和异常类之间的边界。此外，我们的方法可以有效地利用所有可用信息，无论是域知识，还是在半监督环境中作为培训标签。我们在标准基准数据集上以经验证明我们的方法在这些设置中获得了最先进的性能。,https://arxiv.org/abs/2107.07702,IJCAI,True,False,True,False
1734,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention.,"Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang","Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledge External Attention for Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.",人类对Consensensensqa的均等：增强自我注意力，以外部注意力。,当今的大多数AI系统都集中在使用自我发挥机制和变压器体系结构上，以实现令人印象深刻的性能提高。在本文中，我们建议使用外部注意机制来增强变压器体系结构，以带来外部知识和背景。通过将外部信息整合到预测过程中，我们希望减少对大型模型的需求，并增加AI系统的民主化。我们发现，提出的外部注意机制可以显着提高现有AI系统的性能，从而使从业者可以轻松地将基金会AI模型自定义为许多不同的下游应用程序。特别是，我们专注于常识性推理的任务，表明所提出的外部注意机制可以增强现有的变压器模型并显着提高模型的推理能力。所提出的系统，即推理的知识外部关注（KEER），在开放的Commonsensensensensensensenseqa研究基准上达到了人类的奇偶校验，与88.9％的人类准确性相比，精度为89.4 \％。,https://arxiv.org/abs/2112.03254,IJCAI,True,False,False,False
1735,Augmenting Knowledge Graphs for Better Link Prediction.,"Jiang Wang, Filip Ilievski, Pedro Szekely, Ke-Thia Yao","Embedding methods have demonstrated robust performance on the task of link prediction in knowledge graphs, by mostly encoding entity relationships. Recent methods propose to enhance the loss function with a literal-aware term. In this paper, we propose KGA: a knowledge graph augmentation method that incorporates literals in an embedding model without modifying its loss function. KGA discretizes quantity and year values into bins, and chains these bins both horizontally, modeling neighboring values, and vertically, modeling multiple levels of granularity. KGA is scalable and can be used as a pre-processing step for any existing knowledge graph embedding model. Experiments on legacy benchmarks and a new large benchmark, DWD, show that augmenting the knowledge graph with quantities and years is beneficial for predicting both entities and numbers, as KGA outperforms the vanilla models and other relevant baselines. Our ablation studies confirm that both quantities and years contribute to KGA's performance, and that its performance depends on the discretization and binning settings. We make the code, models, and the DWD benchmark publicly available to facilitate reproducibility and future research.",增强知识图，以更好地链接预测。,嵌入方法已经通过编码实体关系证明了知识图中链接预测的任务的强大性能。最近的方法提出了通过字面意识术语增强损失函数的建议。在本文中，我们提出了KGA：一种知识图增强方法，该方法将文字纳入嵌入模型而不修改其损耗函数。KGA将数量和年度值分散到垃圾箱中，并将这些垃圾箱水平链，对相邻值进行建模，并垂直建模多个粒度水平。KGA是可扩展的，可以用作任何现有知识图嵌入模型的预处理步骤。关于传统基准和新的大型基准DWD的实验表明，用数量和年数来增强知识图对预测实体和数字是有益的，因为KGA的表现优于香草模型和其他相关基准。我们的消融研究证实，数量和年数都促进了KGA的表现，并且其绩效取决于离散化和融合设置。我们将公开使用代码，模型和DWD基准，以促进可重复性和未来的研究。,https://arxiv.org/abs/2203.13965,IJCAI,True,False,False,False
1736,Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning.,"Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan, Xueqian Wang, Ping Luo, Huazhe Xu","One of the key challenges in visual Reinforcement Learning (RL) is to learn policies that can generalize to unseen environments. Recently, data augmentation techniques aiming at enhancing data diversity have demonstrated proven performance in improving the generalization ability of learned policies. However, due to the sensitivity of RL training, naively applying data augmentation, which transforms each pixel in a task-agnostic manner, may suffer from instability and damage the sample efficiency, thus further exacerbating the generalization performance. At the heart of this phenomenon is the diverged action distribution and high-variance value estimation in the face of augmented images. To alleviate this issue, we propose Task-aware Lipschitz Data Augmentation (TLDA) for visual RL, which explicitly identifies the task-correlated pixels with large Lipschitz constants, and only augments the task-irrelevant pixels. To verify the effectiveness of TLDA, we conduct extensive experiments on DeepMind Control suite, CARLA and DeepMind Manipulation tasks, showing that TLDA improves both sample efficiency in training time and generalization in test time. It outperforms previous state-of-the-art methods across the 3 different visual control benchmarks.",不要触摸重要的事情：用于视觉增强学习的任务吸引Lipschitz数据增强。,视觉增强学习（RL）的关键挑战之一是学习可以推广到看不见的环境的政策。最近，旨在增强数据多样性的数据增强技术已证明在提高学习策略的概括能力方面已证明了绩效。但是，由于RL训练的敏感性，天真地应用数据增强，从而以任务不可能的方式转换每个像素，可能会遭受不稳定性和损害样品效率的损害，从而进一步加剧了概括性能。这种现象的核心是面对增强图像的动作分布和高变化的价值估计。为了减轻此问题，我们建议使用“ Visual RL”的任务吸引Lipschitz数据增强（TLDA），该数据可以明确识别与大Lipschitz常数相关的任务相关像素，并且仅增强了任务irrelexrelexerrelevervant像素。为了验证TLDA的有效性，我们对DeepMind Control Suite，Carla和DeepMind操纵任务进行了广泛的实验，表明TLDA提高了训练时间的样本效率和测试时间的概括。它的表现优于3个不同的视觉控制基准测试的先前最先进方法。,https://arxiv.org/abs/2202.09982,IJCAI,True,False,False,False
1737,Feature and Instance Joint Selection: A Reinforcement Learning Perspective.,"Wei Fan, Kunpeng Liu, Hao Liu, Hengshu Zhu, Hui Xiong, Yanjie Fu","Feature selection and instance selection are two important techniques of data processing. However, such selections have mostly been studied separately, while existing work towards the joint selection conducts feature/instance selection coarsely; thus neglecting the latent fine-grained interaction between feature space and instance space. To address this challenge, we propose a reinforcement learning solution to accomplish the joint selection task and simultaneously capture the interaction between the selection of each feature and each instance. In particular, a sequential-scanning mechanism is designed as action strategy of agents, and a collaborative-changing environment is used to enhance agent collaboration. In addition, an interactive paradigm introduces prior selection knowledge to help agents for more efficient exploration. Finally, extensive experiments on real-world datasets have demonstrated improved performances.",功能和实例联合选择：增强学习观点。,特征选择和实例选择是数据处理的两种重要技术。但是，这种选择大多是单独研究的，而对联合选择的现有工作则精致。因此，忽略了特征空间和实例空间之间的潜在细粒相互作用。为了应对这一挑战，我们提出了一种加强学习解决方案来完成联合选择任务，并同时捕获每个功能和每个实例的选择之间的相互作用。特别是，连续扫描机制被设计为代理的行动策略，并使用改变协作的环境来增强代理协作。此外，交互式范式引入了先前的选择知识，以帮助代理进行更有效的探索。最后，对现实世界数据集的广泛实验已证明了性能的改善。,https://arxiv.org/abs/2205.07867,IJCAI,True,False,False,False
1738,Zero-Shot Logit Adjustment.,"Dubing Chen, Yuming Shen, Haofeng Zhang, Philip H. S. Torr","Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses challenges in recognizing the novel classes in the test phase. The development of generative models enables current GZSL techniques to probe further into the semantic-visual link, culminating in a two-stage form that includes a generator and a classifier. However, existing generation-based methods focus on enhancing the generator's effect while neglecting the improvement of the classifier. In this paper, we first conduct an analysis of two properties of the generated pseudo unseen sample: bias and homogeneity. Then, we perform variational Bayesian inference to back-derive the evaluation metrics, which reflects the balance of the seen and unseen classes. As a consequence of our derivation, the aforementioned two properties are incorporated into the classifier training as seen-unseen priors via logit adjustment. The Zero-Shot Logit Adjustment further puts semantic-based classifiers into effect in generation-based GZSL. Our experiments demonstrate that the proposed technique achieves the state of the art when combined with the basic generator, and it can improve various generative zero-shot learning frameworks. Our codes are available on \url{https://github.com/cdb342/IJCAI-2022-ZLA}.",零射击logit调整。,基于语义描述者的广义零拍学习（GZSL）在识别测试阶段的新颖类时提出了挑战。生成模型的开发使当前的GZSL技术能够进一步探究语义 - 视觉链接，最终以包括生成器和分类器在内的两阶段形式达到顶点。但是，现有的基于一代的方法着重于增强发电机的效果，同时忽略了分类器的改进。在本文中，我们首先对产生的伪看不见的样本的两种特性进行分析：偏见和同质性。然后，我们执行各种贝叶斯推断以反向评估指标，这反映了可见和看不见的类别的平衡。由于我们的推导，上述两种特性通过logit调整纳入了分类器训练中，作为观察者的先验。零摄影的logit调整进一步使基于语义的分类器在基于一代的GZSL中生效。我们的实验表明，与基本发电机结合使用时，提出的技术可以实现最新技术的状态，并且可以改善各种生成性的零照片学习框架。我们的代码可在\ url {https://github.com/cdb342/ijcai-2022-zla}上获得。,https://arxiv.org/abs/2204.11822,IJCAI,True,False,False,False
1739,JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical Reinforcement Learning.,"Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang","Learning rational behaviors in open-world games like Minecraft remains to be challenging for Reinforcement Learning (RL) research due to the compound challenge of partial observability, high-dimensional visual perception and delayed reward. To address this, we propose JueWu-MC, a sample-efficient hierarchical RL approach equipped with representation learning and imitation learning to deal with perception and exploration. Specifically, our approach includes two levels of hierarchy, where the high-level controller learns a policy to control over options and the low-level workers learn to solve each sub-task. To boost the learning of sub-tasks, we propose a combination of techniques including 1) action-aware representation learning which captures underlying relations between action and representation, 2) discriminator-based self-imitation learning for efficient exploration, and 3) ensemble behavior cloning with consistency filtering for policy robustness. Extensive experiments show that JueWu-MC significantly improves sample efficiency and outperforms a set of baselines by a large margin. Notably, we won the championship of the NeurIPS MineRL 2021 research competition and achieved the highest performance score ever.",Juewu-MC：玩Minecraft，具有样品高效的层次增强学习。,在诸如Minecraft之类的开放世界游戏中学习有理行为，对于强化学习（RL）研究的挑战仍然是挑战，这是由于部分可观察性，高维视觉感知和延迟的奖励的复合挑战。为了解决这个问题，我们提出了Juewu-MC，这是一种具有代表性学习和模仿学习来处理感知和探索的样本效率层次RL方法。具体而言，我们的方法包括两个层次结构，高级控制器学习控制选项的策略，而低级工人学习解决每个子任务。为了促进子任务的学习，我们提出了包括1）行动感知表示学习的技术组合，该学习捕获了行动与代表之间的基本关系，2）基于歧视者的自我模拟学习，以进行有效探索，以及3）合奏行为克隆以一致性过滤的策略鲁棒性。广泛的实验表明，Juewu-MC显着提高了样品效率，并超过了一组基线的幅度。值得注意的是，我们赢得了Neurips Minerl 2021研究比赛的冠军，并取得了有史以来最高的成绩。,https://arxiv.org/abs/2112.04907,IJCAI,True,False,False,False
1740,Self-supervised Semantic Segmentation Grounded in Visual Concepts.,"Wenbin He, William Surmeier, Arvind Kumar Shekar, Liang Gou, Liu Ren","Unsupervised semantic segmentation requires assigning a label to every pixel without any human annotations. Despite recent advances in self-supervised representation learning for individual images, unsupervised semantic segmentation with pixel-level representations is still a challenging task and remains underexplored. In this work, we propose a self-supervised pixel representation learning method for semantic segmentation by using visual concepts (i.e., groups of pixels with semantic meanings, such as parts, objects, and scenes) extracted from images. To guide self-supervised learning, we leverage three types of relationships between pixels and concepts, including the relationships between pixels and local concepts, local and global concepts, as well as the co-occurrence of concepts. We evaluate the learned pixel embeddings and visual concepts on three datasets, including PASCAL VOC 2012, COCO 2017, and DAVIS 2017. Our results show that the proposed method gains consistent and substantial improvements over recent unsupervised semantic segmentation approaches, and also demonstrate that visual concepts can reveal insights into image datasets.",自我监督的语义分割以视觉概念为基础。,无监督的语义细分需要将标签分配给每个像素，而无需任何人类注释。尽管在单个图像的自我监督表示学习方面取得了进步，但使用像素级表示的无监督语义细分仍然是一项艰巨的任务，并且仍然没有被淘汰。在这项工作中，我们通过使用视觉概念（即具有语义含义的像素组，例如零件，对象和场景）提出一种自我监督的像素表示学习方法，以进行语义分割。为了指导自我监督的学习，我们利用像素和概念之间的三种类型的关系，包括像素与本地概念之间的关系，本地和全球概念以及概念的共发生。我们在包括Pascal VOC 2012，Coco 2017和Davis 2017的三个数据集上评估了学识渊博的像素嵌入和视觉概念。我们的结果表明，提议的方法对最近的无监督语义细分方法进行了一致性和实质性改进，并证明了视觉概念的视觉概念。可以向图像数据集揭示洞察力。,https://arxiv.org/abs/2203.13868,IJCAI,True,False,False,False
1741,Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble.,"Zhengyu Yang, Kan Ren, Xufang Luo, Minghuan Liu, Weiqing Liu, Jiang Bian, Weinan Zhang, Dongsheng Li","It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications like financial trading and logistic system due to the noisy observation and environment shifting between training and evaluation. Thus, it requires both high sample efficiency and generalization for resolving real-world tasks. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. Considering the great performance of ensemble methods on both accuracy and generalization in supervised learning (SL), we design a robust and applicable method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove EPPO increases exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.",迈向适用的强化学习：通过政策合奏提高概括和样本效率。,强化学习（RL）算法在现实世界中取得成功，例如金融交易和逻辑系统，这是一项挑战，这是由于训练和评估之间的嘈杂观察和环境变化，这是一项挑战。因此，它需要高样本效率和概括来解决现实世界任务。但是，在这种情况下，直接应用典型的RL算法会导致性能差。考虑到合奏方法在监督学习中的准确性和概括（SL）中的出色表现，我们设计了一种名为合奏近端策略优化（EPPO）的强大而适用的方法，该方法以端到端的方式学习合奏政策。值得注意的是，EPPO将每个政策和策略结合在一起，并同时优化两者。此外，EPPO在政策空间上采用了多样性增强的正则化，有助于普遍看不见的国家并促进勘探。从理论上讲，我们证明了EPPO提高了勘探功效，并且通过对各种任务的全面实验评估，我们证明EPPO可实现更高的效率，并且与香草策略优化算法和其他集合方法相比，EPPO对现实世界应用具有强大的效率。代码和补充材料可在https://seqml.github.io/eppo上获得。,https://arxiv.org/abs/2205.09284,IJCAI,True,False,False,False
1742,"""My nose is running."" ""Are you also coughing?"": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics.","Wenge Liu, Yi Cheng, Hao Wang, Jianheng Tangi, Yafei Liu, Ruihui Zhao, Wenjie Li, Yefeng Zheng, Xiaodan Liang","With the rise of telemedicine, the task of developing Dialogue Systems for Medical Diagnosis (DSMD) has received much attention in recent years. Different from early researches that needed to rely on extra human resources and expertise to help construct the system, recent researches focused on how to build DSMD in a purely data-driven manner. However, the previous data-driven DSMD methods largely overlooked the system interpretability, which is critical for a medical application, and they also suffered from the data sparsity issue at the same time. In this paper, we explore how to bring interpretability to data-driven DSMD. Specifically, we propose a more interpretable decision process to implement the dialogue manager of DSMD by reasonably mimicking real doctors' inquiry logics, and we devise a model with highly transparent components to conduct the inference. Moreover, we collect a new DSMD dataset, which has a much larger scale, more diverse patterns and is of higher quality than the existing ones. The experiments show that our method obtains 7.7%, 10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three datasets, demonstrating the effectiveness of its rational decision process and model design. Our codes and the GMD-12 dataset are available at https://github.com/lwgkzl/BR-Agent.",“我的鼻子在奔跑。”“你也咳嗽吗？”：建立具有可解释的查询逻辑的医学诊断剂。,随着远程医疗的兴起，近年来，开发医学诊断对话系统（DSMD）的任务受到了很多关注。与需要依靠额外的人力资源和专业知识来帮助构建系统的早期研究不同，最近的研究重点是如何以纯粹的数据驱动方式构建DSMD。但是，以前的数据驱动的DSMD方法在很大程度上忽略了系统的解释性，这对于医疗应用至关重要，并且他们还同时遇到了数据稀疏问题。在本文中，我们探讨了如何为数据驱动的DSMD带来解释性。具体而言，我们提出了一个更容易解释的决策过程，通过合理地模仿真正的医生的询问逻辑来实施DSMD的对话经理，并设计了一个具有高度透明组件来进行推理的模型。此外，我们收集了一个新的DSMD数据集，该数据集具有更大的规模，更多样化的模式，并且质量比现有模式更高。实验表明，我们的方法在三个数据集上分别获得了7.7％，10.0％，3.0％的绝对诊断准确性提高，这证明了其合理决策过程和模型设计的有效性。我们的代码和GMD-12数据集可在https://github.com/lwgkzl/br-agent上找到。,https://arxiv.org/abs/2204.13953,IJCAI,True,False,False,False
1743,Communicative Subgraph Representation Learning for Multi-Relational Inductive Drug-Gene Interaction Prediction.,"Jiahua Rao, Shuangjia Zheng, Sijie Mai, Yuedong Yang","Illuminating the interconnections between drugs and genes is an important topic in drug development and precision medicine. Currently, computational predictions of drug-gene interactions mainly focus on the binding interactions without considering other relation types like agonist, antagonist, etc. In addition, existing methods either heavily rely on high-quality domain features or are intrinsically transductive, which limits the capacity of models to generalize to drugs/genes that lack external information or are unseen during the training process. To address these problems, we propose a novel Communicative Subgraph representation learning for Multi-relational Inductive drug-Gene interactions prediction (CoSMIG), where the predictions of drug-gene relations are made through subgraph patterns, and thus are naturally inductive for unseen drugs/genes without retraining or utilizing external domain features. Moreover, the model strengthened the relations on the drug-gene graph through a communicative message passing mechanism. To evaluate our method, we compiled two new benchmark datasets from DrugBank and DGIdb. The comprehensive experiments on the two datasets showed that our method outperformed state-of-the-art baselines in the transductive scenarios and achieved superior performance in the inductive ones. Further experimental analysis including LINCS experimental validation and literature verification also demonstrated the value of our model.",跨性别归纳性药物 - 基因相互作用预测的交流子图表学习。,照亮药物和基因之间的互连是药物开发和精确医学的重要主题。当前，药物相互作用的计算预测主要集中于结合相互作用，而无需考虑其他关系类型，例如激动剂，拮抗剂等。此外，现有方法要么很大程度上依赖于高质量的域特征，要么是本质上的转导，这限制了能力概括到缺乏外部信息或在训练过程中看不见的药物/基因的模型。为了解决这些问题，我们提出了一种新型的沟通子图表来学习，用于多种关系归纳性药物 - 基因相互作用预测（Cosmig），其中通过子图模式进行了药物关系的预测，因此自然地诱导了看不见的药物/没有重新训练或利用外部域特征的基因。此外，该模型通过传播机制加强了毒品基因图上的关系。为了评估我们的方法，我们从药品银行和DGIDB编辑了两个新的基准数据集。这两个数据集的全面实验表明，我们的方法在转导方案中优于最先进的基线，并在归纳诱导的情况下实现了卓越的性能。包括LINCS实验验证和文献验证在内的进一步的实验分析也证明了我们模型的价值。,https://arxiv.org/abs/2205.05957,IJCAI,True,False,False,False
1744,Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment.,"Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa","Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.",通过测试时间类条件特征对齐方式稳健视觉变压器，而无需从划痕重新划痕。,Vision Transformer（VIT）在图像处理中变得越来越流行。具体而言，我们研究了测试时间适应（TTA）对VIT的有效性，VIT是一种已经出现的技术，可以自行纠正其在测试时间期间的预测。首先，我们在VIT-B16和VIT-L16上基准了各种测试时间适应方法。结果表明，使用适当的损耗函数时，TTA对VIT有效，并且先前的投入（明智地选择调制参数）是不需要的。基于观察结果，我们提出了一种称为类条件特征对齐（CFA）的新的测试时间适应方法，该方法将类别条件分布的差异和在线源中隐藏表示的整个分布差异最小化，在线中的整个分布差异方式。图像分类任务（CIFAR-10-C，CIFAR-100-C和Imagenet-C）和域适应性（Digits DataSet和Imagenet-Sketch）的图像分类任务的实验表明，CFA稳定地超过了各种数据集中的现有基础。我们还通过在RESNET，MLP混合和几种VIT变体（Vit-augreg，Deit和Beit）上实验来验证CFA是模型不可知论。使用BEIT主链，CFA在Imagenet-C上达到了19.8％的TOP-1错误率，表现优于现有的测试时间适应基线44.0％。这是不需要改变训练阶段的TTA方法中的最新结果。,https://arxiv.org/abs/2206.13951,IJCAI,True,False,False,False
1745,Better Collective Decisions via Uncertainty Reduction.,"Shiri Alouf-Heffetz, Laurent Bulteau, Edith Elkind, Nimrod Talmon, Nicholas Teh","We consider an agent community wishing to decide on several binary issues by means of issue-by-issue majority voting. For each issue and each agent, one of the two options is better than the other. However, some of the agents may be confused about some of the issues, in which case they may vote for the option that is objectively worse for them. A benevolent external party wants to help the agents to make better decisions, i.e., select the majority-preferred option for as many issues as possible. This party may have one of the following tools at its disposal: (1) educating some of the agents, so as to enable them to vote correctly on all issues, (2) appointing a subset of highly competent agents to make decisions on behalf of the entire group, or (3) guiding the agents on how to delegate their votes to other agents, in a way that is consistent with the agents' opinions. For each of these tools, we study the complexity of the decision problem faced by this external party, obtaining both NP-hardness results and fixed-parameter tractability results.",通过减少不确定性来更好地集体决策。,我们考虑一个希望通过逐个发行的多数投票来决定几个二进制问题的代理商社区。对于每个问题和每个代理，两个选项之一都比另一个更好。但是，某些代理商可能会对某些问题感到困惑，在这种情况下，他们可能会投票赞成客观上对他们的选择。仁慈的外部方希望帮助代理商做出更好的决定，即，为尽可能多的问题选择多数偏爱的选项。该方可能拥有以下工具之一：（1）教育一些代理商，以便使他们能够正确地对所有问题进行投票，（2）任命一部分高度称职的代理人代表代表做出决定整个小组，或（3）指导代理商如何将其投票委托给其他代理商，以与代理商的意见一致的方式。对于这些工具，我们研究了该外部方所面临的决策问题的复杂性，同时获得了NP硬度结果和固定参数的障碍结果。,https://arxiv.org/abs/2207.04983,IJCAI,True,False,False,False
1746,Learning Meta Word Embeddings by Unsupervised Weighted Concatenation of Source Embeddings.,Danushka Bollegala,"Given multiple source word embeddings learnt using diverse algorithms and lexical resources, meta word embedding learning methods attempt to learn more accurate and wide-coverage word embeddings.   Prior work on meta-embedding has repeatedly discovered that simple vector concatenation of the source embeddings to be a competitive baseline.   However, it remains unclear as to why and when simple vector concatenation can produce accurate meta-embeddings.   We show that weighted concatenation can be seen as a spectrum matching operation between each source embedding and the meta-embedding, minimising the pairwise inner-product loss.   Following this theoretical analysis, we propose two \emph{unsupervised} methods to learn the optimal concatenation weights for creating meta-embeddings from a given set of source embeddings.   Experimental results on multiple benchmark datasets show that the proposed weighted concatenated meta-embedding methods outperform previously proposed meta-embedding learning methods.",通过无监督的加权串联来学习元单词嵌入。,给定多种源单词嵌入使用了多种算法和词汇资源，元单词嵌入学习方法试图学习更准确和宽覆盖的单词嵌入。先前关于元装置的工作反复发现，简单的媒介嵌入嵌入是竞争性的基线。但是，尚不清楚为什么以及何时何时简单的矢量串联会产生准确的元嵌入。我们表明，加权串联可以看作是每个源嵌入和元装置之间的频谱匹配操作，从而最大程度地减少了成对的内部产物损失。经过理论分析，我们提出了两种\ emph {无监督}方法，以学习从给定的一组源嵌入的最佳串联权重来创建元嵌入的权重。多个基准数据集的实验结果表明，所提出的加权串联元装置方法的表现优于先前提出的元装置学习方法。,https://arxiv.org/abs/2204.12386,IJCAI,True,False,False,False
1747,The Dichotomous Affiliate Stable Matching Problem: Approval-Based Matching with Applicant-Employer Relations.,"Marina Knittel, Samuel Dooley, John P. Dickerson","While the stable marriage problem and its variants model a vast range of matching markets, they fail to capture complex agent relationships, such as the affiliation of applicants and employers in an interview marketplace. To model this problem, the existing literature on matching with externalities permits agents to provide complete and total rankings over matchings based off of both their own and their affiliates' matches. This complete ordering restriction is unrealistic, and further the model may have an empty core. To address this, we introduce the Dichotomous Affiliate Stable Matching (DASM) Problem, where agents' preferences indicate dichotomous acceptance or rejection of another agent in the marketplace, both for themselves and their affiliates.   We also assume the agent's preferences over entire matchings are determined by a general weighted valuation function of their (and their affiliates') matches. Our results are threefold: (1) we use a human study to show that real-world matching rankings follow our assumed valuation function; (2) we prove that there always exists a stable solution by providing an efficient, easily-implementable algorithm that finds such a solution; and (3) we experimentally validate the efficiency of our algorithm versus a linear-programming-based approach.",二分法会员稳定匹配问题：基于批准的匹配与申请人雇主关系。,尽管稳定的婚姻问题及其变体模拟了广泛的匹配市场，但他们无法捕获复杂的代理关系，例如访谈市场中申请人和雇主的隶属关系。为了模拟这个问题，现有的有关与外部性匹配的文献允许代理可以在基于自己的和分支机构的比赛中提供完整和总排名。这种完整的订购限制是不现实的，此外，该模型可能具有空心。为了解决这个问题，我们介绍了二分法会员稳定的匹配（DASM）问题，在其中代理人的偏好表明对自己和他们的关联公司的市场中其他代理商的二分法接受或拒绝。我们还假设代理商对整个匹配的偏好是由其（及其分支机构）匹配的一般加权估值函数确定的。我们的结果是三倍：（1）我们使用人类研究表明现实世界中的匹配排名遵循我们假定的估值函数；（2）我们证明，通过提供有效，易于实现的算法来找到这种解决方案，始终存在稳定的解决方案。（3）我们通过实验验证算法的效率与基于线性编程的方法。,https://arxiv.org/abs/2202.11095,IJCAI,True,False,False,False
1748,Online Planning in POMDPs with Self-Improving Simulators.,"Jinke He, Miguel Suau, Hendrik Baier, Michael Kaisers, Frans A. Oliehoek","How can we plan efficiently in a large and complex environment when the time budget is limited? Given the original simulator of the environment, which may be computationally very demanding, we propose to learn online an approximate but much faster simulator that improves over time. To plan reliably and efficiently while the approximate simulator is learning, we develop a method that adaptively decides which simulator to use for every simulation, based on a statistic that measures the accuracy of the approximate simulator. This allows us to use the approximate simulator to replace the original simulator for faster simulations when it is accurate enough under the current context, thus trading off simulation speed and accuracy. Experimental results in two large domains show that when integrated with POMCP, our approach allows to plan with improving efficiency over time.",带有自我改善的模拟器的POMDP的在线计划。,当时间预算有限时，我们如何在庞大而复杂的环境中有效地计划？鉴于环境的原始模拟器，在计算上可能非常苛刻，我们建议在线学习一个近似但更快的模拟器，随着时间的推移会有所改善。为了在近似模拟器学习时可靠，有效地计划，我们开发了一种方法，该方法基于测量近似模拟器的准确性的统计量来适应用于每个模拟的模拟器用于每个模拟。这使我们可以使用近似模拟器在当前上下文中足够准确时替换原始模拟器以更快的模拟，从而使模拟速度和准确性交换。两个大域中的实验结果表明，与POMCP集成时，我们的方法可以随着时间的推移提高效率而计划。,https://arxiv.org/abs/2201.11404,IJCAI,True,False,False,False
1749,Adaptive Convolutional Dictionary Network for CT Metal Artifact Reduction.,"Hong Wang, Yuexiang Li, Deyu Meng, Yefeng Zheng","Inspired by the great success of deep neural networks, learning-based methods have gained promising performances for metal artifact reduction (MAR) in computed tomography (CT) images. However, most of the existing approaches put less emphasis on modelling and embedding the intrinsic prior knowledge underlying this specific MAR task into their network designs. Against this issue, we propose an adaptive convolutional dictionary network (ACDNet), which leverages both model-based and learning-based methods. Specifically, we explore the prior structures of metal artifacts, e.g., non-local repetitive streaking patterns, and encode them as an explicit weighted convolutional dictionary model. Then, a simple-yet-effective algorithm is carefully designed to solve the model. By unfolding every iterative substep of the proposed algorithm into a network module, we explicitly embed the prior structure into a deep network, \emph{i.e.,} a clear interpretability for the MAR task. Furthermore, our ACDNet can automatically learn the prior for artifact-free CT images via training data and adaptively adjust the representation kernels for each input CT image based on its content. Hence, our method inherits the clear interpretability of model-based methods and maintains the powerful representation ability of learning-based methods. Comprehensive experiments executed on synthetic and clinical datasets show the superiority of our ACDNet in terms of effectiveness and model generalization. {\color{blue}{{\textit{Code is available at {\url{https://github.com/hongwang01/ACDNet}}.}}}}",CT金属伪像还原的自适应卷积词典网络。,受深神经网络的巨大成功的启发，基于学习的方法在计算机断层扫描（CT）图像中获得了有希望的金属伪像（MAR）的表现。但是，大多数现有方法更加强调建模并嵌入本特定MAR任务的内在先验知识中，将其纳入其网络设计中。在这个问题上，我们提出了一个自适应卷积词典网络（ACDNET），该网络利用基于模型的方法和基于学习的方法。具体而言，我们探讨了金属伪像的先前结构，例如非本地重复条纹模式，并将其编码为显式加权卷积词典模型。然后，仔细设计了一种简单的算法来解决模型。通过将所提出算法的每个迭代取代展开到网络模块中，我们将先前的结构明确嵌入到深网中，\ emph {i.e。，}对MAR任务的明确解释性。此外，我们的ACDNET可以通过训练数据自动学习无伪影CT图像的先验，并根据其内容自适应地调整每个输入CT图像的表示内核。因此，我们的方法继承了基于模型的方法的明确解释性，并保持了基于学习的方法的强大表示能力。在合成和临床数据集上执行的综合实验表明，在有效性和模型概括方面，我们的ACDNET的优越性。{\ color {blue} {{\ textIt {代码可在{\ url {https://github.com/hongwang01/acdnet}}}}}}}}}}}}}}}},https://arxiv.org/abs/2205.07471,IJCAI,True,False,False,False
1750,Two for One & One for All: Two-Sided Manipulation in Matching Markets.,"Hadi Hosseini, Fatima Umar, Rohit Vaish","Strategic behavior in two-sided matching markets has been traditionally studied in a ""one-sided"" manipulation setting where the agent who misreports is also the intended beneficiary. Our work investigates ""two-sided"" manipulation of the deferred acceptance algorithm where the misreporting agent and the manipulator (or beneficiary) are on different sides. Specifically, we generalize the recently proposed accomplice manipulation model (where a man misreports on behalf of a woman) along two complementary dimensions: (a) the two for one model, with a pair of misreporting agents (man and woman) and a single beneficiary (the misreporting woman), and (b) the one for all model, with one misreporting agent (man) and a coalition of beneficiaries (all women).   Our main contribution is to develop polynomial-time algorithms for finding an optimal manipulation in both settings. We obtain these results despite the fact that an optimal one for all strategy fails to be inconspicuous, while it is unclear whether an optimal two for one strategy satisfies the inconspicuousness property. We also study the conditions under which stability of the resulting matching is preserved. Experimentally, we show that two-sided manipulations are more frequently available and offer better quality matches than their one-sided counterparts.",所有人都有两个：在匹配市场中的双面操纵。,传统上，在“单面”操纵环境中研究了双方匹配市场中的战略行为，在这种环境中，虚假报告的代理商也是预期的受益人。我们的工作调查了对递延接受算法的“双面”操纵，其中错误报告的代理和操纵器（或受益人）处于不同方面。具体而言，我们沿两个互补的维度概括了最近提出的同伙操纵模型（男人代表女人的失误）：（a）两种模型的两种模型，一对错误的媒介（男人和女人）和一个受益人（犯错的妇女）和（b）所有模型的一个模特，有一位虚假的代理人（男人）和一个受益人联盟（所有妇女）。我们的主要贡献是开发多项式时间算法，以在两种情况下找到最佳操作。尽管事实是，所有策略的最佳结果都不是不明显的，但我们获得了这些结果，而尚不清楚一个策略的最佳两个策略是否满足了不起眼的特性。我们还研究了保留结果匹配的稳定性的条件。在实验上，我们表明双面操作更频繁地可用，并且比单方面的操作更优质。,https://arxiv.org/abs/2201.08774,IJCAI,True,False,False,False
1751,Learning to Assemble Geometric Shapes.,"Jinhwi Lee, Jungtaek Kim, Hyunsoo Chung, Jaesik Park, Minsu Cho","Assembling parts into an object is a combinatorial problem that arises in a variety of contexts in the real world and involves numerous applications in science and engineering. Previous related work tackles limited cases with identical unit parts or jigsaw-style parts of textured shapes, which greatly mitigate combinatorial challenges of the problem. In this work, we introduce the more challenging problem of shape assembly, which involves textureless fragments of arbitrary shapes with indistinctive junctions, and then propose a learning-based approach to solving it. We demonstrate the effectiveness on shape assembly tasks with various scenarios, including the ones with abnormal fragments (e.g., missing and distorted), the different number of fragments, and different rotation discretization.",学习组装几何形状。,将零件组装成对象是一个组合问题，在现实世界中的各种情况下都会出现，并且涉及科学和工程学中的许多应用。以前的相关工作可以解决限制案例，其单位零件或拼图形状的部分相同，这大大减轻了问题的组合挑战。在这项工作中，我们介绍了形状组装的更具挑战性的问题，该问题涉及具有模糊连接的任意形状的无纹理碎片，然后提出了一种基于学习的方法来解决它。我们证明了具有各种情况的形状组装任务的有效性，包括具有异常片段（例如缺失和扭曲），不同数量的片段和不同旋转离散化的情况。,https://arxiv.org/abs/2205.11809,IJCAI,True,False,False,False
1752,Public Signaling in Bayesian Ad Auctions.,"Francesco Bacchiocchi, Matteo Castiglioni, Alberto Marchesi, Giulia Romano, Nicola Gatti","We study signaling in Bayesian ad auctions, in which bidders' valuations depend on a random, unknown state of nature. The auction mechanism has complete knowledge of the actual state of nature, and it can send signals to bidders so as to disclose information about the state and increase revenue. For instance, a state may collectively encode some features of the user that are known to the mechanism only, since the latter has access to data sources unaccessible to the bidders. We study the problem of computing how the mechanism should send signals to bidders in order to maximize revenue. While this problem has already been addressed in the easier setting of second-price auctions, to the best of our knowledge, our work is the first to explore ad auctions with more than one slot. In this paper, we focus on public signaling and VCG mechanisms, under which bidders truthfully report their valuations. We start with a negative result, showing that, in general, the problem does not admit a PTAS unless P = NP, even when bidders' valuations are known to the mechanism. The rest of the paper is devoted to settings in which such negative result can be circumvented. First, we prove that, with known valuations, the problem can indeed be solved in polynomial time when either the number of states d or the number of slots m is fixed. Moreover, in the same setting, we provide an FPTAS for the case in which bidders are single minded, but d and m can be arbitrary. Then, we switch to the random valuations setting, in which these are randomly drawn according to some probability distribution. In this case, we show that the problem admits an FPTAS, a PTAS, and a QPTAS, when, respectively, d is fixed, m is fixed, and bidders' valuations are bounded away from zero.",贝叶斯广告拍卖中的公共信号。,我们研究贝叶斯AD扣的信号传导，其中投标人的估值取决于随机的，未知的自然状态。拍卖机制完全了解自然的实际状态，并且可以向投标人发送信号，以披露有关国家的信息并增加收入。例如，一个状态可以集体编码仅是该机制已知的用户的某些功能，因为后者可以访问投标人无法访问的数据源。我们研究计算该机制应如何向竞标者发送信号以最大化收入的问题。据我们所知，虽然在更轻松的第二价格拍卖的设置中已经解决了这个问题，但我们的工作是第一个探索具有多个插槽的广告拍卖的工作。在本文中，我们着重于公共信号传导和VCG机制，在这些机制下，投标人真实地报告了他们的估值。我们从负面结果开始，表明通常，即使P = NP，即使竞标者的估值是该机制已知的，否则问题也不会接收PTA。本文的其余部分专门用于可以规避这种负面结果的设置。首先，我们证明，凭借已知的估值，当固定状态D或插槽M的数量固定时，确实可以在多项式时间内解决问题。此外，在相同的环境中，我们为投标人单身的情况提供了一个FPTA，但D和M可以任意。然后，我们切换到随机估值设置，其中根据某些概率分布随机绘制这些设置。在这种情况下，我们表明该问题承认了一个FPTA，PTA和QPTA，分别固定D时，M固定了D，并且竞标者的估值远离零。,https://arxiv.org/abs/2201.09728,IJCAI,True,False,False,False
1753,How to Sample Approval Elections?,"Stanisław Szufa, Piotr Faliszewski, Łukasz Janeczko, Martin Lackner, Arkadii Slinko, Krzysztof Sornat, Nimrod Talmon","We study the multifaceted question of how to sample approval elections in a meaningful way. Our analysis aims to discern the properties of various statistical cultures (both established and new ones). Based on the map-of-elections framework by Szufa et al. [2020], we graphically represent statistical cultures; and, by that, provide an intuitive understanding of their differences and properties.",如何采样批准选举？,我们研究了如何以有意义的方式进行批准选举的多方面问题。我们的分析旨在辨别各种统计文化（既建立和新统计文化）的特性。基于Szufa等人的当前框架。[2020]，我们以图形方式表示统计文化；因此，对其差异和特性提供了直观的理解。,https://arxiv.org/abs/2207.01140,IJCAI,True,False,False,False
1754,DPVI: A Dynamic-Weight Particle-Based Variational Inference Framework,"Chao Zhang, Zhijian Li, Hui Qian, Xin Du","The recently developed Particle-based Variational Inference (ParVI) methods drive the empirical distribution of a set of \emph{fixed-weight} particles towards a given target distribution $\pi$ by iteratively updating particles' positions. However, the fixed weight restriction greatly confines the empirical distribution's approximation ability, especially when the particle number is limited. In this paper, we propose to dynamically adjust particles' weights according to a Fisher-Rao reaction flow. We develop a general Dynamic-weight Particle-based Variational Inference (DPVI) framework according to a novel continuous composite flow, which evolves the positions and weights of particles simultaneously. We show that the mean-field limit of our composite flow is actually a Wasserstein-Fisher-Rao gradient flow of certain dissimilarity functional $\mathcal{F}$, which leads to a faster decrease of $\mathcal{F}$ than the Wasserstein gradient flow underlying existing fixed-weight ParVIs. By using different finite-particle approximations in our general framework, we derive several efficient DPVI algorithms. The empirical results demonstrate the superiority of our derived DPVI algorithms over their fixed-weight counterparts.",DPVI：基于动态的粒子变分推理框架,最近开发的基于粒子的变异推理（PARVI）方法通过迭代更新粒子的位置，驱动了一组\ emph {fixed-jeight}粒子的经验分布向给定的目标分布$ \ pi $。但是，固定重量限制极大地限制了经验分布的近似能力，尤其是当粒子数受到限制时。在本文中，我们建议根据Fisher-Rao反应流动地调节颗粒的重量。我们根据一种新型的连续复合流，开发了一个总体动态粒子的变分推断（DPVI）框架，该框架同时进化了粒子的位置和权重。我们表明，我们的复合流的平均场限制实际上是某些差异功能$ \ MATHCAL {F} $的Wasserstein-fisher-fisher-rao梯度流，这会导致比$ \ Mathcal {f} $更快地减少。Wasserstein梯度流源是现有的固定重量parvis。通过在我们的一般框架中使用不同的有限粒子近似值，我们得出了几种有效的DPVI算法。经验结果表明，我们衍生的DPVI算法优于其固定重量对应物。,https://arxiv.org/abs/2112.00945,IJCAI,True,False,False,False
1755,Can Buyers Reveal for a Better Deal?,"Daniel Halpern, Gregory Kehne, Jamie Tucker-Foltz","We study small-scale market interactions in which buyers are allowed to credibly reveal partial information about their types to the seller. Previous recent work has studied the special case where there is one buyer and one good, showing that such communication can simultaneously improve social welfare and ex ante buyer utility. With multiple buyers, we find that the buyer-optimal signalling schemes from the one-buyer case are actually harmful to buyer welfare. Moreover, we prove several impossibility results showing that, with either multiple i.i.d. buyers or multiple i.i.d. goods, maximizing buyer utility can be at odds with social efficiency, which is a surprising contrast to the one-buyer, one-good case. Finally, we investigate the computational tractability of implementing desirable equilibrium outcomes. We find that, even with one buyer and one good, optimizing buyer utility is generally NP-hard, but tractable in a practical restricted setting.",买家可以透露更好的交易吗？,我们研究了小规模的市场互动，其中允许买家可靠地向卖方揭示有关其类型的部分信息。以前的工作研究了一个特殊情况，其中有一个买家和一个好的情况，表明这种交流可以同时改善社会福利和事前买家实用程序。有了多个买家，我们发现来自单一购买者案件的买家最佳信号计划实际上对买方福利有害。此外，我们证明了几个不可能的结果表明，任一多个I.I.D.买家或多个I.I.D.商品，最大化买方公用事业可能与社会效率矛盾，这与一个购买者，一个好的案例形成了鲜明的对比。最后，我们研究了实施理想平衡结果的计算障碍。我们发现，即使有一个买家和一个好的买家，优化买家公用事业通常是NP-HARD，但可以在实际限制环境中进行操作。,https://arxiv.org/abs/2106.13882,IJCAI,True,False,False,False
1756,Strategyproof Mechanisms for Group-Fair Facility Location Problems.,"Houyu Zhou, Minming Li, Hau Chan","Motivated by the societal need to provide fair accessibility or representation among groups of agents, we study the group-fair facility location problems where agents are divided into groups based on criteria such as race, gender, or age. The agents are located on a real line, modeling agents' private ideal preferences/points for the facility's location (e.g., a public school or representative). Our aim is to design mechanisms to locate a facility to (approximately) minimize the costs of groups of agents to the facility fairly while eliciting the agents' private locations truthfully. We first introduce various well-motivated group-fair cost objectives and show that many natural objectives have an unbounded approximation ratio. We then consider the objectives of minimizing the maximum total group cost and minimizing the average group cost. For the first objective, we show that the approximation ratio of the median mechanism depends on the number of groups and provide a new group-based mechanism with an approximation ratio of 3. For the second objective, the median mechanism obtains a ratio of 3, and we propose a randomized mechanism that obtains a better approximation ratio. We also provide lower bounds for both objectives. We then study the notion of intergroup and intragroup fairness that measures fairness between groups and within each group. We consider various objectives and provide mechanisms with tight approximation ratios.",集体生气设施位置问题的策略性防护机制。,由于社会需要提供公平的可访问性或代理人群体的代表性的动机，我们研究了集体生气设施的位置问题，在这些设施中，代理商根据种族，性别或年龄等标准将代理分为组。这些代理位于真实的线上，为设施的位置（例如，公立学校或代表）建模了代理商的私人理想偏好/点。我们的目的是设计机制，以设置设施，以公平地（大致）将代理团体的成本公平地降低到设施，同时真实地吸引了代理商的私人位置。我们首先引入各种动机的集体成本目标，并表明许多自然目标的近似比率是无限的。然后，我们考虑最大程度地降低总组成本并最小化平均小组成本的目标。对于第一个目标，我们表明中值机理的近似比取决于组的数量，并提供了一个新的基于组的机制，其近似值比为3。对于第二个目标，中值机制获得了3，3，3，我们提出了一种随机机制，该机制获得了更好的近似值。我们还为这两个目标提供了下限。然后，我们研究了衡量各组之间和每个组内公平性的组间和群内公平性的概念。我们考虑各种目标，并提供具有紧密近似比的机制。,https://arxiv.org/abs/2107.05175,IJCAI,True,False,False,False
1757,Learn to Reverse DNNs from AI Programs Automatically.,"Simin Chen, Hamed Khanpour, Cong Liu, Wei Yang","With the privatization deployment of DNNs on edge devices, the security of on-device DNNs has raised significant concern. To quantify the model leakage risk of on-device DNNs automatically, we propose NNReverse, the first learning-based method which can reverse DNNs from AI programs without domain knowledge. NNReverse trains a representation model to represent the semantics of binary code for DNN layers. By searching the most similar function in our database, NNReverse infers the layer type of a given function's binary code. To represent assembly instructions semantics precisely, NNReverse proposes a more fine-grained embedding model to represent the textual and structural-semantic of assembly functions.",学会自动从AI程序反向DNN。,随着DNN在边缘设备上的私有化部署，DNNS的安全性引起了重大关注。为了自动量化设备DNNS的模型泄漏风险，我们提出了NNREVERSE，这是第一个基于学习的方法，它可以从AI程序中反向DNN，而无需域知识。NNREVERSE训练代表模型，以表示DNN层的二进制代码的语义。通过在我们的数据库中搜索最相似的函数，NnReverse Innreverse Inde Inde Inde dive dister dunction的二进制代码的层类型。为了准确地表示汇编指令语义，Nnreverse提出了一个更细粒度的嵌入模型，以表示组装函数的文本和结构语义。,https://arxiv.org/abs/2205.10364,IJCAI,True,False,False,False
1758,Maxmin Participatory Budgeting.,"Gogulapati Sreedurga, Mayank Ratan Bhardwaj, Y. Narahari","Participatory Budgeting (PB) is a popular voting method by which a limited budget is divided among a set of projects, based on the preferences of voters over the projects. PB is broadly categorised as divisible PB (if the projects are fractionally implementable) and indivisible PB (if the projects are atomic). Egalitarianism, an important objective in PB, has not received much attention in the context of indivisible PB. This paper addresses this gap through a detailed study of a natural egalitarian rule, Maxmin Participatory Budgeting (MPB), in the context of indivisible PB. Our study is in two parts: (1) computational (2) axiomatic. In the first part, we prove that MPB is computationally hard and give pseudo-polynomial time and polynomial-time algorithms when parameterized by certain well-motivated parameters. We propose an algorithm that achieves for MPB, additive approximation guarantees for restricted spaces of instances and empirically show that our algorithm in fact gives exact optimal solutions on real-world PB datasets. We also establish an upper bound on the approximation ratio achievable for MPB by the family of exhaustive strategy-proof PB algorithms. In the second part, we undertake an axiomatic study of the MPB rule by generalizing known axioms in the literature. Our study leads to the proposal of a new axiom, maximal coverage, which captures fairness aspects. We prove that MPB satisfies maximal coverage.",Maxmin参与预算。,参与式预算（PB）是一种流行的投票方法，基于选民对项目的偏好，通过该方法将有限的预算分为一组项目。PB广泛地归类为可分开的PB（如果项目是可以分数实现的）和不可分割的PB（如果项目是原子的）。平等主义是PB的一个重要目标，在不可分割的PB背景下并没有得到太多关注。本文通过在不可分割的PB的背景下对自然平等统治（Maxmin参与预算（MPB））的详细研究解决了这一差距。我们的研究分为两部分：（1）计算（2）公理。在第一部分中，我们证明MPB在计算上是硬性的，并在通过某些良好动机参数参数化时给出伪多项式时间和多项式时间算法。我们提出了一种实现MPB的算法，用于限制实例空间的添加近似保证，并从经验上表明我们的算法实际上提供了现实世界中PB数据集的精确最佳解决方案。我们还建立了由详尽策略PB算法的家族为MPB实现的近似比的上限。在第二部分中，我们通过在文献中概括已知的公理来对MPB规则进行公理研究。我们的研究导致了新的公理，最大覆盖范围的提议，该覆盖范围捕捉了公平方面。我们证明MPB满足最大覆盖范围。,https://arxiv.org/abs/2204.13923,IJCAI,True,False,False,False
1759,Forming Effective Human-AI Teams: Building Machine Learning Models that Complement the Capabilities of Multiple Experts.,"Patrick Hemmer, Sebastian Schellhammer, Michael Vössing, Johannes Jakubik, Gerhard Satzger","Machine learning (ML) models are increasingly being used in application domains that often involve working together with human experts. In this context, it can be advantageous to defer certain instances to a single human expert when they are difficult to predict for the ML model. While previous work has focused on scenarios with one distinct human expert, in many real-world situations several human experts with varying capabilities may be available. In this work, we propose an approach that trains a classification model to complement the capabilities of multiple human experts. By jointly training the classifier together with an allocation system, the classifier learns to accurately predict those instances that are difficult for the human experts, while the allocation system learns to pass each instance to the most suitable team member -- either the classifier or one of the human experts. We evaluate our proposed approach in multiple experiments on public datasets with ""synthetic"" experts and a real-world medical dataset annotated by multiple radiologists. Our approach outperforms prior work and is more accurate than the best human expert or a classifier. Furthermore, it is flexibly adaptable to teams of varying sizes and different levels of expert diversity.",组成有效的人类团队：建立与多位专家能力相辅相成的机器学习模型。,机器学习（ML）模型越来越多地被用于通常涉及与人类专家合作的应用领域。在这种情况下，当很难预测ML模型时，将某些实例推荐给单个人类专家可能是有利的。尽管以前的工作重点是与一位截然不同的人类专家的场景，但在许多现实情况下，可能会有一些具有不同功能的人类专家。在这项工作中，我们提出了一种培训分类模型以补充多名人类专家的能力的方法。通过共同培训分类器与分配系统，分类器学会学会准确预测那些对人类专家难以进行的实例，而分配系统学会将每个实例传递给最合适的团队成员，或人类专家。我们在公共数据集的多个实验中评估了我们提出的方法，并使用“合成”专家和由多个放射科医生注释的现实世界医学数据集。我们的方法表现优于先前的工作，比最好的人类专家或分类器更准确。此外，它灵活适应各种大小和不同水平的专家多样性的团队。,https://arxiv.org/abs/2206.07948,IJCAI,True,False,False,False
1760,Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation.,"Zekang Chen, Jia Wei, Rui Li","In clinical practice, well-aligned multi-modal images, such as Magnetic Resonance (MR) and Computed Tomography (CT), together can provide complementary information for image-guided therapies. Multi-modal image registration is essential for the accurate alignment of these multi-modal images. However, it remains a very challenging task due to complicated and unknown spatial correspondence between different modalities. In this paper, we propose a novel translation-based unsupervised deformable image registration approach to convert the multi-modal registration problem to a mono-modal one. Specifically, our approach incorporates a discriminator-free translation network to facilitate the training of the registration network and a patchwise contrastive loss to encourage the translation network to preserve object shapes. Furthermore, we propose to replace an adversarial loss, that is widely used in previous multi-modal image registration methods, with a pixel loss in order to integrate the output of translation into the target modality. This leads to an unsupervised method requiring no ground-truth deformation or pairs of aligned images for training. We evaluate four variants of our approach on the public Learn2Reg 2021 datasets \cite{hering2021learn2reg}. The experimental results demonstrate that the proposed architecture achieves state-of-the-art performance. Our code is available at https://github.com/heyblackC/DFMIR.",通过无鉴别图像对图像翻译的无监督多模式医学图像注册。,在临床实践中，良好的多模式图像，例如磁共振（MR）和计算机断层扫描（CT），可以为图像引导的疗法提供互补信息。多模式图像注册对于这些多模式图像的准确对齐至关重要。但是，由于不同模式之间的复杂且未知的空间对应关系，这仍然是一项非常具有挑战性的任务。在本文中，我们提出了一种基于翻译的新型无监督的变形图像登记方法，以将多模式注册问题转换为单模式。具体而言，我们的方法结合了一个无歧视者的翻译网络，以促进注册网络的培训和贴片对比损失，以鼓励翻译网络保留对象形状。此外，我们建议替换对对抗性损失，该损失被广泛用于先前的多模式图像登记方法，并具有像素损失，以便将翻译的输出整合到目标模式中。这导致了一种无监督的方法，不需要地面真相变形或对齐的图像对训练。我们在public Learn 2reg 2021数据集\ cite {hering2021leln2reg}上评估了四个方法的变体。实验结果表明，所提出的体系结构实现了最先进的性能。我们的代码可在https://github.com/heyblackc/dfmir上找到。,https://arxiv.org/abs/2204.13656,IJCAI,True,False,False,False
1761,Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds.,"Heng Wang, Chaoyi Zhang, Jianhui Yu, Weidong Cai","Dense captioning in 3D point clouds is an emerging vision-and-language task involving object-level 3D scene understanding. Apart from coarse semantic class prediction and bounding box regression as in traditional 3D object detection, 3D dense captioning aims at producing a further and finer instance-level label of natural language description on visual appearance and spatial relations for each scene object of interest. To detect and describe objects in a scene, following the spirit of neural machine translation, we propose a transformer-based encoder-decoder architecture, namely SpaCap3D, to transform objects into descriptions, where we especially investigate the relative spatiality of objects in 3D scenes and design a spatiality-guided encoder via a token-to-token spatial relation learning objective and an object-centric decoder for precise and spatiality-enhanced object caption generation. Evaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed SpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in CIDEr@0.5IoU, respectively. Our project page with source code and supplementary files is available at https://SpaCap3D.github.io/ .",在点云上的3D密集字幕的空间引导变压器。,3D点云中的密集字幕是一项涉及对象级3D场景理解的新兴视觉和语言任务。除了传统的3D对象检测中，除了粗糙的语义类预测和边界框回归外，3D密集的字幕旨在为每个场景的对象生成有关视觉外观和空间关系的自然语言描述的更详细，更精细的实例级别标签。为了在场景中检测和描述对象，遵循神经机器的精神翻译的精神，我们提出了一个基于变压器的编码器编码器架构，即SPACAP3D，以将对象转换为描述，我们特别研究了3D场景和3D场景中对象的相对空间性通过令牌到框的空间关系学习目标设计空间性引导的编码器，并以对象为中心的解码器进行精确和空间增强对象字幕的生成。在两个基准数据集（ScanRefer和Referit3D）上进行了评估，我们提出的SPACAP3D的表现分别优于基线方法Scan2CAP 4.94％和9.61％，分别在cider@0.5iou中。我们的带有源代码和补充文件的项目页面可在https://spacap3d.github.io/上获得。,https://arxiv.org/abs/2204.10688,IJCAI,True,False,False,False
1762,TinyLight: Adaptive Traffic Signal Control on Devices with Extremely Limited Resources.,"Dong Xing, Qian Zheng, Qianhui Liu, Gang Pan","Recent advances in deep reinforcement learning (DRL) have largely promoted the performance of adaptive traffic signal control (ATSC). Nevertheless, regarding the implementation, most works are cumbersome in terms of storage and computation. This hinders their deployment on scenarios where resources are limited. In this work, we propose TinyLight, the first DRL-based ATSC model that is designed for devices with extremely limited resources. TinyLight first constructs a super-graph to associate a rich set of candidate features with a group of light-weighted network blocks. Then, to diminish the model's resource consumption, we ablate edges in the super-graph automatically with a novel entropy-minimized objective function. This enables TinyLight to work on a standalone microcontroller with merely 2KB RAM and 32KB ROM. We evaluate TinyLight on multiple road networks with real-world traffic demands. Experiments show that even with extremely limited resources, TinyLight still achieves competitive performance. The source code and appendix of this work can be found at \url{https://bit.ly/38hH8t8}.",Tinylight：具有极为有限的资源的设备上的自适应交通信号控制。,深度强化学习（DRL）的最新进展在很大程度上促进了自适应交通信号控制（ATSC）的性能。但是，关于实施，大多数作品在存储和计算方面都很麻烦。这阻碍了他们在资源有限的方案中的部署。在这项工作中，我们提出了Tinylight，这是第一个基于DRL的ATSC模型，该模型是为资源极为有限的设备而设计的。Tinylight首先构建了一个超级雕像，将丰富的候选功能与一组轻加权网络块相关联。然后，为了减少模型的资源消耗，我们会以新型的熵最小化的目标函数自动融入超级雕像中。这使Tinylight可以使用仅2KB RAM和32KB ROM的独立微控制器。我们评估了具有真实世界交通需求的多个道路网络上的Tinylight。实验表明，即使资源极为有限，Tinylight仍然可以达到竞争性能。可以在\ url {https://bit.ly/38HH8T8}上找到此工作的源代码和附录。,https://arxiv.org/abs/2205.00427,IJCAI,True,False,False,False
1763,Explaining Preferences by Multiple Patterns in Voters' Behavior.,"Sonja Kraiczy, Edith Elkind","In some preference aggregation scenarios, voters' preferences are highly structured: e.g., the set of candidates may have one-dimensional structure (so that voters' preferences are single-peaked) or be described by a binary decision tree (so that voters' preferences are group-separable). However, sometimes a single axis or a decision tree is insufficient to capture the voters' preferences; rather, there is a small number $k$ of axes or decision trees such that each vote in the profile is consistent with one of these axes (resp., trees). In this work, we study the complexity of deciding whether voters' preferences can be explained in this manner. For $k=2$, we obtain a polynomial-time algorithm for several domains: single-peaked preferences (thereby answering a question left open by Erdelyi, Lackner and Pfandler (JAIR'17), value-restricted preferences, group-separable preferences, and a natural subdomain of group-separable preferences, namely, caterpillar group-separable preferences. For $k\ge 3$, the problem is known to be hard for single-peaked preferences; we show that this is also the case for value-restricted and group-separable preferences. Our positive results for $k=2$ make use of forbidden minor characterizations of the respective domains; in particular, we establish that the domain of caterpillar group-separable preferences admits a forbidden minor characterization.",通过选民行为中的多种模式来解释偏好。,在某些偏好聚合方案中，选民的偏好是高度结构化的：是分组的）。但是，有时单轴或决策树不足以捕捉选民的喜好。相反，有少数$ k $的轴或决策树，因此配置文件中的每票都与这些轴之一（分别为，树木）一致。在这项工作中，我们研究了决定选民偏好是否可以以这种方式解释的复杂性。对于$ k = 2 $，我们获得了多个域的多项式时间算法：单峰偏好（从而回答Erdelyi，Lackner和Pfandler（Jair'17）留下的问题，以及一个自然的群体分离偏好的子域，即，毛毛虫群 - 可分离的偏好。对于$ k \ ge 3 $，已知该问题对于单峰的偏好很难；我们证明，值也是值的情况。 - 限制性和分离的偏好。我们对$ k = 2 $的积极结果利用了相应域的禁止的次要特征；尤其是，我们确定caterpillar群 - 分离的偏好的领域承认了禁止的次要表征。,https://arxiv.org/abs/2201.11195,IJCAI,True,False,False,False
1764,Optimal Anonymous Independent Reward Scheme Design.,"Mengjing Chen, Pingzhong Tang, Zihe Wang, Shenke Xiao, Xiwang Yang","We consider designing reward schemes that incentivize agents to create high-quality content (e.g., videos, images, text, ideas). The problem is at the center of a real-world application where the goal is to optimize the overall quality of generated content on user-generated content platforms. We focus on anonymous independent reward schemes (AIRS) that only take the quality of an agent's content as input. We prove the general problem is NP-hard. If the cost function is convex, we show the optimal AIRS can be formulated as a convex optimization problem and propose an efficient algorithm to solve it. Next, we explore the optimal linear reward scheme and prove it has a 1/2-approximation ratio, and the ratio is tight. Lastly, we show the proportional scheme can be arbitrarily bad compared to AIRS.",最佳匿名独立奖励方案设计。,我们考虑设计奖励方案，以激励代理创建高质量内容（例如，视频，图像，文本，想法）。问题是现实世界应用程序的中心，其目标是优化用户生成的内容平台上生成的内容的整体质量。我们专注于匿名独立奖励方案（AIRS），这些计划仅将代理商的内容作为输入。我们证明了一般问题是NP-HARD。如果成本函数是凸的，我们表明最佳空气可以作为凸优化问题进行配制，并提出有效的算法来解决它。接下来，我们探索最佳线性奖励方案，并证明其具有1/2的附属能力比，并且比率很紧。最后，我们表明比例方案与AIRS相比可能是任意不良的。,https://arxiv.org/abs/2205.00192,IJCAI,True,False,False,False
1765,Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations.,"Sangwon Seo, Vaibhav V. Unhelkar","We present Bayesian Team Imitation Learner (BTIL), an imitation learning algorithm to model behavior of teams performing sequential tasks in Markovian domains. In contrast to existing multi-agent imitation learning techniques, BTIL explicitly models and infers the time-varying mental states of team members, thereby enabling learning of decentralized team policies from demonstrations of suboptimal teamwork. Further, to allow for sample- and label-efficient policy learning from small datasets, BTIL employs a Bayesian perspective and is capable of learning from semi-supervised demonstrations. We demonstrate and benchmark the performance of BTIL on synthetic multi-agent tasks as well as a novel dataset of human-agent teamwork. Our experiments show that BTIL can successfully learn team policies from demonstrations despite the influence of team members' (time-varying and potentially misaligned) mental states on their behavior.",从次优示威中对团队政策的半监督模仿学习。,我们提出了贝叶斯团队模仿学习者（BTIL），这是一种模仿学习算法，以模拟马尔可夫域中执行顺序任务的团队的行为。与现有的多机构模仿学习技术相反，BTIL明确模型并渗透了团队成员的时间变化的心理状态，从而从次优的团队合作的演示中实现了分散的团队政策的学习。此外，为了允许从小型数据集中进行样本和标签有效的政策学习，Btil采用了贝叶斯的角度，并且能够从半监督的示范中学习。我们证明并基准了BTIL在合成多代理任务以及人类代理团队工作的新型数据集上的性能。我们的实验表明，尽管团队成员（随时间变化且可能未对准）精神状态对其行为的影响，BTIL可以成功地从示威中学习团队政策。,https://arxiv.org/abs/2205.02959,IJCAI,True,False,False,False
1766,Adaptive Information Belief Space Planning.,"Moran Barenboim, Vadim Indelman","Reasoning about uncertainty is vital in many real-life autonomous systems. However, current state-of-the-art planning algorithms cannot either reason about uncertainty explicitly, or do so with a high computational burden. Here, we focus on making informed decisions efficiently, using reward functions that explicitly deal with uncertainty. We formulate an approximation, namely an abstract observation model, that uses an aggregation scheme to alleviate computational costs. We derive bounds on the expected information-theoretic reward function and, as a consequence, on the value function. We then propose a method to refine aggregation to achieve identical action selection with a fraction of the computational time.",自适应信息信念空间规划。,关于不确定性的推理在许多现实生活中的自治系统中至关重要。但是，当前的最新计划算法不能明确地考虑不确定性的原因，也不能承担较高的计算负担。在这里，我们专注于有效地采用明确处理不确定性的奖励功能，以有效地做出明智的决策。我们制定了一个近似值，即一种抽象观察模型，该模型使用聚合方案来减轻计算成本。我们在预期的信息理论奖励函数上得出界限，并因此在价值函数上得出了界限。然后，我们提出了一种方法来完善聚合以在计算时间的一部分中实现相同的动作选择。,https://arxiv.org/abs/2201.05673,IJCAI,True,False,False,False
1767,Iterative Geometry-Aware Cross Guidance Network for Stereo Image Inpainting.,"Ang Li, Shanshan Zhao, Qingjie Zhang, Qiuhong Ke","Currently, single image inpainting has achieved promising results based on deep convolutional neural networks. However, inpainting on stereo images with missing regions has not been explored thoroughly, which is also a significant but different problem. One crucial requirement for stereo image inpainting is stereo consistency. To achieve it, we propose an Iterative Geometry-Aware Cross Guidance Network (IGGNet). The IGGNet contains two key ingredients, i.e., a Geometry-Aware Attention (GAA) module and an Iterative Cross Guidance (ICG) strategy. The GAA module relies on the epipolar geometry cues and learns the geometry-aware guidance from one view to another, which is beneficial to make the corresponding regions in two views consistent. However, learning guidance from co-existing missing regions is challenging. To address this issue, the ICG strategy is proposed, which can alternately narrow down the missing regions of the two views in an iterative manner. Experimental results demonstrate that our proposed network outperforms the latest stereo image inpainting model and state-of-the-art single image inpainting models.",迭代性几何形状感知的跨指导网络，用于插入立体声图像。,目前，基于深层卷积神经网络的单图像介入已取得了令人鼓舞的结果。但是，尚未对具有缺失区域的立体声图像进行介绍，这也是一个重大但不同的问题。立体声图像填充的一个关键要求是立体声一致性。为了实现这一目标，我们提出了一个迭代的几何学跨指导网络（IGGNET）。IGGNET包含两种关键成分，即几何学意识（GAA）模块和迭代交叉指导（ICG）策略。GAA模块依赖于外两极的几何提示，并从一种视图到另一种视图学习了几何学意识的指导，这有助于使两个视图中的相应区域保持一致。但是，从共存的缺失地区学习指导是具有挑战性的。为了解决这个问题，提出了ICG策略，可以以迭代方式交替缩小两个观点的缺失区域。实验结果表明，我们提出的网络优于最新的立体声图像介入模型和最先进的单图像介绍模型。,https://arxiv.org/abs/2205.03825,IJCAI,True,False,False,False
1768,Efficient and Accurate Conversion of Spiking Neural Network with Burst Spikes.,"Yang Li, Yi Zeng","Spiking neural network (SNN), as a brain-inspired energy-efficient neural network, has attracted the interest of researchers. While the training of spiking neural networks is still an open problem. One effective way is to map the weight of trained ANN to SNN to achieve high reasoning ability. However, the converted spiking neural network often suffers from performance degradation and a considerable time delay. To speed up the inference process and obtain higher accuracy, we theoretically analyze the errors in the conversion process from three perspectives: the differences between IF and ReLU, time dimension, and pooling operation. We propose a neuron model for releasing burst spikes, a cheap but highly efficient method to solve residual information. In addition, Lateral Inhibition Pooling (LIPooling) is proposed to solve the inaccuracy problem caused by MaxPooling in the conversion process. Experimental results on CIFAR and ImageNet demonstrate that our algorithm is efficient and accurate. For example, our method can ensure nearly lossless conversion of SNN and only use about 1/10 (less than 100) simulation time under 0.693$\times$ energy consumption of the typical method. Our code is available at https://github.com/Brain-Inspired-Cognitive-Engine/Conversion_Burst.",具有突发尖峰的尖峰神经网络的有效而准确的转换。,尖峰神经网络（SNN）是一种受脑启发的节能神经网络，吸引了研究人员的兴趣。虽然培训尖峰神经网络仍然是一个空旷的问题。一种有效的方法是将训练有素的ANN的重量映射到SNN，以获得高推理能力。但是，转换后的尖峰神经网络通常会遭受性能降解和相当大的时间延迟。为了加快推理过程并获得更高的准确性，我们从三个角度从理论上分析了转换过程中的错误：IF和RELU之间的差异，时间维度和合并操作。我们提出了一个用于释放爆发尖峰的神经元模型，这是一种廉价但高效的方法来解决残差信息。此外，提出了横向抑制池（脂肪）来解决转换过程中最大化引起的不准确问题。CIFAR和Imagenet的实验结果表明，我们的算法是有效而准确的。例如，我们的方法可以确保SNN的几乎无损转换，并且仅在0.693 $ \ times $ $ 1/10（少于100）的模拟时间下使用典型方法的能源消耗。我们的代码可在https://github.com/brain-inspired-cognitive-engine/conversion_burst上找到。,https://arxiv.org/abs/2204.13271,IJCAI,True,False,False,False
1769,The Power of Media Agencies in Ad Auctions: Improving Utility through Coordinated Bidding.,"Giulia Romano, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti","The increasing competition in digital advertising induced a proliferation of media agencies playing the role of intermediaries between advertisers and platforms selling ad slots. When a group of competing advertisers is managed by a common agency, many forms of collusion, such as bid rigging, can be implemented by coordinating bidding strategies, dramatically increasing advertisers' value. We study the problem of finding bids and monetary transfers maximizing the utility of a group of colluders, under GSP and VCG mechanisms. First, we introduce an abstract bid optimization problem -- called weighted utility problem (WUP) -- , which is useful in proving our results. We show that the utilities of bidding strategies are related to the length of paths in a directed acyclic weighted graph, whose structure and weights depend on the mechanism under study. This allows us to solve WUP in polynomial time by finding a shortest path of the graph. Next, we switch to our original problem, focusing on two settings that differ for the incentives they allow for. Incentive constraints ensure that colluders do not leave the agency, and they can be enforced by implementing monetary transfers between the agency and the advertisers. In particular, we study the arbitrary transfers setting, where any kind of monetary transfer to and from the advertisers is allowed, and the more realistic limited liability setting, in which no advertiser can be paid by the agency. In the former, we cast the problem as a WUP instance and solve it by our graph-based algorithm, while, in the latter, we formulate it as a linear program with exponentially-many variables efficiently solvable by applying the ellipsoid algorithm to its dual. This requires to solve a suitable separation problem in polynomial time, which can be done by reducing it to a WUP instance.",媒体机构在广告拍卖中的权力：通过协调的招标来改善公用事业。,数字广告中日益增长的竞争引起了媒体机构在广告商和销售广告插槽的平台之间扮演中介机构的角色的扩散。当一组竞争广告商由普通代理商管理时，可以通过协调投标策略来实施许多形式的勾结，例如投标索具，从而大大提高广告商的价值。我们研究了在GSP和VCG机制下发现竞标和货币转移最大化的货币转移的问题。首先，我们引入了一个抽象的投标优化问题 - 称为加权实用程序问题（WUP），这对于证明我们的结果很有用。我们表明，投标策略的实用程序与定向无环的图形中的路径长度有关，其结构和权重取决于所研究的机制。这使我们能够通过找到图形的最短路径来解决多项式时间。接下来，我们切换到我们的原始问题，重点关注两个设置，这些设置因其允许的激励措施而有所不同。激励限制确保颜色不会离开代理机构，并且可以通过在代理商和广告商之间实施货币转让来执行它们。特别是，我们研究了任意转移设置，允许往返广告商的任何类型的货币转移，以及更现实的有限责任设置，该机构无法支付广告商。在前者中，我们将问题作为WUP实例施加，并通过基于图的算法解决问题，而在后者中，我们将其作为线性程序制定，并通过将椭圆形算法应用于其双重方向，可以有效地求解呈指数式的变量。。这需要在多项式时间内解决合适的分离问题，这可以通过将其简化为WUP实例来完成。,https://arxiv.org/abs/2204.13772,IJCAI,True,False,False,False
1770,Online Approval Committee Elections.,"Virginie Do, Matthieu Hervouin, Jérôme Lang, Piotr Skowron","Assume $k$ candidates need to be selected. The candidates appear over time. Each time one appears, it must be immediately selected or rejected -- a decision that is made by a group of individuals through voting. Assume the voters use approval ballots, i.e., for each candidate they only specify whether they consider it acceptable or not. This setting can be seen as a voting variant of choosing $k$ secretaries. Our contribution is twofold. (1) We assess to what extent the committees that are computed online can proportionally represent the voters. (2) If a prior probability over candidate approvals is available, we show how to compute committees with maximal expected score.",在线批准委员会选举。,假设需要选择$ K $候选人。候选人会随着时间的流逝而出现。每次出现时，都必须立即选择或拒绝它 - 一组个人通过投票做出的决定。假设选民使用批准选票，即，对于每个候选人，他们只指定他们是否认为可以接受。该设置可以看作是选择$ K $秘书的投票变体。我们的贡献是双重的。（1）我们评估在线计算的委员会在多大程度上可以按比例地代表选民。（2）如果获得候选人批准的先验概率，我们将展示如何以最大预期得分计算委员会。,https://arxiv.org/abs/2202.06830,IJCAI,True,False,False,False
1771,Runtime Analysis of Single- and Multi-Objective Evolutionary Algorithms for Chance Constrained Optimization Problems with Normally Distributed Random Variables.,"Frank Neumann, Carsten Witt","Chance constrained optimization problems allow to model problems where constraints involving stochastic components should only be violated with a small probability. Evolutionary algorithms have recently been applied to this scenario and shown to achieve high quality results. With this paper, we contribute to the theoretical understanding of evolutionary algorithms for chance constrained optimization. We study the scenario of stochastic components that are independent and Normally distributed. By generalizing results for the class of linear functions to the sum of transformed linear functions, we show that the (1+1)~EA can optimize the chance constrained setting without additional constraints in time O(n log n). However, we show that imposing an additional uniform constraint already leads to local optima for very restricted scenarios and an exponential optimization time for the (1+1)~EA. We therefore propose a multi-objective formulation of the problem which trades off the expected cost and its variance. We show that multi-objective evolutionary algorithms are highly effective when using this formulation and obtain a set of solutions that contains an optimal solution for any possible confidence level imposed on the constraint. Furthermore, we show that this approach can also be used to compute a set of optimal solutions for the chance constrained minimum spanning tree problem.",与正态分布随机变量的机会约束优化问题的单目标进化算法的运行时分析。,机会受到限制的优化问题允许建模问题，其中涉及随机组件的约束仅应以较小的概率侵犯。进化算法最近已应用于此情况，并证明可以实现高质量的结果。在本文中，我们有助于对进化算法的理论理解，以进行偶然的优化。我们研究独立且正态分布的随机组件的场景。通过将线性函数类的结果概括为转换的线性函数的总和，我们表明（1+1）〜EA可以优化机会约束设置，而无需时间O（n log n）。但是，我们表明，对非常有限的场景和（1+1）〜EA的指数优化时间已经导致局部最佳限制。因此，我们提出了问题的多目标公式，以摆脱预期成本及其差异。我们表明，在使用此公式时，多目标进化算法是非常有效的，并获得一组解决方案，该解决方案包含最佳解决方案，以适用于施加在约束上的任何可能的置信度。此外，我们证明这种方法还可以用于计算一组最佳解决方案，以限制最小跨越树问题。,https://arxiv.org/abs/2109.05799,IJCAI,True,False,False,False
1772,Parameterized Algorithms for Kidney Exchange.,"Arnab Maiti, Palash Dey","In kidney exchange programs, multiple patient-donor pairs each of whom are otherwise incompatible, exchange their donors to receive compatible kidneys. The Kidney Exchange problem is typically modelled as a directed graph where every vertex is either an altruistic donor or a pair of patient and donor; directed edges are added from a donor to its compatible patients. The computational task is to find if there exists a collection of disjoint cycles and paths starting from altruistic donor vertices of length at most l_c and l_p respectively that covers at least some specific number t of non-altruistic vertices (patients). We study parameterized algorithms for the kidney exchange problem in this paper. Specifically, we design FPT algorithms parameterized by each of the following parameters: (1) the number of patients who receive kidney, (2) treewidth of the input graph + max{l_p, l_c}, and (3) the number of vertex types in the input graph when l_p <= l_c. We also present interesting algorithmic and hardness results on the kernelization complexity of the problem. Finally, we present an approximation algorithm for an important special case of Kidney Exchange.",肾脏交换的参数化算法。,在肾脏交换计划中，多个患者对否则不兼容，将其捐赠者交换以接收兼容的肾脏。肾脏交换问题通常被建模为有向图，每个顶点都是无私的供体或一对患者和捐赠者。从捐赠者添加到兼容患者的方向边缘。计算任务是找出是否存在从最多l_c和l_p的利他主义供体顶点开始的不相交周期和路径的集合，该顶点至少涵盖了至少一些特定数量的t非alltruistic thertices（患者）。我们在本文中研究了肾脏交换问题的参数化算法。具体而言，我们设计了由以下每个参数参数化的fpt算法：（1）接受肾脏的患者数量，（2）输入图 + max {l_p，l_c}的树宽，以及（3）VERTETEX类型的数量在输入图中，当l_p <= l_c时。我们还介绍了有关问题的内核复杂性的有趣算法和硬度结果。最后，我们提出了一种重要的肾脏交换特殊情况的近似算法。,https://arxiv.org/abs/2112.10250,IJCAI,True,False,False,False
1773,Learning First-Order Rules with Differentiable Logic Program Semantics.,"Kun Gao, Katsumi Inoue, Yongzhi Cao, Hanpin Wang","Learning first-order logic programs (LPs) from relational facts which yields intuitive insights into the data is a challenging topic in neuro-symbolic research. We introduce a novel differentiable inductive logic programming (ILP) model, called differentiable first-order rule learner (DFOL), which finds the correct LPs from relational facts by searching for the interpretable matrix representations of LPs. These interpretable matrices are deemed as trainable tensors in neural networks (NNs). The NNs are devised according to the differentiable semantics of LPs. Specifically, we first adopt a novel propositionalization method that transfers facts to NN-readable vector pairs representing interpretation pairs. We replace the immediate consequence operator with NN constraint functions consisting of algebraic operations and a sigmoid-like activation function. We map the symbolic forward-chained format of LPs into NN constraint functions consisting of operations between subsymbolic vector representations of atoms. By applying gradient descent, the trained well parameters of NNs can be decoded into precise symbolic LPs in forward-chained logic format. We demonstrate that DFOL can perform on several standard ILP datasets, knowledge bases, and probabilistic relation facts and outperform several well-known differentiable ILP models. Experimental results indicate that DFOL is a precise, robust, scalable, and computationally cheap differentiable ILP model.",使用可区分的逻辑程序语义学习一阶规则。,从关系事实中学习一阶逻辑程序（LPS），这些事实能够对数据产生直观的见解，这是神经符号研究的一个挑战性主题。我们介绍了一种新型的可区分归纳逻辑编程（ILP）模型，称为一阶规则学习者（DFOL），该模型通过搜索LPS的可解释的矩阵表示来从关系事实中找到正确的LP。这些可解释的矩阵被认为是神经网络（NNS）中的可训练张量。NN是根据LPS的可区分语义设计的。具体而言，我们首先采用了一种新颖的命题方法，该方法将事实转移到代表解释对的NN可读矢量对。我们用NN约束函数代替了直接的后果操作员，该功能由代数操作和类似Sigmoid的激活函数组成。我们将LPS的符号前链格式映射到NN约束函数中，该函数由原子的亚符号矢量表示之间的操作组成。通过应用梯度下降，可以将训练的NN的井参数解码为前链逻辑格式的精确符号LPS。我们证明DFOL可以在几个标准的ILP数据集，知识库和概率关系事实上执行，并且表现优于几个众所周知的可区分ILP模型。实验结果表明，DFOL是一种精确，可靠，可扩展和计算上便宜的ILP模型。,https://arxiv.org/abs/2204.13570,IJCAI,True,False,False,False
1774,SHAPE: An Unified Approach to Evaluate the Contribution and Cooperation of Individual Modalities.,"Pengbo Hu, Xingyu Li, Yi Zhou","As deep learning advances, there is an ever-growing demand for models capable of synthesizing information from multi-modal resources to address the complex tasks raised from real-life applications. Recently, many large multi-modal datasets have been collected, on which researchers actively explore different methods of fusing multi-modal information. However, little attention has been paid to quantifying the contribution of different modalities within the proposed models. In this paper, we propose the {\bf SH}apley v{\bf A}lue-based {\bf PE}rceptual (SHAPE) scores that measure the marginal contribution of individual modalities and the degree of cooperation across modalities. Using these scores, we systematically evaluate different fusion methods on different multi-modal datasets for different tasks. Our experiments suggest that for some tasks where different modalities are complementary, the multi-modal models still tend to use the dominant modality alone and ignore the cooperation across modalities. On the other hand, models learn to exploit cross-modal cooperation when different modalities are indispensable for the task. In this case, the scores indicate it is better to fuse different modalities at relatively early stages. We hope our scores can help improve the understanding of how the present multi-modal models operate on different modalities and encourage more sophisticated methods of integrating multiple modalities.",形状：一种评估单个方式的贡献和合作的统一方法。,随着深度学习的进步，对能够从多模式资源中综合信息的模型的需求不断增长，以解决从现实生活应用程序中提出的复杂任务。最近，已经收集了许多大型多模式数据集，研究人员在其上积极探索融合多模式信息的不同方法。但是，很少有人注意量化所提出模型中不同方式的贡献。在本文中，我们提出了基于{\ bf sh} apley v {\ bf a} lue {\ bf pe} rceptual（shape）得分，以测量单个模态的边际贡献和跨模态的合作程度。使用这些分数，我们系统地评估了不同任务的不同多模式数据集上的不同融合方法。我们的实验表明，对于某些不同模式是互补的任务，多模式模型仍然倾向于单独使用主要模式，而忽略了跨模态的合作。另一方面，在任务必不可少的情况下，模型学会利用跨模式合作。在这种情况下，分数表明最好在相对较早的阶段融合不同的方式。我们希望我们的分数能够帮助提高人们对当前多模式模型如何以不同方式运作的理解，并鼓励更复杂的多种模式的方法。,https://arxiv.org/abs/2205.00302,IJCAI,True,False,False,False
1775,On Verifying Expectations and Observations of Intelligent Agents.,"Sourav Chakraborty, Avijeet Ghosh, Sujata Ghosh, François Schwarzentruber","Public observation logic (POL) is a variant of dynamic epistemic logic to reason about agent expectations and agent observations. Agents have certain expectations, regarding the situation at hand, that are actuated by the relevant protocols, and they eliminate possible worlds in which their expectations do not match with their observations. In this work, we investigate the computational complexity of the model checking problem for POL and prove its PSPACE-completeness. We also study various syntactic fragments of POL. We exemplify the applicability of POL model checking in verifying different characteristics and features of an interactive system with respect to the distinct expectations and (matching) observations of the system. Finally, we provide a discussion on the implementation of the model checking algorithms.",关于验证智能代理的期望和观察。,公众观察逻辑（POL）是动态认知逻辑的一种变体，以理论代理人期望和代理观察。代理人对当前情况有一定的期望，这些期望是由相关协议驱动的，他们消除了他们的期望与他们的观察结果不符的可能世界。在这项工作中，我们研究了POL的模型检查问题的计算复杂性，并证明其PSPACE完整性。我们还研究了POL的各种句法片段。我们体现了POL模型检查在验证系统的不同期望和（匹配）系统的不同特征和特征中的适用性。最后，我们提供了有关模型检查算法的实现的讨论。,https://arxiv.org/abs/2205.00784,IJCAI,True,False,False,False
1776,On the Optimization of Margin Distribution.,"Meng-Zhang Qian, Zheng Ai, Teng Zhang, Wei Gao","Margin has played an important role on the design and analysis of learning algorithms during the past years, mostly working with the maximization of the minimum margin. Recent years have witnessed the increasing empirical studies on the optimization of margin distribution according to different statistics such as medium margin, average margin, margin variance, etc., whereas there is a relative paucity of theoretical understanding. In this work, we take one step on this direction by providing a new generalization error bound, which is heavily relevant to margin distribution by incorporating ingredients such as average margin and semi-variance, a new margin statistics for the characterization of margin distribution. Inspired by the theoretical findings, we propose the MSVMAv, an efficient approach to achieve better performance by optimizing margin distribution in terms of its empirical average margin and semi-variance. We finally conduct extensive experiments to show the superiority of the proposed MSVMAv approach.",关于边缘分布的优化。,在过去的几年中，Margin在学习算法的设计和分析中发挥了重要作用，主要与最小利润的最大化一起工作。近年来，根据不同统计数据的优化分布的实证研究越来越多，例如中边缘，平均保证金，边缘差异等，而理论理解的相对匮乏。在这项工作中，我们通过提供新的概括误差绑定来迈出这个方向的一步，该误差与边缘分布相关，通过合并诸如平均保证金和半变化的成分，这是一种新的保证金统计数据，用于表征边距分布。受理论发现的启发，我们提出了MSVMAV，这是一种有效的方法，可以通过优化其经验平均利润率和半变异的优化利润分布来实现更好的性能。我们最终进行了广泛的实验，以显示提出的MSVMAV方法的优越性。,https://arxiv.org/abs/2204.14118,IJCAI,True,False,False,False
1777,Sample Complexity Bounds for Robustly Learning Decision Lists against Evasion Attacks.,"Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, James Worrell","A fundamental problem in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks. In this paper we address this issue within the framework of PAC learning, focusing on the class of decision lists. Given that distributional assumptions are essential in the adversarial setting, we work with probability distributions on the input data that satisfy a Lipschitz condition: nearby points have similar probability. Our key results illustrate that the adversary's budget (that is, the number of bits it can perturb on each input) is a fundamental quantity in determining the sample complexity of robust learning. Our first main result is a sample-complexity lower bound: the class of monotone conjunctions (essentially the simplest non-trivial hypothesis class on the Boolean hypercube) and any superclass has sample complexity at least exponential in the adversary's budget. Our second main result is a corresponding upper bound: for every fixed $k$ the class of $k$-decision lists has polynomial sample complexity against a $\log(n)$-bounded adversary. This sheds further light on the question of whether an efficient PAC learning algorithm can always be used as an efficient $\log(n)$-robust learning algorithm under the uniform distribution.",针对逃避攻击的强大学习决策清单的样本复杂性界限。,对抗机器学习中的一个基本问题是量化在逃避攻击的情况下需要多少培训数据。在本文中，我们在PAC学习框架内解决了这个问题，重点是决策列表。鉴于分布假设在对抗环境中至关重要，因此我们在满足Lipschitz条件的输入数据上使用概率分布：附近的点具有相似的概率。我们的主要结果表明，对手的预算（即，每个输入中可能会扰动的位数）是确定强大学习样本复杂性的基本数量。我们的第一个主要结果是样品复杂性下限：单调连词类别（基本上是布尔hyperean hypercube上最简单的非平凡假设类别），任何超级类别都至少在敌方预算中具有样品复杂性。我们的第二个主要结果是相应的上限：对于每个固定的$ k $，$ k $  - 决策列表的类别具有多项式样本复杂性，而$ \ log（n）$界面对手。这进一步阐明了一个问题，即是否始终将有效的PAC学习算法用作有效的$ \ log（n）$  - 在均匀分布下的强大学习算法。,https://arxiv.org/abs/2205.06127,IJCAI,True,False,False,False
1778,Achieving Envy-Freeness with Limited Subsidies under Dichotomous Valuations.,"Siddharth Barman, Anand Krishna, Y. Narahari, Soumyarup Sadhukhan","We study the problem of allocating indivisible goods among agents in a fair manner. While envy-free allocations of indivisible goods are not guaranteed to exist, envy-freeness can be achieved by additionally providing some subsidy to the agents. These subsidies can be alternatively viewed as a divisible good (money) that is fractionally assigned among the agents to realize an envy-free outcome. In this setup, we bound the subsidy required to attain envy-freeness among agents with dichotomous valuations, i.e., among agents whose marginal value for any good is either zero or one.   We prove that, under dichotomous valuations, there exists an allocation that achieves envy-freeness with a per-agent subsidy of either $0$ or $1$. Furthermore, such an envy-free solution can be computed efficiently in the standard value-oracle model. Notably, our results hold for general dichotomous valuations and, in particular, do not require the (dichotomous) valuations to be additive, submodular, or even subadditive. Also, our subsidy bounds are tight and provide a linear (in the number of agents) factor improvement over the bounds known for general monotone valuations.",在二分法估值下获得有限补贴，实现嫉妒。,我们研究以公平方式在代理商中分配不可分割的商品的问题。尽管不能保证存在不可分割的商品的无嫉妒分配，但可以通过向代理商提供一些补贴来实现嫉妒的福祉。可以将这些补贴视为可分割的商品（金钱），在代理商中分配了分配以实现嫉妒的结果。在此设置中，我们约束了具有二分法估值的代理商之间所需的补贴，即，在任何商品上的边际价值的代理商中，均为零或一个。我们证明，根据二分值估值，存在一个分配，以每人$ 0 $或$ 1 $的价格获得嫉妒性。此外，可以在标准值轨道模型中有效地计算这种嫉妒的解决方案。值得注意的是，我们的结果适用于一般的二分法估值，尤其是不需要（二分法）估值是添加剂，下suppular，甚至是亚基。同样，我们的补贴界限很紧，并提供了与一般单调估值所知的界限的线性（在代理数量中）。,https://arxiv.org/abs/2201.07419,IJCAI,True,False,False,False
1779,To Fold or Not to Fold: a Necessary and Sufficient Condition on Batch-Normalization Layers Folding.,"Edouard Yvinec, Arnaud Dapogny, Kevin Bailly","Batch-Normalization (BN) layers have become fundamental components in the evermore complex deep neural network architectures. Such models require acceleration processes for deployment on edge devices. However, BN layers add computation bottlenecks due to the sequential operation processing: thus, a key, yet often overlooked component of the acceleration process is BN layers folding. In this paper, we demonstrate that the current BN folding approaches are suboptimal in terms of how many layers can be removed. We therefore provide a necessary and sufficient condition for BN folding and a corresponding optimal algorithm. The proposed approach systematically outperforms existing baselines and allows to dramatically reduce the inference time of deep neural networks.",折叠或不折叠：批准层折叠的必要条件。,分批归一化（BN）层已成为越来越复杂的深度神经网络体系结构中的基本组成部分。这样的模型需要在边缘设备上部署的加速过程。但是，由于顺序操作处理，BN层添加了计算瓶颈：因此，加速过程的键但通常被忽略的组件是BN层折叠。在本文中，我们证明了当前的BN折叠方法在可以去除多少层方面是次优的。因此，我们为BN折叠和相应的最佳算法提供了必要且充分的条件。所提出的方法系统地超过了现有的基线，并允许大大减少深神经网络的推理时间。,https://arxiv.org/abs/2203.14646,IJCAI,True,False,False,False
1780,Video Frame Interpolation Based on Deformable Kernel Region.,"Haoyue Tian, Pan Gao, Xiaojiang Peng","Video frame interpolation task has recently become more and more prevalent in the computer vision field. At present, a number of researches based on deep learning have achieved great success. Most of them are either based on optical flow information, or interpolation kernel, or a combination of these two methods. However, these methods have ignored that there are grid restrictions on the position of kernel region during synthesizing each target pixel. These limitations result in that they cannot well adapt to the irregularity of object shape and uncertainty of motion, which may lead to irrelevant reference pixels used for interpolation. In order to solve this problem, we revisit the deformable convolution for video interpolation, which can break the fixed grid restrictions on the kernel region, making the distribution of reference points more suitable for the shape of the object, and thus warp a more accurate interpolation frame. Experiments are conducted on four datasets to demonstrate the superior performance of the proposed model in comparison to the state-of-the-art alternatives.",视频框架插值基于可变形的内核区域。,视频框架插值任务最近在计算机视野字段中变得越来越普遍。目前，基于深度学习的许多研究取得了巨大的成功。它们中的大多数是基于光流信息或插值内核的，或者是这两种方法的组合。但是，这些方法忽略了在合成每个目标像素期间内核区域的位置存在网格限制。这些局限性导致它们不能很好地适应物体形状的不规则性和运动的不确定性，这可能导致用于插值的参考像素无关。为了解决此问题，我们重新审视视频插值的可变形卷积，这可以打破核对内核区域的固定网格限制，使参考点的分布更适合于对象的形状，从而更准确地插值。框架。实验是在四个数据集上进行的，以证明与最先进的替代方案相比，提出的模型的出色性能。,https://arxiv.org/abs/2204.11396,IJCAI,True,False,False,False
1781,Dynamic Domain Generalization.,"Zhishu Sun, Zhifeng Shen, Luojun Lin, Yuanlong Yu, Zhifeng Yang, Shicai Yang, Weijie Chen","Domain generalization (DG) is a fundamental yet very challenging research topic in machine learning. The existing arts mainly focus on learning domain-invariant features with limited source domains in a static model. Unfortunately, there is a lack of training-free mechanism to adjust the model when generalized to the agnostic target domains. To tackle this problem, we develop a brand-new DG variant, namely Dynamic Domain Generalization (DDG), in which the model learns to twist the network parameters to adapt the data from different domains. Specifically, we leverage a meta-adjuster to twist the network parameters based on the static model with respect to different data from different domains. In this way, the static model is optimized to learn domain-shared features, while the meta-adjuster is designed to learn domain-specific features. To enable this process, DomainMix is exploited to simulate data from diverse domains during teaching the meta-adjuster to adapt to the upcoming agnostic target domains. This learning mechanism urges the model to generalize to different agnostic target domains via adjusting the model without training. Extensive experiments demonstrate the effectiveness of our proposed method. Code is available at: https://github.com/MetaVisionLab/DDG",动态域的概括。,域泛化（DG）是机器学习中的基本研究主题。现有的艺术主要集中在静态模型中具有有限源域的学习域不变特征上。不幸的是，缺乏无训练的机制来调整该模型，从而推广到不可知论目标域。为了解决这个问题，我们开发了一个全新的DG变体，即动态域概括（DDG），其中模型学会了扭曲网络参数以调整来自不同域的数据。具体而言，我们利用元评估来基于静态模型的网络参数，相对于来自不同域的不同数据。通过这种方式，静态模型被优化以学习域共享的功能，而元评估旨在学习特定领域的特征。为了启用此过程，在教授元及压缩过程中，将利用域mix来模拟来自不同域的数据以适应即将到来的不可知论目标域。这种学习机制敦促该模型通过在不训练的情况下调整模型来推广到不同的不可知论目标域。广泛的实验证明了我们提出的方法的有效性。代码可用：https：//github.com/metavisionlab/ddg,https://arxiv.org/abs/2205.13913,IJCAI,True,False,False,False
1782,Proportional Budget Allocations: A Systematization,"Maaike Los, Zoé Christoff, Davide Grossi",We contribute to the programme of lifting proportionality axioms from the multi-winner voting setting to participatory budgeting. We define novel proportionality axioms for participatory budgeting and test them on known proportionality-driven rules such as Phragm\'en and Rule X. We investigate logical implications among old and new axioms and provide a systematic overview of proportionality criteria in participatory budgeting.,比例预算分配：系统化,我们为将比例公理从多赢家投票设置到参与性预算做出了贡献。我们为参与性预算定义了新颖的比例公理，并根据已知的比例驱动的规则（例如phragm \'en and Rule x）对其进行测试。我们研究了旧公理和新公理之间的逻辑含义，并提供了有关参与预算中比例性标准的系统概述。,https://arxiv.org/abs/2203.12324,IJCAI,True,False,False,False
1783,Approximately EFX Allocations for Indivisible Chores.,"Shengwei Zhou, Xiaowei Wu","In this paper we study how to fairly allocate a set of m indivisible chores to a group of n agents, each of which has a general additive cost function on the items. Since envy-free (EF) allocation is not guaranteed to exist, we consider the notion of envy-freeness up to any item (EFX). In contrast to the fruitful results regarding the (approximation of) EFX allocations for goods, very little is known for the allocation of chores. Prior to our work, for the allocation of chores, it is known that EFX allocations always exist for two agents, or general number of agents with IDO cost functions. For general instances, no non-trivial approximation result regarding EFX allocation is known. In this paper we make some progress in this direction by showing that for three agents we can always compute a 5-approximation of EFX allocation in polynomial time. For n>=4 agents, our algorithm always computes an allocation that achieves an approximation ratio of O(n^2) regarding EFX.",大约针对不可分割的家务的EFX分配。,在本文中，我们研究了如何将一组不可分割的杂务分配给一组N代理，每个代理都具有一般的附加成本功能。由于不能保证存在无嫉妒的（EF）分配，因此我们考虑到任何项目（EFX）的嫉妒概念。与关于商品的EFX分配（近似）分配的富有成果的结果相反，琐事分配很少。在我们的工作之前，对于杂务的分配，众所周知，EFX分配始终存在于两种代理商或具有IDO成本功能的代理数量。对于一般实例，对于EFX分配，没有不平凡的近似结果。在本文中，我们通过证明三种代理可以在多项式时间内计算EFX分配的5个标准，从而在这个方向上取得了一些进展。对于N> = 4个代理，我们的算法始终计算出在EFX方面达到O（n^2）的近似值的分配。,https://arxiv.org/abs/2109.07313,IJCAI,True,False,False,False
1784,RecipeRec: A Heterogeneous Graph Learning Model for Recipe Recommendation.,"Yijun Tian, Chuxu Zhang, Zhichun Guo, Chao Huang, Ronald Metoyer, Nitesh V. Chawla","Recipe recommendation systems play an essential role in helping people decide what to eat. Existing recipe recommendation systems typically focused on content-based or collaborative filtering approaches, ignoring the higher-order collaborative signal such as relational structure information among users, recipes and food items. In this paper, we formalize the problem of recipe recommendation with graphs to incorporate the collaborative signal into recipe recommendation through graph modeling. In particular, we first present URI-Graph, a new and large-scale user-recipe-ingredient graph. We then propose RecipeRec, a novel heterogeneous graph learning model for recipe recommendation. The proposed model can capture recipe content and collaborative signal through a heterogeneous graph neural network with hierarchical attention and an ingredient set transformer. We also introduce a graph contrastive augmentation strategy to extract informative graph knowledge in a self-supervised manner. Finally, we design a joint objective function of recommendation and contrastive learning to optimize the model. Extensive experiments demonstrate that RecipeRec outperforms state-of-the-art methods for recipe recommendation. Dataset and codes are available at https://github.com/meettyj/RecipeRec.",食谱：用于食谱建议的异质图学习模型。,食谱推荐系统在帮助人们决定饮食方面起着至关重要的作用。现有的食谱建议系统通常集中在基于内容或协作的过滤方法上，而忽略了高阶协作信号，例如用户，食谱和食品之间的关系结构信息。在本文中，我们将配方建议的问题与图形形式化，以通过图形建模将协作信号纳入食谱建议中。特别是，我们首先介绍了一个新的大规模用户 - 配置图形图。然后，我们提出了一种新型的异质图学习模型，用于食谱建议。所提出的模型可以通过层次关注和成分集变压器的异质图神经网络捕获食谱内容和协作信号。我们还引入了图形对比度增强策略，以一种自我监督的方式提取信息图的知识。最后，我们设计了推荐和对比度学习的联合目标功能，以优化模型。广泛的实验表明，食谱优于食谱建议的最先进方法。数据集和代码可在https://github.com/meettyj/reciperec上找到。,https://arxiv.org/abs/2205.14005,IJCAI,True,False,False,False
1785,Reconstruction Enhanced Multi-View Contrastive Learning for Anomaly Detection on Attributed Networks.,"Jiaqiang Zhang, Senzhang Wang, Songcan Chen","Detecting abnormal nodes from attributed networks is of great importance in many real applications, such as financial fraud detection and cyber security. This task is challenging due to both the complex interactions between the anomalous nodes with other counterparts and their inconsistency in terms of attributes. This paper proposes a self-supervised learning framework that jointly optimizes a multi-view contrastive learning-based module and an attribute reconstruction-based module to more accurately detect anomalies on attributed networks. Specifically, two contrastive learning views are firstly established, which allow the model to better encode rich local and global information related to the abnormality. Motivated by the attribute consistency principle between neighboring nodes, a masked autoencoder-based reconstruction module is also introduced to identify the nodes which have large reconstruction errors, then are regarded as anomalies. Finally, the two complementary modules are integrated for more accurately detecting the anomalous nodes. Extensive experiments conducted on five benchmark datasets show our model outperforms current state-of-the-art models.",重建增强了对归因网络的异常检测的多视图对比度学习。,在许多真实应用中，例如财务欺诈检测和网络安全，从归因网络中检测异常节点非常重要。由于异常节点与其他对应物之间的复杂相互作用及其在属性方面的不一致之处，因此这项任务是具有挑战性的。本文提出了一个自我监督的学习框架，该框架共同优化了基于学习的多视图对比度模块和基于属性重建的模块，以更准确地检测属性网络的异常情况。具体而言，首先建立了两个对比度学习观点，这使模型可以更好地编码与异常相关的丰富本地和全球信息。还引入了由相邻节点之间的属性一致性原理的动机，还引入了基于掩盖自动编码器的重建模块，以识别具有较大重建错误的节点，然后被视为异常。最后，集成了两个互补模块，以更准确地检测异常节点。在五个基准数据集上进行的广泛实验表明，我们的模型的表现优于当前最新模型。,https://arxiv.org/abs/2205.04816,IJCAI,True,False,False,False
1786,Towards Adversarially Robust Deep Image Denoising.,"Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, Vincent Y. F. Tan","This work systematically investigates the adversarial robustness of deep image denoisers (DIDs), i.e, how well DIDs can recover the ground truth from noisy observations degraded by adversarial perturbations. Firstly, to evaluate DIDs' robustness, we propose a novel adversarial attack, namely Observation-based Zero-mean Attack ({\sc ObsAtk}), to craft adversarial zero-mean perturbations on given noisy images. We find that existing DIDs are vulnerable to the adversarial noise generated by {\sc ObsAtk}. Secondly, to robustify DIDs, we propose an adversarial training strategy, hybrid adversarial training ({\sc HAT}), that jointly trains DIDs with adversarial and non-adversarial noisy data to ensure that the reconstruction quality is high and the denoisers around non-adversarial data are locally smooth. The resultant DIDs can effectively remove various types of synthetic and adversarial noise. We also uncover that the robustness of DIDs benefits their generalization capability on unseen real-world noise. Indeed, {\sc HAT}-trained DIDs can recover high-quality clean images from real-world noise even without training on real noisy data. Extensive experiments on benchmark datasets, including Set68, PolyU, and SIDD, corroborate the effectiveness of {\sc ObsAtk} and {\sc HAT}.",迈向对抗性强大的深层图像。,这项工作系统地研究了深度图像Denoisers（DIDS）的对抗性鲁棒性，即DIDS如何从对抗性扰动降解的嘈杂观察中恢复地面真理。首先，为了评估DIDS的鲁棒性，我们提出了一种新颖的对抗性攻击，即基于观察的零均值攻击（{\ sc obsatk}），以在给定的嘈杂图像上制作对抗零均值的扰动。我们发现现有DID容易受到{\ sc obsatk}产生的对抗噪声的影响。其次，为了鲁棒性，我们提出了一种对抗性训练策略，混合对抗训练（{\ sc hat}），该训练与对抗性和非对抗性噪声数据共同培训DID，以确保重建质量高，并且围绕非词汇质量较高，并且非词汇量很高。对抗数据在局部平滑。最终的DID可以有效地消除各种类型的合成和对抗噪声。我们还发现，DIDS的鲁棒性使他们在看不见的现实世界噪声上的概括能力受益。的确，即使没有对真实的噪声数据进行训练，{\ sc hat}训练的涂料也可以从现实世界噪声中恢复高质量的干净图像。在包括Set68，Polyu和Sidd在内的基准数据集上进行了广泛的实验，证实了{\ sc obsatk}和{\ sc hat}的有效性。,https://arxiv.org/abs/2201.04397,IJCAI,True,False,False,False
1787,A Unified Meta-Learning Framework for Dynamic Transfer Learning.,"Jun Wu, Jingrui He","Transfer learning refers to the transfer of knowledge or information from a relevant source task to a target task. However, most existing works assume both tasks are sampled from a stationary task distribution, thereby leading to the sub-optimal performance for dynamic tasks drawn from a non-stationary task distribution in real scenarios. To bridge this gap, in this paper, we study a more realistic and challenging transfer learning setting with dynamic tasks, i.e., source and target tasks are continuously evolving over time. We theoretically show that the expected error on the dynamic target task can be tightly bounded in terms of source knowledge and consecutive distribution discrepancy across tasks. This result motivates us to propose a generic meta-learning framework L2E for modeling the knowledge transferability on dynamic tasks. It is centered around a task-guided meta-learning problem with a group of meta-pairs of tasks, based on which we are able to learn the prior model initialization for fast adaptation on the newest target task. L2E enjoys the following properties: (1) effective knowledge transferability across dynamic tasks; (2) fast adaptation to the new target task; (3) mitigation of catastrophic forgetting on historical target tasks; and (4) flexibility in incorporating any existing static transfer learning algorithms. Extensive experiments on various image data sets demonstrate the effectiveness of the proposed L2E framework.",动态传输学习的统一元学习框架。,转移学习是指知识或信息从相关源任务转移到目标任务。但是，大多数现有作品都假设两个任务都是从固定任务分布中取样的，从而导致在实际场景中从非平稳任务分布中绘制的动态任务的次优性能。为了弥合这一差距，在本文中，我们研究了一种动态任务的更现实和挑战性的转移学习设置，即源和目标任务随着时间的推移不断发展。从理论上讲，我们表明，动态目标任务上的预期错误可以在跨任务之间的源知识和连续分配差异方面紧密界定。这个结果激发了我们提出一个通用的元学习框架L2E，以建模动态任务上的知识传递性。它围绕一个任务引导的元学习问题，其中包括一组元对任务，基于我们能够学习先前的模型初始化，以快速适应最新的目标任务。L2E享有以下属性：（1）跨动态任务的有效知识传递性；（2）快速适应新目标任务；（3）缓解历史目标任务的灾难性遗忘；（4）合并任何现有的静态转移学习算法的灵活性。各种图像数据集的广泛实验证明了所提出的L2E框架的有效性。,https://arxiv.org/abs/2207.01784,IJCAI,True,False,False,False
1788,Entity Alignment with Reliable Path Reasoning and Relation-aware Heterogeneous Graph Transformer.,"Weishan Cai, Wenjun Ma, Jieyu Zhan, Yuncheng Jiang","Entity Alignment (EA) has attracted widespread attention in both academia and industry, which aims to seek entities with same meanings from different Knowledge Graphs (KGs). There are substantial multi-step relation paths between entities in KGs, indicating the semantic relations of entities. However, existing methods rarely consider path information because not all natural paths facilitate for EA judgment. In this paper, we propose a more effective entity alignment framework, RPR-RHGT, which integrates relation and path structure information, as well as the heterogeneous information in KGs. Impressively, an initial reliable path reasoning algorithm is developed to generate the paths favorable for EA task from the relation structures of KGs, which is the first algorithm in the literature to successfully use unrestricted path information. In addition, to efficiently capture heterogeneous features in entity neighborhoods, a relation-aware heterogeneous graph transformer is designed to model the relation and path structures of KGs. Extensive experiments on three well-known datasets show RPR-RHGT significantly outperforms 11 state-of-the-art methods, exceeding the best performing baseline up to 8.62% on Hits@1. We also show its better performance than the baselines on different ratios of training set, and harder datasets.",实体对准具有可靠的路径推理和关系感知的异质图变压器。,实体对齐（EA）在学术界和工业中都引起了广泛的关注，该行业旨在寻求具有不同知识图（KGS）相同含义的实体。KGS中的实体之间存在实质性的多步关系路径，表明实体的语义关系。但是，现有方法很少考虑路径信息，因为并非所有自然路径都促进EA判断。在本文中，我们提出了一个更有效的实体对齐框架RPR-RHGT，该框架集成了关系和路径结构信息以及KGS中的异质信息。令人印象深刻的是，开发了一种初始可靠的路径推理算法来生成有利于EA任务的路径，从KGS的关系结构中，这是文献中第一个成功使用无限制路径信息的算法。此外，为了有效地捕获实体社区中的异质特征，设计的异质图变压器旨在建模KGS的关系和路径结构。在三个著名数据集上进行的广泛实验表明，RPR-RHGT的表现明显优于11种最佳方法，超过了命中率@1的最佳性能基线最高8.62％。我们还表现出比基线在训练集的不同比率和更难数据集的基线上更好的性能。,https://arxiv.org/abs/2205.08806,IJCAI,True,False,False,False
1789,CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning.,"Chenyu Sun, Hangwei Qian, Chunyan Miao","In reinforcement learning (RL), it is challenging to learn directly from high-dimensional observations, where data augmentation has recently been shown to remedy this via encoding invariances from raw pixels. Nevertheless, we empirically find that not all samples are equally important and hence simply injecting more augmented inputs may instead cause instability in Q-learning. In this paper, we approach this problem systematically by developing a model-agnostic Contrastive-Curiosity-Driven Learning Framework (CCLF), which can fully exploit sample importance and improve learning efficiency in a self-supervised manner. Facilitated by the proposed contrastive curiosity, CCLF is capable of prioritizing the experience replay, selecting the most informative augmented inputs, and more importantly regularizing the Q-function as well as the encoder to concentrate more on under-learned data. Moreover, it encourages the agent to explore with a curiosity-based reward. As a result, the agent can focus on more informative samples and learn representation invariances more efficiently, with significantly reduced augmented inputs. We apply CCLF to several base RL algorithms and evaluate on the DeepMind Control Suite, Atari, and MiniGrid benchmarks, where our approach demonstrates superior sample efficiency and learning performances compared with other state-of-the-art methods.",CCLF：针对样品有效增强学习的对比度持续的学习框架。,在强化学习（RL）中，直接从高维观察结果中学习是一项挑战，在该观察结果中，最近已证明数据增强可以通过从RAW PIXELS中编码Invariances进行补救。然而，我们从经验上发现，并非所有样本都同样重要，因此仅仅注入更多增强的输入可能会导致Q学习中的不稳定。在本文中，我们通过开发模型不合时宜的对比度驱动的学习框架（CCLF）来系统地解决此问题，该框架可以完全利用样本重要性并以自我监督的方式提高学习效率。CCLF由提出的对比度的好奇心促进，能够优先考虑经验重播，选择最有用的增强输入，更重要的是将Q功能的正规化以及编码器正规化，以便将更多的数据集中在潜在的数据上。此外，它鼓励代理商以好奇心的奖励进行探索。结果，代理可以专注于更有用的样本，并更有效地学习表示的不变，并显着减少了增强输入。我们将CCLF应用于多种基本RL算法，并对DeepMind Control Suite，Atari和Minigrid基准进行评估，我们的方法与其他最先进的方法相比，我们的方法证明了卓越的样本效率和学习性能。,https://arxiv.org/abs/2205.00943,IJCAI,True,False,False,False
1790,Vertically Federated Graph Neural Network for Privacy-Preserving Node Classification,"J Zhou, C Chen, L Zheng, H Wu, J Wu, X Zheng, B Wu, Z Liu, L Wang",nan,用于隐私节点分类的垂直联合图形神经网络, ,https://scholar.google.com.hk,IJCAI,False,True,False,False
1791,Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting.,"Mingyang Chen, Wen Zhang, Zhen Yao, Xiangnan Chen, Mengxiao Ding, Fei Huang, Huajun Chen","We study the knowledge extrapolation problem to embed new components (i.e., entities and relations) that come with emerging knowledge graphs (KGs) in the federated setting. In this problem, a model trained on an existing KG needs to embed an emerging KG with unseen entities and relations. To solve this problem, we introduce the meta-learning setting, where a set of tasks are sampled on the existing KG to mimic the link prediction task on the emerging KG. Based on sampled tasks, we meta-train a graph neural network framework that can construct features for unseen components based on structural information and output embeddings for them. Experimental results show that our proposed method can effectively embed unseen components and outperforms models that consider inductive settings for KGs and baselines that directly use conventional KG embedding methods.",基于元学习的知识外推针对联合环境中的知识图。,我们研究了在联合环境中带有新兴知识图（KGS）附带的新组件（即实体和关系）的知识外推问题。在这个问题中，经过培训的现有KG的模型需要将新兴的公园与看不见的实体和关系嵌入。为了解决这个问题，我们介绍了元学习设置，其中在现有kg上对一组任务进行采样，以模仿新兴kg上的链接预测任务。基于采样任务，我们将图形神经网络框架进行元训练，该框架可以基于结构信息和输出嵌入来构建看不见的组件的功能。实验结果表明，我们提出的方法可以有效地嵌入看不见的组件，并优于考虑直接使用常规KG嵌入方法的KG和基准的诱导设置的模型。,https://arxiv.org/abs/2205.04692,IJCAI,True,False,False,False
1792,FedDUAP: Federated Learning with Dynamic Update and Adaptive Pruning Using Shared Data on the Server.,"Hong Zhang, Ji Liu, Juncheng Jia, Yang Zhou, Huaiyu Dai, Dejing Dou","Despite achieving remarkable performance, Federated Learning (FL) suffers from two critical challenges, i.e., limited computational resources and low training efficiency. In this paper, we propose a novel FL framework, i.e., FedDUAP, with two original contributions, to exploit the insensitive data on the server and the decentralized data in edge devices to further improve the training efficiency. First, a dynamic server update algorithm is designed to exploit the insensitive data on the server, in order to dynamically determine the optimal steps of the server update for improving the convergence and accuracy of the global model. Second, a layer-adaptive model pruning method is developed to perform unique pruning operations adapted to the different dimensions and importance of multiple layers, to achieve a good balance between efficiency and effectiveness. By integrating the two original techniques together, our proposed FL model, FedDUAP, significantly outperforms baseline approaches in terms of accuracy (up to 4.8% higher), efficiency (up to 2.8 times faster), and computational cost (up to 61.9% smaller).",FedDuap：使用服务器上的共享数据进行动态更新和自适应修剪的联合学习。,尽管表现出色，但联邦学习（FL）仍面临两个关键挑战，即计算资源有限和培训效率低。在本文中，我们提出了一个新颖的FL框架，即用两种原始贡献来利用服务器上的不敏感数据和边缘设备中的分散数据，以进一步提高训练效率。首先，动态服务器更新算法旨在利用服务器上的不敏感数据，以动态确定服务器更新的最佳步骤，以提高全局模型的收敛性和准确性。其次，开发了一种层自适应模型修剪方法，以执行适合多层不同维度和重要性的独特修剪操作，以在效率和有效性之间取得良好的平衡。通过将两种原始技术集成在一起，我们提出的FL模型FedDuap在准确性（高达4.8％），效率（最高2.8倍）和计算成本（最高61.9％）方面极大地超过了基线方法（高达4.8％）。,https://arxiv.org/abs/2204.11536,IJCAI,True,False,False,False
1793,Relational Triple Extraction: One Step is Enough.,"Yu-Ming Shang, Heyan Huang, Xin Sun, Wei Wei, Xian-Ling Mao","Extracting relational triples from unstructured text is an essential task in natural language processing and knowledge graph construction. Existing approaches usually contain two fundamental steps: (1) finding the boundary positions of head and tail entities; (2) concatenating specific tokens to form triples. However, nearly all previous methods suffer from the problem of error accumulation, i.e., the boundary recognition error of each entity in step (1) will be accumulated into the final combined triples. To solve the problem, in this paper, we introduce a fresh perspective to revisit the triple extraction task, and propose a simple but effective model, named DirectRel. Specifically, the proposed model first generates candidate entities through enumerating token sequences in a sentence, and then transforms the triple extraction task into a linking problem on a ""head $\rightarrow$ tail"" bipartite graph. By doing so, all triples can be directly extracted in only one step. Extensive experimental results on two widely used datasets demonstrate that the proposed model performs better than the state-of-the-art baselines.",关系三重提取：一步就足够了。,从非结构化文本中提取关系三联元是自然语言处理和知识图构造中的重要任务。现有方法通常包含两个基本步骤：（1）找到头部和尾部实体的边界位置；（2）串联特定令牌形成三元组。但是，几乎所有以前的方法都遇到了误差累积问题，即，步骤（1）中每个实体的边界识别误差都将累积到最终的组合三元组中。为了解决问题，在本文中，我们介绍了一个新的视角来重新审视三重提取任务，并提出了一个简单但有效的模型，名为Directrel。具体而言，提出的模型首先通过句子中的列举令牌序列生成候选实体，然后将三重提取任务转换为“ head $ \ rightarrow $ tail”两部分图上的链接问题。通过这样做，所有三元组只能在一个步骤中直接提取。在两个广泛使用的数据集上进行的广泛实验结果表明，所提出的模型的性能比最先进的基线更好。,https://arxiv.org/abs/2205.05270,IJCAI,True,False,False,False
1794,Region-level Contrastive and Consistency Learning for Semi-Supervised Semantic Segmentation.,"Jianrong Zhang, Tianyi Wu, Chuanghao Ding, Hongwei Zhao, Guodong Guo","Current semi-supervised semantic segmentation methods mainly focus on designing pixel-level consistency and contrastive regularization. However, pixel-level regularization is sensitive to noise from pixels with incorrect predictions, and pixel-level contrastive regularization has memory and computational cost with O(pixel_num^2). To address the issues, we propose a novel region-level contrastive and consistency learning framework (RC^2L) for semi-supervised semantic segmentation. Specifically, we first propose a Region Mask Contrastive (RMC) loss and a Region Feature Contrastive (RFC) loss to accomplish region-level contrastive property. Furthermore, Region Class Consistency (RCC) loss and Semantic Mask Consistency (SMC) loss are proposed for achieving region-level consistency. Based on the proposed region-level contrastive and consistency regularization, we develop a region-level contrastive and consistency learning framework (RC^2L) for semi-supervised semantic segmentation, and evaluate our RC$^2$L on two challenging benchmarks (PASCAL VOC 2012 and Cityscapes), outperforming the state-of-the-art.",半监督语义分割的区域级对比度和一致性学习。,当前的半监督语义分割方法主要集中于设计像素级的一致性和对比度正则化。但是，像素级正则化对具有错误预测的像素的噪声敏感，并且像素级对比度正则化具有O（Pixel_num^2）的内存和计算成本。为了解决这些问题，我们为半监督语义分割提出了一个新颖的区域级对比度和一致性学习框架（RC^2L）。具体而言，我们首先提出区域掩模对比度（RMC）损失和区域特征对比度（RFC）损失，以完成区域级的对比性质。此外，为实现区域级别的一致性提出了区域类别一致性（RCC）损失和语义面具一致性（SMC）损失。基于提议的区域层面的对比和一致性正则化，我们为半监督语义分割开发了一个区域级别的对比度和一致性学习框架（RC^2L），并评估我们的RC $^2 $ L在两个具有挑战性的基准上（Pascal）（PascalVOC 2012和CityScapes），表现优于最先进的。,https://arxiv.org/abs/2204.13314,IJCAI,True,False,False,False
1795,Spiking Graph Convolutional Networks.,"Zulun Zhu, Jiaying Peng, Jintang Li, Liang Chen, Qi Yu, Siqiang Luo","Graph Convolutional Networks (GCNs) achieve an impressive performance due to the remarkable representation ability in learning the graph information. However, GCNs, when implemented on a deep network, require expensive computation power, making them difficult to be deployed on battery-powered devices. In contrast, Spiking Neural Networks (SNNs), which perform a bio-fidelity inference process, offer an energy-efficient neural architecture. In this work, we propose SpikingGCN, an end-to-end framework that aims to integrate the embedding of GCNs with the biofidelity characteristics of SNNs. The original graph data are encoded into spike trains based on the incorporation of graph convolution. We further model biological information processing by utilizing a fully connected layer combined with neuron nodes. In a wide range of scenarios (e.g. citation networks, image graph classification, and recommender systems), our experimental results show that the proposed method could gain competitive performance against state-of-the-art approaches. Furthermore, we show that SpikingGCN on a neuromorphic chip can bring a clear advantage of energy efficiency into graph data analysis, which demonstrates its great potential to construct environment-friendly machine learning models.",尖峰图卷积网络。,图形卷积网络（GCN）由于学习图信息的显着表示能力而实现了令人印象深刻的性能。但是，GCN在深网上实施时需要昂贵的计算功率，因此很难将其部署在电池供电的设备上。相比之下，执行生物保真推理过程的尖峰神经网络（SNN）提供了节能的神经结构。在这项工作中，我们提出了SpikingGCN，这是一个端到端框架，旨在将GCN的嵌入与SNN的生物层性特征相结合。原始图数据根据图形卷积的合并编码为尖峰列车。我们通过利用与神经元节点结合的完全连接的层来进一步对生物信息处理进行建模。在各种场景（例如引用网络，图像图分类和推荐系统）中，我们的实验结果表明，该方法可以针对最新方法获得竞争性能。此外，我们表明，在神经形态芯片上的SpikingGCN可以将能源效率的明显优势带入图形数据分析中，这表明了其构建环境友好的机器学习模型的巨大潜力。,https://arxiv.org/abs/2205.02767,IJCAI,True,False,False,False
1796,Ensemble Multi-Relational Graph Neural Networks.,"Yuling Wang, Hao Xu, Yanhua Yu, Mengdi Zhang, Zhenhao Li, Yuji Yang, Wei Wu","It is well established that graph neural networks (GNNs) can be interpreted and designed from the perspective of optimization objective. With this clear optimization objective, the deduced GNNs architecture has sound theoretical foundation, which is able to flexibly remedy the weakness of GNNs. However, this optimization objective is only proved for GNNs with single-relational graph. Can we infer a new type of GNNs for multi-relational graphs by extending this optimization objective, so as to simultaneously solve the issues in previous multi-relational GNNs, e.g., over-parameterization? In this paper, we propose a novel ensemble multi-relational GNNs by designing an ensemble multi-relational (EMR) optimization objective. This EMR optimization objective is able to derive an iterative updating rule, which can be formalized as an ensemble message passing (EnMP) layer with multi-relations. We further analyze the nice properties of EnMP layer, e.g., the relationship with multi-relational personalized PageRank. Finally, a new multi-relational GNNs which well alleviate the over-smoothing and over-parameterization issues are proposed. Extensive experiments conducted on four benchmark datasets well demonstrate the effectiveness of the proposed model.",合奏多关系图神经网络。,众所周知，可以从优化目标的角度来解释和设计图形神经网络（GNN）。有了这个明确的优化目标，推论的GNNS体系结构具有合理的理论基础，能够灵活地纠正GNN的弱点。但是，仅对具有单相关图的GNN证明了此优化目标。我们是否可以通过扩展此优化目标来推断一种新型的GNN用于多关系图，以便同时解决以前的多关系GNN中的问题，例如过度参数化？在本文中，我们通过设计一个集合多关系（EMR）优化目标来提出一种新颖的集合多关系GNN。此EMR优化目标能够得出一个迭代更新规则，该规则可以正式化为具有多关系的整体消息传递（ENMP）层。我们进一步分析了ENMP层的良好属性，例如，与多关系个性化Pagerank的关系。最后，提出了一种新的跨关系GNN，可以很好地缓解过度平滑和过度参数化的问题。在四个基准数据集上进行的广泛实验很好地证明了该模型的有效性。,https://arxiv.org/abs/2205.12076,IJCAI,True,False,False,False
1797,Fine-Tuning Graph Neural Networks via Graph Topology Induced Optimal Transport.,"Jiying Zhang, Xi Xiao, Long-Kai Huang, Yu Rong, Yatao Bian","Recently, the pretrain-finetuning paradigm has attracted tons of attention in graph learning community due to its power of alleviating the lack of labels problem in many real-world applications. Current studies use existing techniques, such as weight constraint, representation constraint, which are derived from images or text data, to transfer the invariant knowledge from the pre-train stage to fine-tuning stage. However, these methods failed to preserve invariances from graph structure and Graph Neural Network (GNN) style models. In this paper, we present a novel optimal transport-based fine-tuning framework called GTOT-Tuning, namely, Graph Topology induced Optimal Transport fine-Tuning, for GNN style backbones. GTOT-Tuning is required to utilize the property of graph data to enhance the preservation of representation produced by fine-tuned networks. Toward this goal, we formulate graph local knowledge transfer as an Optimal Transport (OT) problem with a structural prior and construct the GTOT regularizer to constrain the fine-tuned model behaviors. By using the adjacency relationship amongst nodes, the GTOT regularizer achieves node-level optimal transport procedures and reduces redundant transport procedures, resulting in efficient knowledge transfer from the pre-trained models. We evaluate GTOT-Tuning on eight downstream tasks with various GNN backbones and demonstrate that it achieves state-of-the-art fine-tuning performance for GNNs.",通过图形拓扑诱导最佳传输的微调图神经网络。,最近，由于其在许多现实世界应用中减轻了标签问题的能力，因此在图形学习社区中引起了很多关注。当前的研究使用从图像或文本数据得出的现有技术，例如重量约束，表示的约束，将不变知识从训练阶段转移到微调阶段。但是，这些方法无法从图形结构和图神经网络（GNN）样式模型中保存不变。在本文中，我们提出了一个新型的基于最佳运输的微型微调框架，称为GTOT调整，即GNN样式骨架，即图形拓扑诱导的最佳传输微调。需要GTOT调整来利用图形数据的属性来增强微调网络产生的表示形式。为了实现这一目标，我们将图形局部知识转移作为最佳传输（OT）问题，并具有结构性先验，并构建GTOT正常器以限制微型模型行为。通过使用节点之间的邻接关系，GTOT正常化程序可以实现节点级最佳传输程序并减少冗余传输程序，从而从预训练的模型中有效地传递了知识转移。我们用各种GNN骨架评估了八项下游任务的GTOT调整，并证明它可以为GNN实现最新的微调性能。,https://arxiv.org/abs/2203.10453,IJCAI,True,False,False,False
1798,Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning.,"Yae Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, Dimitrios Dimitriadis","Federated learning (FL) enables edge-devices to collaboratively learn a model without disclosing their private data to a central aggregating server. Most existing FL algorithms require models of identical architecture to be deployed across the clients and server, making it infeasible to train large models due to clients' limited system resources. In this work, we propose a novel ensemble knowledge transfer method named Fed-ET in which small models (different in architecture) are trained on clients, and used to train a larger model at the server. Unlike in conventional ensemble learning, in FL the ensemble can be trained on clients' highly heterogeneous data. Cognizant of this property, Fed-ET uses a weighted consensus distillation scheme with diversity regularization that efficiently extracts reliable consensus from the ensemble while improving generalization by exploiting the diversity within the ensemble. We show the generalization bound for the ensemble of weighted models trained on heterogeneous datasets that supports the intuition of Fed-ET. Our experiments on image and language tasks show that Fed-ET significantly outperforms other state-of-the-art FL algorithms with fewer communicated parameters, and is also robust against high data-heterogeneity.",在联邦学习中培训大型模型的异质集合知识转移。,联合学习（FL）使边缘设备可以协作学习模型，而无需向中央聚合服务器披露其私人数据。大多数现有的FL算法都需要在客户和服务器上部署相同的体系结构模型，这使得由于客户端的系统资源有限，因此训练大型模型是不可行的。在这项工作中，我们提出了一种名为fed-et的新型集成知识转移方法，其中小型模型（不同的体系结构）是对客户端进行培训的，并用于在服务器上培训更大的模型。与传统的合奏学习不同，在FL中，合奏可以接受客户高度异质数据的培训。认识到该特性，Fed-ET使用具有多样性正则化的加权共识方案，从而有效地从合奏中提取可靠的共识，同时通过利用合奏中的多样性来改善概括。我们显示了在支持FED-ET直觉的异质数据集上训练的加权模型集合的概括。我们在图像和语言任务上的实验表明，Fed-Et明显胜过其他最先进的FL算法，而传达参数较少，并且在高数据杂种性方面也具有鲁棒性。,https://arxiv.org/abs/2204.12703,IJCAI,True,False,False,False
1799,CATrans: Context and Affinity Transformer for Few-Shot Segmentation.,"Shan Zhang, Tianyi Wu, Sitong Wu, Guodong Guo","Few-shot segmentation (FSS) aims to segment novel categories given scarce annotated support images. The crux of FSS is how to aggregate dense correlations between support and query images for query segmentation while being robust to the large variations in appearance and context. To this end, previous Transformer-based methods explore global consensus either on context similarity or affinity map between support-query pairs. In this work, we effectively integrate the context and affinity information via the proposed novel Context and Affinity Transformer (CATrans) in a hierarchical architecture. Specifically, the Relation-guided Context Transformer (RCT) propagates context information from support to query images conditioned on more informative support features. Based on the observation that a huge feature distinction between support and query pairs brings barriers for context knowledge transfer, the Relation-guided Affinity Transformer (RAT) measures attention-aware affinity as auxiliary information for FSS, in which the self-affinity is responsible for more reliable cross-affinity. We conduct experiments to demonstrate the effectiveness of the proposed model, outperforming the state-of-the-art methods.",CATRANS：少数分段的上下文和亲和力变压器。,几乎没有射击分割（FSS）旨在分割新的类别，并给出稀缺的注释支持图像。FSS的症结在于如何汇总支持图像和查询图像之间的密集相关性，以进行查询分割，同时对外观和上下文的较大变化进行稳健。为此，先前基于变压器的方法在支持 - 查询对之间的上下文相似性或亲和力图上探索全局共识。在这项工作中，我们通过拟议中的新颖上下文和层次结构中提出的新颖上下文和亲和力变压器（CATRANS）有效地整合了上下文和亲和力信息。具体而言，关系引导的上下文变压器（RCT）传播上下文信息，从支持到查询图像以更有信息的支持功能为条件。基于这样的观察，即支持和查询对之间的巨大特征区别为上下文知识转移带来了障碍，关系引导的亲和力变压器（Rat）将注意力感知的亲和力作为FSS的辅助信息，在其中自我承受的责任负责。更可靠的跨亲属。我们进行实验以证明所提出的模型的有效性，优于最先进的方法。,https://arxiv.org/abs/2204.12817,IJCAI,True,False,False,False
1800,Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph.,"Tong Zhu, Xiaoye Qu, Wenliang Chen, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan, Min Zhang","There are two main challenges in document-level event extraction: 1) argument entities are scattered in different sentences, and 2) event triggers are often not available. To address these challenges, most previous studies mainly focus on building argument chains in an autoregressive way, which is inefficient in both training and inference. In contrast to the previous studies, we propose a fast and lightweight model named as PTPCG. We design a non-autoregressive decoding algorithm to perform event argument combination extraction on pruned complete graphs, which are constructed under the guidance of the automatically selected pseudo triggers. Compared to the previous systems, our system achieves competitive results with lower resource consumption, taking only 3.6% GPU time (pfs-days) for training and up to 8.5 times faster for inference. Besides, our approach shows superior compatibility for the datasets with (or without) triggers and the pseudo triggers can be the supplements for annotated triggers to make further improvements.",通过伪触发式修剪完整的图形提取有效的文档级事件提取。,文档级事件提取中有两个主要挑战：1）参数实体分散在不同的句子中，2）事件触发器通常不可用。为了应对这些挑战，大多数以前的研究主要集中在自回归方式上建立论证链，这在培训和推理方面效率低下。与以前的研究相反，我们提出了一个名为PTPCG的快速轻巧模型。我们设计了一种非自动回解码算法，以在修剪完整的图表上执行事件参数组合提取，这些图是在自动选择的伪触发器的指导下构建的。与以前的系统相比，我们的系统以较低的资源消耗获得了竞争成果，仅花费3.6％的GPU时间（PFS-DAYS）进行培训，并且推断的速度最高为8.5倍。此外，我们的方法显示出具有（或没有）触发器的数据集的较高兼容性，并且伪触发器可以是带注释的触发器的补充，以进一步改进。,https://arxiv.org/abs/2112.06013,IJCAI,True,False,False,False
1801,Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation.,"Pei Ke, Haozhe Ji, Zhenyu Yang, Yi Huang, Junlan Feng, Xiaoyan Zhu, Minlie Huang","Despite the success of text-to-text pre-trained models in various natural language generation (NLG) tasks, the generation performance is largely restricted by the number of labeled data in downstream tasks, particularly in data-to-text generation tasks. Existing works mostly utilize abundant unlabeled structured data to conduct unsupervised pre-training for task adaption, which fail to model the complex relationship between source structured data and target texts. Thus, we introduce self-training as a better few-shot learner than task-adaptive pre-training, which explicitly captures this relationship via pseudo-labeled data generated by the pre-trained model. To alleviate the side-effect of low-quality pseudo-labeled data during self-training, we propose a novel method called Curriculum-Based Self-Training (CBST) to effectively leverage unlabeled data in a rearranged order determined by the difficulty of text generation. Experimental results show that our method can outperform fine-tuning and task-adaptive pre-training methods, and achieve state-of-the-art performance in the few-shot setting of data-to-text generation.",基于课程的自我培训使数据到文本生成更好。,尽管在各种自然语言生成（NLG）任务中，文本到文本预先训练的模型成功，但生成性能在很大程度上受到下游任务中标记的数据的数量，尤其是在数据到文本生成任务中的数量。现有的作品主要利用丰富的未标记结构化数据来进行无监督的任务适应预训练，这无法建模源结构化数据和目标文本之间的复杂关系。因此，我们将自我训练作为比任务自适应的预训练更好的几次学习者的介绍，后者通过预先训练的模型生成的伪标记数据明确捕获了这种关系。为了减轻自训练期间低质量伪标记数据的副作用，我们提出了一种称为基于课程的自我训练（CBST）的新方法，以在重新排列的顺序中有效利用未标记的数据，该数据由文本生成难度确定。实验结果表明，我们的方法可以胜过微调和任务自适应的预训练方法，并在数据到文本生成的几次设置中实现最先进的性能。,https://arxiv.org/abs/2206.02712,IJCAI,True,False,False,False
1802,Raising the Bar in Graph-level Anomaly Detection.,"Chen Qiu, Marius Kloft, Stephan Mandt, Maja Rudolph","Graph-level anomaly detection has become a critical topic in diverse areas, such as financial fraud detection and detecting anomalous activities in social networks. While most research has focused on anomaly detection for visual data such as images, where high detection accuracies have been obtained, existing deep learning approaches for graphs currently show considerably worse performance. This paper raises the bar on graph-level anomaly detection, i.e., the task of detecting abnormal graphs in a set of graphs. By drawing on ideas from self-supervised learning and transformation learning, we present a new deep learning approach that significantly improves existing deep one-class approaches by fixing some of their known problems, including hypersphere collapse and performance flip. Experiments on nine real-world data sets involving nine techniques reveal that our method achieves an average performance improvement of 11.8% AUC compared to the best existing approach.",在图形异常检测中提高栏。,图形异常检测已成为不同领域的关键主题，例如财务欺诈检测和检测社交网络中的异常活动。尽管大多数研究都集中在视觉数据（例如图像）的异常检测上，但在该图像中获得了高检测精度，但目前的现有图形的深度学习方法表现出较差的性能。本文提高了图级异常检测的标准，即在一组图中检测异常图的任务。通过利用自我监督的学习和转型学习中的思想，我们提出了一种新的深度学习方法，该方法通过解决了他们的一些已知问题，包括Hypersphere Collapse和Exprormance Flip，从而显着改善了现有的深层单级方法。涉及九种技术的九个现实世界数据集的实验表明，与最佳现有方法相比，我们的方法的平均性能提高了11.8％。,https://arxiv.org/abs/2205.13845,IJCAI,True,False,False,False
1803,FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework.,"Guozheng Li, Xu Chen, Peng Wang, Jiafeng Xie, Qiqing Luo","Recent work for extracting relations from texts has achieved excellent performance. However, most existing methods pay less attention to the efficiency, making it still challenging to quickly extract relations from massive or streaming text data in realistic scenarios. The main efficiency bottleneck is that these methods use a Transformer-based pre-trained language model for encoding, which heavily affects the training speed and inference speed. To address this issue, we propose a fast relation extraction model (FastRE) based on convolutional encoder and improved cascade binary tagging framework. Compared to previous work, FastRE employs several innovations to improve efficiency while also keeping promising performance. Concretely, FastRE adopts a novel convolutional encoder architecture combined with dilated convolution, gated unit and residual connection, which significantly reduces the computation cost of training and inference, while maintaining the satisfactory performance. Moreover, to improve the cascade binary tagging framework, FastRE first introduces a type-relation mapping mechanism to accelerate tagging efficiency and alleviate relation redundancy, and then utilizes a position-dependent adaptive thresholding strategy to obtain higher tagging accuracy and better model generalization. Experimental results demonstrate that FastRE is well balanced between efficiency and performance, and achieves 3-10x training speed, 7-15x inference speed faster, and 1/100 parameters compared to the state-of-the-art models, while the performance is still competitive.",Fastre：与卷积编码器和改进的级联二进制标签框架进行快速关系提取。,从文本中提取关系的最新工作取得了出色的表现。但是，大多数现有方法对效率的关注较少，这使得在现实情况下快速从大规模或流式的文本数据中提取关系仍然具有挑战性。主要效率瓶颈是这些方法使用基于变压器的预训练的语言模型进行编码，从而严重影响训练速度和推理速度。为了解决此问题，我们建议基于卷积编码器和改进的级联二进制标记框架的快速关系提取模型（Fastre）。与以前的工作相比，法斯特（Fastre）采用了几项创新来提高效率，同时保持有希望的绩效。具体而言，法斯特（Fastre）采用了一种新颖的卷积编码器结构，结合了扩张的卷积，封闭式单位和残留连接，从而大大降低了训练和推理的计算成本，同时保持令人满意的性能。此外，为了改善级联二进制标记框架，Fastre首先引入了类型相关的映射机制，以加速标记效率并减轻关系冗余，然后利用依赖位置的适应性阈值策略来获得更高的标记准确性和更好的模型通用。实验结果表明，法斯特在效率和性能之间取得了很好的平衡，与最新的ART模型相比，相比竞争的。,https://arxiv.org/abs/2205.02490,IJCAI,True,False,False,False
1804,Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer.,"Guangwei Gao, Zhengxue Wang, Juncheng Li, Wenjie Li, Yi Yu, Tieyong Zeng","Single-image super-resolution (SISR) has achieved significant breakthroughs with the development of deep learning. However, these methods are difficult to be applied in real-world scenarios since they are inevitably accompanied by the problems of computational and memory costs caused by the complex operations. To solve this issue, we propose a Lightweight Bimodal Network (LBNet) for SISR. Specifically, an effective Symmetric CNN is designed for local feature extraction and coarse image reconstruction. Meanwhile, we propose a Recursive Transformer to fully learn the long-term dependence of images thus the global information can be fully used to further refine texture details. Studies show that the hybrid of CNN and Transformer can build a more efficient model. Extensive experiments have proved that our LBNet achieves more prominent performance than other state-of-the-art methods with a relatively low computational cost and memory consumption. The code is available at https://github.com/IVIPLab/LBNet.",通过对称CNN和递归变压器，用于单像超分辨率的轻型双峰网络。,单像超分辨率（SISR）通过深度学习的发展取得了重大突破。但是，这些方法很难在现实世界中应用，因为它们不可避免地伴随着复杂操作引起的计算和记忆成本问题。为了解决此问题，我们为SISR提出了一个轻型双峰网络（LBNET）。具体而言，有效的对称CNN设计用于局部特征提取和粗图像重建。同时，我们提出了一个递归变压器来充分学习图像的长期依赖性，因此可以完全使用全局信息来进一步完善纹理细节。研究表明，CNN和Transform的混合动力可以构建一个更有效的模型。广泛的实验证明，我们的LBNET比其他最先进的方法具有相对较低的计算成本和记忆消耗。该代码可在https://github.com/iviplab/lbnet上找到。,https://arxiv.org/abs/2204.13286,IJCAI,True,False,False,False
1805,Learning Target-aware Representation for Visual Tracking via Informative Interactions.,"Mingzhe Guo, Zhipeng Zhang, Heng Fan, Liping Jing, Yilin Lyu, Bing Li, Weiming Hu","We introduce a novel backbone architecture to improve target-perception ability of feature representation for tracking. Specifically, having observed that de facto frameworks perform feature matching simply using the outputs from backbone for target localization, there is no direct feedback from the matching module to the backbone network, especially the shallow layers. More concretely, only the matching module can directly access the target information (in the reference frame), while the representation learning of candidate frame is blind to the reference target. As a consequence, the accumulation effect of target-irrelevant interference in the shallow stages may degrade the feature quality of deeper layers. In this paper, we approach the problem from a different angle by conducting multiple branch-wise interactions inside the Siamese-like backbone networks (InBN). At the core of InBN is a general interaction modeler (GIM) that injects the prior knowledge of reference image to different stages of the backbone network, leading to better target-perception and robust distractor-resistance of candidate feature representation with negligible computation cost. The proposed GIM module and InBN mechanism are general and applicable to different backbone types including CNN and Transformer for improvements, as evidenced by our extensive experiments on multiple benchmarks. In particular, the CNN version (based on SiamCAR) improves the baseline with 3.2/6.9 absolute gains of SUC on LaSOT/TNL2K, respectively. The Transformer version obtains SUC scores of 65.7/52.0 on LaSOT/TNL2K, which are on par with recent state of the arts. Code and models will be released.",通过信息互动来学习视觉跟踪的目标感知表示。,我们引入了一种新型的骨干结构，以提高特征表示跟踪的目标感知能力。具体而言，在观察到事实上的框架可以执行功能匹配，只需使用主链的输出进行目标定位，因此没有直接反馈从匹配模块到骨干网络，尤其是浅层层。更具体地说，只有匹配模块才能直接访问目标信息（在参考框架中），而候选框架的表示形式对参考目标视而不见。结果，目标 - 近水平干扰在浅阶段的积累效应可能会降低更深层的特征质量。在本文中，我们通过在类似暹罗的骨干网络（INBN）内进行多种分支相互作用来解决问题。INBN的核心是一家通用交互建模器（GIM），它将参考图像的先验知识注入了骨干网络的不同阶段，从而使候选特征表示的更好的目标感知和强大的分散分散分心的计算计算成本。所提出的GIM模块和INBN机制是一般的，适用于包括CNN和Transferer在内的不同主干类型进行改进，这是我们在多个基准上进行的广泛实验所证明的。特别是，CNN版本（基于Siamcar）分别在Lasot/TNL2K上以3.2/6.9的绝对增益来改善基线。Transformer版本在Lasot/TNL2K上获得65.7/52.0的SUC分数，与最近的艺术状态相当。代码和模型将发布。,https://arxiv.org/abs/2201.02526,IJCAI,True,False,False,False
1806,Robust Fine-tuning via Perturbation and Interpolation from In-batch Instances.,"Shoujie Tong, Qingxiu Dong, Damai Dai, Yifan song, Tianyu Liu, Baobao Chang, Zhifang Sui","Fine-tuning pretrained language models (PLMs) on downstream tasks has become common practice in natural language processing. However, most of the PLMs are vulnerable, e.g., they are brittle under adversarial attacks or imbalanced data, which hinders the application of the PLMs on some downstream tasks, especially in safe-critical scenarios. In this paper, we propose a simple yet effective fine-tuning method called Match-Tuning to force the PLMs to be more robust. For each instance in a batch, we involve other instances in the same batch to interact with it. To be specific, regarding the instances with other labels as a perturbation, Match-Tuning makes the model more robust to noise at the beginning of training. While nearing the end, Match-Tuning focuses more on performing an interpolation among the instances with the same label for better generalization. Extensive experiments on various tasks in GLUE benchmark show that Match-Tuning consistently outperforms the vanilla fine-tuning by $1.64$ scores. Moreover, Match-Tuning exhibits remarkable robustness to adversarial attacks and data imbalance.",通过扰动和插入实例的插值进行稳健的微调。,在下游任务上进行微调预审计的语言模型（PLM）已成为自然语言处理中的普遍做法。但是，大多数PLM都是脆弱的，例如，在对抗性攻击或不平衡数据下它们是脆弱的，这阻碍了PLM在某些下游任务上的应用，尤其是在安全关键的情况下。在本文中，我们提出了一种称为“匹配调节”的简单而有效的微调方法，以迫使PLM更强大。对于批处理中的每个实例，我们将同一批次中的其他实例与之互动。具体来说，关于其他标签作为扰动的实例，匹配调整使该模型在训练开始时对噪声更强大。在接近末端时，比赛调整的重点更多地是在具有相同标签的实例之间进行插值以更好地概括。关于胶水基准中各种任务的广泛实验表明，匹配调节始终优于香草微调的$ 1.64 $分数。此外，对抗攻击和数据失衡的匹配调整表现出显着的鲁棒性。,https://arxiv.org/abs/2205.00633,IJCAI,True,False,False,False
1807,Stochastic Coherence Over Attention Trajectory For Continuous Learning In Video Streams.,"Matteo Tiezzi, Simone Marullo, Lapo Faggi, Enrico Meloni, Alessandro Betti, Stefano Melacci","Devising intelligent agents able to live in an environment and learn by observing the surroundings is a longstanding goal of Artificial Intelligence. From a bare Machine Learning perspective, challenges arise when the agent is prevented from leveraging large fully-annotated dataset, but rather the interactions with supervisory signals are sparsely distributed over space and time. This paper proposes a novel neural-network-based approach to progressively and autonomously develop pixel-wise representations in a video stream. The proposed method is based on a human-like attention mechanism that allows the agent to learn by observing what is moving in the attended locations. Spatio-temporal stochastic coherence along the attention trajectory, paired with a contrastive term, leads to an unsupervised learning criterion that naturally copes with the considered setting. Differently from most existing works, the learned representations are used in open-set class-incremental classification of each frame pixel, relying on few supervisions. Our experiments leverage 3D virtual environments and they show that the proposed agents can learn to distinguish objects just by observing the video stream. Inheriting features from state-of-the art models is not as powerful as one might expect.",在视频流中连续学习的关注轨迹的随机连贯性。,设计能够在环境中生活并通过观察周围环境学习的智能代理是人工智能的长期目标。从裸露的机器学习的角度来看，当阻止代理商利用大型完全注销的数据集时，就会出现挑战，而与监督信号的交互在时空和时间上分布稀少。本文提出了一种基于神经网络的新型方法，可以在视频流中逐步自主地开发像素表示。所提出的方法基于一种类似人类的注意机制，该机制使代理可以通过观察参加的位置移动来学习。沿注意轨迹的时空随机连贯性与对比度术语配对，导致无监督的学习标准自然地应对所考虑的设置。与大多数现有作品不同，学到的表示形式用于每个框架像素的开放集类侵入性分类，依赖于很少的监督。我们的实验利用了3D虚拟环境，他们表明拟议的代理只能通过观察视频流来学会区分对象。从最先进的模型继承功能并不像人们期望的那么强大。,https://arxiv.org/abs/2204.12193,IJCAI,True,False,False,False
1808,"I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose
  Estimation","Yiwei Ding, Wenjin Deng, Yinglin Zheng, Pengfei Liu, Meihong Wang, Xuan Cheng, Jianmin Bao, Dong Chen, Ming Zeng","In this paper, we present the Intra- and Inter-Human Relation Networks (I^2R-Net) for Multi-Person Pose Estimation. It involves two basic modules. First, the Intra-Human Relation Module operates on a single person and aims to capture Intra-Human dependencies. Second, the Inter-Human Relation Module considers the relation between multiple instances and focuses on capturing Inter-Human interactions. The Inter-Human Relation Module can be designed very lightweight by reducing the resolution of feature map, yet learn useful relation information to significantly boost the performance of the Intra-Human Relation Module. Even without bells and whistles, our method can compete or outperform current competition winners. We conduct extensive experiments on COCO, CrowdPose, and OCHuman datasets. The results demonstrate that the proposed model surpasses all the state-of-the-art methods. Concretely, the proposed method achieves 77.4% AP on CrowPose dataset and 67.8% AP on OCHuman dataset respectively, outperforming existing methods by a large margin. Additionally, the ablation study and visualization analysis also prove the effectiveness of our model.","I^2R-NET：多人姿势的内部和人际关系网络
  估计",在本文中，我们介绍了人际内和人际关系网络（I^2R-NET），以进行多人姿势估计。它涉及两个基本模块。首先，人类内部关系模块在一个人身上运行，旨在捕获人类内部依赖性。其次，人际关系模块考虑了多个实例之间的关系，并着重于捕获人间的相互作用。人际关系间的关系模块可以通过减少特征图的分辨率来设计非常轻巧，但学习有用的关系信息以显着提高人类内部关系模块的性能。即使没有铃铛和哨子，我们的方法也可以竞争或胜过当前的比赛获胜者。我们对可可，人群和ochuman数据集进行了广泛的实验。结果表明，所提出的模型超过了所有最新方法。具体而言，所提出的方法在众群数据集上达到了77.4％的AP和Ochuman数据集上的67.8％AP，从而超过了现有方法的大幅度优于较大的利润率。此外，消融研究和可视化分析还证明了我们的模型的有效性。,https://arxiv.org/abs/2206.10892,IJCAI,True,False,False,False
1809,Logically Consistent Adversarial Attacks for Soft Theorem Provers,"Alexander Gaskell, Yishu Miao, Lucia Specia, Francesca Toni","Recent efforts within the AI community have yielded impressive results towards ""soft theorem proving"" over natural language sentences using language models. We propose a novel, generative adversarial framework for probing and improving these models' reasoning capabilities. Adversarial attacks in this domain suffer from the logical inconsistency problem, whereby perturbations to the input may alter the label. Our Logically consistent AdVersarial Attacker, LAVA, addresses this by combining a structured generative process with a symbolic solver, guaranteeing logical consistency. Our framework successfully generates adversarial attacks and identifies global weaknesses common across multiple target models. Our analyses reveal naive heuristics and vulnerabilities in these models' reasoning capabilities, exposing an incomplete grasp of logical deduction under logic programs. Finally, in addition to effective probing of these models, we show that training on the generated samples improves the target model's performance.",逻辑上一致的对抗攻击，用于软定理掠夺者,AI社区中的最新努力为使用语言模型对自然语言句子的“软定理证明”产生了令人印象深刻的结果。我们提出了一个新颖的生成对抗框架，用于探测和改善这些模型的推理能力。该领域中的对抗性攻击遇到了逻辑上的不一致问题，因此对输入的扰动可能会改变标签。我们在逻辑上一致的对抗攻击者熔岩通过将结构化生成过程与符号求解器相结合，从而确保逻辑一致性来解决这一问题。我们的框架成功地产生了对抗性攻击，并确定了多个目标模型中常见的全球弱点。我们的分析揭示了这些模型的推理能力中天真的启发式方法和脆弱性，从而揭示了逻辑程序下逻辑扣除的不完全掌握。最后，除了对这些模型的有效探索外，我们还表明，对生成样品进行培训可以改善目标模型的性能。,https://arxiv.org/abs/2205.00047,IJCAI,True,False,False,False
1810,On the Utility of Prediction Sets in Human-AI Teams.,"Varun Babbar, Umang Bhatt, Adrian Weller","Research on human-AI teams usually provides experts with a single label, which ignores the uncertainty in a model's recommendation. Conformal prediction (CP) is a well established line of research that focuses on building a theoretically grounded, calibrated prediction set, which may contain multiple labels. We explore how such prediction sets impact expert decision-making in human-AI teams. Our evaluation on human subjects finds that set valued predictions positively impact experts. However, we notice that the predictive sets provided by CP can be very large, which leads to unhelpful AI assistants. To mitigate this, we introduce D-CP, a method to perform CP on some examples and defer to experts. We prove that D-CP can reduce the prediction set size of non-deferred examples. We show how D-CP performs in quantitative and in human subject experiments ($n=120$). Our results suggest that CP prediction sets improve human-AI team performance over showing the top-1 prediction alone, and that experts find D-CP prediction sets are more useful than CP prediction sets.",关于人类团队中预测集的实用性。,关于人类团队的研究通常为专家提供单个标签，这忽略了模型建议中的不确定性。共形预测（CP）是一项良好的研究线，重点是构建理论上接地的校准预测集，该集合可能包含多个标签。我们探讨了这种预测如何影响人类AI团队的专家决策。我们对人类受试者的评估发现，将有价值的预测对专家产生积极影响。但是，我们注意到CP提供的预测集可能非常大，这导致了无助的AI助手。为了减轻这种情况，我们介绍了D-CP，这是一种在某些示例中执行CP并延迟专家的方法。我们证明D-CP可以减少非脱佛像示例的预测集大小。我们展示了D-CP在定量和人类主题实验中的表现（$ n = 120 $）。我们的结果表明，CP预测集改善了人类AI团队的性能，而不是仅显示TOP-1预测，并且专家发现D-CP预测集比CP预测集更有用。,https://arxiv.org/abs/2205.01411,IJCAI,True,False,False,False
1811,Towards Universal Backward-Compatible Representation Learning.,"Binjie Zhang, Yixiao Ge, Yantao Shen, Shupeng Su, Chun Yuan, Xuyuan Xu, Yexin Wang, Ying Shan","Conventional model upgrades for visual search systems require offline refresh of gallery features by feeding gallery images into new models (dubbed as ""backfill""), which is time-consuming and expensive, especially in large-scale applications. The task of backward-compatible representation learning is therefore introduced to support backfill-free model upgrades, where the new query features are interoperable with the old gallery features. Despite the success, previous works only investigated a close-set training scenario (i.e., the new training set shares the same classes as the old one), and are limited by more realistic and challenging open-set scenarios. To this end, we first introduce a new problem of universal backward-compatible representation learning, covering all possible data split in model upgrades. We further propose a simple yet effective method, dubbed as Universal Backward-Compatible Training (UniBCT) with a novel structural prototype refinement algorithm, to learn compatible representations in all kinds of model upgrading benchmarks in a unified manner. Comprehensive experiments on the large-scale face recognition datasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.",朝着普遍的向后兼容表示学习。,视觉搜索系统的常规模型升级需要通过将图库图像馈入新型号（称为“回填”），需要脱机画廊功能，这是耗时且昂贵的，尤其是在大规模应用中。因此，介绍了向后兼容表示学习的任务，以支持无回填的模型升级，其中新的查询功能与旧画廊功能可互操作。尽管取得了成功，但以前的作品仅研究了近距离培训方案（即，新培训集与旧类别相同），并且受到更现实和更具挑战性的开放式场景的限制。为此，我们首先引入了通用向后兼容表示学习的新问题，涵盖了模型升级中所有可能的数据分配。我们进一步提出了一种简单而有效的方法，该方法将其称为具有新颖的结构原型改进算法的通用后向兼容训练（UNIBCT），以以统一的方式以各种模型升级基准来学习兼容表示。大规模面部识别数据集MS1MV3和IJB-C的全面实验完全证明了我们方法的有效性。,https://arxiv.org/abs/2203.01583,IJCAI,True,False,False,False
1812,Towards Robust Dense Retrieval via Local Ranking Alignment.,"xuanang chen, jian luo, ben he, le sun, yingfei sun","Dense retrieval (DR) has extended the employment of pre-trained language models, like BERT, for text ranking. However, recent studies have raised the robustness issue of DR model against query variations, like query with typos, along with non-trivial performance losses. Herein, we argue that it would be beneficial to allow the DR model to learn to align the relative positions of query-passage pairs in the representation space, as query variations cause the query vector to drift away from its original position, affecting the subsequent DR effectiveness. To this end, we propose RoDR, a novel robust DR model that learns to calibrate the in-batch local ranking of query variation to that of original query for the DR space alignment. Extensive experiments on MS MARCO and ANTIQUE datasets show that RoDR significantly improves the retrieval results on both the original queries and different types of query variations. Meanwhile, RoDR provides a general query noise-tolerate learning framework that boosts the robustness and effectiveness of various existing DR models. Our code and models are openly available at https://github.com/cxa-unique/RoDR.",通过本地排名对准进行稳健的密集检索。,密集的检索（DR）已扩展了诸如伯特（Bert）等文本排名的预训练语言模型的使用。然而，最近的研究提出了DR模型的鲁棒性问题，可以针对查询变化（例如与错别字查询）以及非平凡的性能损失。在本文中，我们认为，允许DR模型学会对齐在表示空间中的查询 - 通用对的相对位置将是有益的，因为查询变化会导致查询向量从其原始位置移开，从而影响了后续DR效力。为此，我们提出了Rodr，这是一种新颖的强大DR模型，该模型学会了将查询变化的批量内部排名校准为DR空间对齐的原始查询的局部排名。关于MARCO女士和古董数据集的广泛实验表明，RODR显着改善了原始查询和不同类型的查询变化的检索结果。同时，Rodr提供了一个通用的查询噪声学习框架，可提高各种现有DR模型的鲁棒性和有效性。我们的代码和型号可在https://github.com/cxa-unique/rodr上公开获得。,http://www.icip.org.cn/team/sunle/,IJCAI,True,False,False,False
1813,PG3: Policy-Guided Planning for Generalized Policy Generation.,"Ryan Yang, Tom Silver, Aidan Curtis, Tomas Lozano-Perez, Leslie Pack Kaelbling","A longstanding objective in classical planning is to synthesize policies that generalize across multiple problems from the same domain. In this work, we study generalized policy search-based methods with a focus on the score function used to guide the search over policies. We demonstrate limitations of two score functions and propose a new approach that overcomes these limitations. The main idea behind our approach, Policy-Guided Planning for Generalized Policy Generation (PG3), is that a candidate policy should be used to guide planning on training problems as a mechanism for evaluating that candidate. Theoretical results in a simplified setting give conditions under which PG3 is optimal or admissible. We then study a specific instantiation of policy search where planning problems are PDDL-based and policies are lifted decision lists. Empirical results in six domains confirm that PG3 learns generalized policies more efficiently and effectively than several baselines. Code: https://github.com/ryangpeixu/pg3",PG3：对广义政策生成的政策指导计划。,古典计划的一个长期目标是合成从同一领域跨越多个问题的政策。在这项工作中，我们研究了基于策略搜索的通用方法，重点是用于指导搜索政策的分数功能。我们证明了两个分数函数的局限性，并提出了一种克服这些局限性的新方法。我们的方法背后的主要思想，政策指导的广义政策生成计划（PG3），是应使用候选政策来指导培训问题的计划，以此作为评估该候选人的机制。简化设置中的理论结果给出了PG3最佳或可接受的条件。然后，我们研究了政策搜索的特定实例化，在该搜索中，计划问题是基于PDDL的，并取消了决策列表。六个领域的经验结果证实，与几个基准相比，PG3更有效地学习通用政策。代码：https：//github.com/ryangpeixu/pg3,https://arxiv.org/abs/2204.10420,IJCAI,True,False,False,False
1814,One Weird Trick to Improve Your Semi-Weakly Supervised Semantic Segmentation Model.,"Wonho Bae, Junhyug Noh, Milad Jalali Asadabadi, Danica J. Sutherland","Semi-weakly supervised semantic segmentation (SWSSS) aims to train a model to identify objects in images based on a small number of images with pixel-level labels, and many more images with only image-level labels. Most existing SWSSS algorithms extract pixel-level pseudo-labels from an image classifier - a very difficult task to do well, hence requiring complicated architectures and extensive hyperparameter tuning on fully-supervised validation sets. We propose a method called prediction filtering, which instead of extracting pseudo-labels, just uses the classifier as a classifier: it ignores any segmentation predictions from classes which the classifier is confident are not present. Adding this simple post-processing method to baselines gives results competitive with or better than prior SWSSS algorithms. Moreover, it is compatible with pseudo-label methods: adding prediction filtering to existing SWSSS algorithms further improves segmentation performance.",一个奇怪的技巧，可以改善您的半弱监督语义细分模型。,半弱监督的语义分割（SWSSS）旨在训练模型，以基于少数带有像素级标签的图像以及仅带有图像级标签的更多图像来识别图像中的对象。大多数现有的SWSS算法从图像分类器中提取像素级伪标签 - 这是一个非常困难的任务，因此需要复杂的体系结构和对完全监督验证集进行大量的超参数调整。我们提出了一种称为预测过滤的方法，该方法不是提取伪标签，而只是将分类器用作分类器：它忽略了分类器不存在的类中的任何分割预测。将此简单的后处理方法添加到基准中，与先前的SWSSS算法相比，结果具有竞争力或更好的结果。此外，它与伪标签方法兼容：将预测过滤添加到现有的SWSSS算法中进一步改善了细分性能。,https://arxiv.org/abs/2205.01233,IJCAI,True,False,False,False
1815,Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition.,"Cheng Luo, Siyang Song, Weicheng Xie, Linlin Shen, Hatice Gunes","The activations of Facial Action Units (AUs) mutually influence one another. While the relationship between a pair of AUs can be complex and unique, existing approaches fail to specifically and explicitly represent such cues for each pair of AUs in each facial display. This paper proposes an AU relationship modelling approach that deep learns a unique graph to explicitly describe the relationship between each pair of AUs of the target facial display. Our approach first encodes each AU's activation status and its association with other AUs into a node feature. Then, it learns a pair of multi-dimensional edge features to describe multiple task-specific relationship cues between each pair of AUs. During both node and edge feature learning, our approach also considers the influence of the unique facial display on AUs' relationship by taking the full face representation as an input. Experimental results on BP4D and DISFA datasets show that both node and edge feature learning modules provide large performance improvements for CNN and transformer-based backbones, with our best systems achieving the state-of-the-art AU recognition results. Our approach not only has a strong capability in modelling relationship cues for AU recognition but also can be easily incorporated into various backbones. Our PyTorch code is made available.",学习基于面部动作单元识别的多维边缘特征AU关系图。,面部动作单位（AUS）的激活相互影响。尽管一对AU之间的关系可能是复杂且独特的，但现有方法无法具体而明确地代表每个面部显示中每对AUS的此类提示。本文提出了一种AU关系建模方法，该方法深入了解独特的图表，以明确描述目标面部显示的每对AU之间的关系。我们的方法首先将每个AU的激活状态及其与其他AU的关联编码为节点功能。然后，它学习了一对多维边缘功能，以描述每对AUS之间的多个特定于任务的关系线索。在节点和边缘功能学习期间，我们的方法还考虑了独特的面部展示对AUS关系的影响，通过将完整的面部表示作为输入。BP4D和DISFA数据集的实验结果表明，节点和边缘特征学习模块都为CNN和基于变压器的骨架提供了巨大的性能改进，我们的最佳系统可实现最先进的AU识别结果。我们的方法不仅具有强大的AU识别建模关系线索的能力，而且可以轻松地将其纳入各种骨架中。我们的Pytorch代码可用。,https://arxiv.org/abs/2205.01782,IJCAI,True,False,False,False
1816,SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels.,"Yangdi Lu, Wenbo He","Deep neural networks are prone to overfitting noisy labels, resulting in poor generalization performance. To overcome this problem, we present a simple and effective method self-ensemble label correction (SELC) to progressively correct noisy labels and refine the model. We look deeper into the memorization behavior in training with noisy labels and observe that the network outputs are reliable in the early stage. To retain this reliable knowledge, SELC uses ensemble predictions formed by an exponential moving average of network outputs to update the original noisy labels. We show that training with SELC refines the model by gradually reducing supervision from noisy labels and increasing supervision from ensemble predictions. Despite its simplicity, compared with many state-of-the-art methods, SELC obtains more promising and stable results in the presence of class-conditional, instance-dependent, and real-world label noise. The code is available at https://github.com/MacLLL/SELC.",SELC：自我填充标签校正可改善嘈杂的标签的学习。,深度神经网络容易适合嘈杂的标签，从而导致概括性能差。为了克服这个问题，我们提出了一个简单有效的方法自我安装标签校正（SELC），以逐步纠正嘈杂的标签并完善模型。我们更深入地研究了带有嘈杂标签的训练中的记忆行为，并观察到网络输出在早期阶段是可靠的。为了保留这种可靠的知识，SELC使用网络输出的指数移动平均值形成的集合预测来更新原始噪声标签。我们表明，使用SELC的培训通过逐渐减少嘈杂标签的监督并增加整体预测的监督来完善模型。尽管它很简单，但与许多最先进的方法相比，SELC在存在类别条件，依赖实例和现实世界标签噪声的情况下获得了更有希望和稳定的结果。该代码可在https://github.com/maclll/selc上找到。,https://arxiv.org/abs/2205.01156,IJCAI,True,False,False,False
1817,The Limits of Morality in Strategic Games.,"Rui Cao, Pavel Naumov","A coalition is blameable for an outcome if the coalition had a strategy to prevent it. It has been previously suggested that the cost of prevention, or the cost of sacrifice, can be used to measure the degree of blameworthiness. The paper adopts this approach and proposes a modal logical system for reasoning about the degree of blameworthiness. The main technical result is a completeness theorem for the proposed system.",战略游戏中道德的局限性。,如果联盟有防止联盟的策略，则联盟可以取得成果。以前已经提出，预防成本或牺牲成本可用于衡量责备程度。该论文采用了这种方法，并提出了一个模态逻辑系统，用于推理责备程度。主要技术结果是拟议系统的完整定理。,db/journals/corr/corr1901.html#abs-1901-08467,IJCAI,True,False,False,False
1818,Forgiving Debt in Financial Network Games.,"Panagiotis Kanellopoulos, Maria Kyropoulou, Hao Zhou","A financial system is represented by a network, where nodes correspond to banks, and directed labeled edges correspond to debt contracts between banks. Once a payment schedule has been defined, where we assume that a bank cannot refuse a payment towards one of its lenders if it has sufficient funds, the liquidity of the system is defined as the sum of total payments made in the network. Maximizing systemic liquidity is a natural objective of any financial authority, so, we study the setting where the financial authority offers bailout money to some bank(s) or forgives the debts of others in order to maximize liquidity, and examine efficient ways to achieve this. We investigate the approximation ratio provided by the greedy bailout policy compared to the optimal one, and we study the computational hardness of finding the optimal debt-removal and budget-constrained optimal bailout policy, respectively.   We also study financial systems from a game-theoretic standpoint. We observe that the removal of some incoming debt might be in the best interest of a bank, if that helps one of its borrowers remain solvent and avoid costs related to default. Assuming that a bank's well-being (i.e., utility) is aligned with the incoming payments they receive from the network, we define and analyze a game among banks who want to maximize their utility by strategically giving up some incoming payments. In addition, we extend the previous game by considering bailout payments. After formally defining the above games, we prove results about the existence and quality of pure Nash equilibria, as well as the computational complexity of finding such equilibria.",宽恕金融网络游戏中的债务。,金融体系由网络代表，该网络与银行相对应，并定向标记的边缘对应于银行之间的债务合同。一旦定义了付款时间表，我们假设银行将无法拒绝其一家贷方付款，如果该贷方有足够的资金，则该系统的流动性定义为网络中付款的总付款总和。最大化系统流动性是任何财务机构的自然目标，因此，我们研究金融机构向某些银行提供救助金钱或原谅他人债务以最大程度地提高流动性并检查有效的方法来实现这一目标的环境。我们研究了贪婪的救助政策与最佳计划相比提供的近似值，我们研究了分别查找最佳债务解释和预算约束最佳救助政策的计算硬度。我们还从游戏理论的角度研究金融系统。我们观察到，如果有助于其一名借款人保持偿付能力并避免与违约有关的成本，那么消除某些传入的债务可能符合银行的最大利益。假设银行的福祉（即公用事业）与他们从网络中收到的传入付款保持一致，我们将在希望通过策略性地放弃一些传入的付款来最大化其效用的银行中定义和分析一场游戏。此外，我们通过考虑救助付款来扩展上一款游戏。在正式定义上述游戏之后，我们证明了纯纳什平衡的存在和质量以及找到这种均衡的计算复杂性。,https://arxiv.org/abs/2202.10986,IJCAI,True,False,False,False
1819,Dynamic Graph Learning Based on Hierarchical Memory for Origin-Destination Demand Prediction.,"Ruixing Zhang, Liangzhe Han, Boyi Liu, Jiayuan Zeng, Leilei Sun","Recent years have witnessed a rapid growth of applying deep spatiotemporal methods in traffic forecasting. However, the prediction of origin-destination (OD) demands is still a challenging problem since the number of OD pairs is usually quadratic to the number of stations. In this case, most of the existing spatiotemporal methods fail to handle spatial relations on such a large scale. To address this problem, this paper provides a dynamic graph representation learning framework for OD demands prediction. In particular, a hierarchical memory updater is first proposed to maintain a time-aware representation for each node, and the representations are updated according to the most recently observed OD trips in continuous-time and multiple discrete-time ways. Second, a spatiotemporal propagation mechanism is provided to aggregate representations of neighbor nodes along a random spatiotemporal route which treats origin and destination as two different semantic entities. Last, an objective function is designed to derive the future OD demands according to the most recent node representations, and also to tackle the data sparsity problem in OD prediction. Extensive experiments have been conducted on two real-world datasets, and the experimental results demonstrate the superiority of the proposed method. The code and data are available at https://github.com/Rising0321/HMOD.",基于层次记忆的动态图学习，用于原点用途需求预测。,近年来，在交通预测中采用深层时空方法的迅速增长。但是，由于OD对数的数量通常对站的数量是二次的，因此原点用途（OD）需求的预测仍然是一个具有挑战性的问题。在这种情况下，大多数现有的时空方法都无法在如此大的规模上处理空间关系。为了解决这个问题，本文为OD需求预测提供了动态图表的学习框架。特别是，首先提出了分层内存更新程序，以维持每个节点的时间感知表示形式，并根据最近观察到的OD Trips以连续的时间和多个离散时间方式进行更新。其次，提供了时空传播机制，以沿随机时空途径汇总邻居节点的表示，该途径将原点和目的地视为两个不同的语义实体。最后，目标函数旨在根据最新的节点表示来得出未来的OD需求，并在OD预测中解决数据稀疏问题。已经在两个现实世界数据集上进行了广泛的实验，实验结果证明了该方法的优越性。代码和数据可在https://github.com/rising0321/hmod上获得。,https://arxiv.org/abs/2205.14593,IJCAI,True,False,False,False
1820,Lyra: A Benchmark for Turducken-Style Code Generation.,"Qingyuan Liang, Zeyu Sun, Qihao Zhu, Wenjie Zhang, Lian Yu, Yingfei Xiong, Lu Zhang","Recently, neural techniques have been used to generate source code automatically. While promising for declarative languages, these approaches achieve much poorer performance on datasets for imperative languages. Since a declarative language is typically embedded in an imperative language (i.e., the turducken-style programming) in real-world software development, the promising results on declarative languages can hardly lead to significant reduction of manual software development efforts. In this paper, we define a new code generation task: given a natural language comment, this task aims to generate a program in a base imperative language with an embedded declarative language. To our knowledge, this is the first turducken-style code generation task. For this task, we present Lyra: a dataset in Python with embedded SQL. This dataset contains 2,000 carefully annotated database manipulation programs from real-world projects. Each program is paired with both a Chinese comment and an English comment. In our experiment, we adopted Transformer, BERT-style, and GPT-style models as baselines. In the best setting, the generation performance of GPT-style models is better than others, where the AST exact matching accuracy is 24% and 25.5% when using Chinese and English comments, respectively. Therefore, we believe that Lyra provides a new challenge for code generation. Yet, overcoming this challenge may significantly boost the applicability of code generation techniques for real-world software development.",Lyra：Turducken式代码生成的基准。,"最近，神经技术已用于自动生成源代码。这些方法在有望获得声明语言的同时，在命令式语言的数据集上的性能差得多。由于通常将声明性语言嵌入了现实世界软件开发中的命令式语言（即Turducken式编程）中，因此声明语言的有希望的结果几乎不会导致手动软件开发工作大幅减少。在本文中，我们定义了一项新的代码生成任务：鉴于自然语言评论，此任务旨在用嵌入式声明语言以基本命令性语言生成程序。据我们所知，这是第一个Turducken风格的代码生成任务。对于此任务，我们将Lyra：Python中的数据集提出了嵌入式SQL。该数据集包含来自现实世界项目的2,000个精心注释的数据库操作程序。每个程序都与中文评论和英文评论配对。在我们的实验中，我们采用了变压器，伯特风格和GPT风格的模型作为基础。在最佳环境中，GPT风格模型的生成性能比其他模型更好，在使用中文和英语评论时，AST精确匹配的精度分别为24％和25.5％。因此，我们认为Lyra为代码生成提供了新的挑战。但是，克服这一挑战可能会大大提高代码生成技术在现实世界软件开发中的适用性。",https://arxiv.org/abs/2108.12144,IJCAI,True,False,False,False
1821,Augmenting Anchors by the Detector Itself.,"Xiaopei Wan, Shengjie Chen, Yujiu Yang, Zhenhua Guo, Fangbo Tao","It is difficult to determine the scale and aspect ratio of anchors for anchor-based object detection methods. Current state-of-the-art object detectors either determine anchor parameters according to objects' shape and scale in a dataset, or avoid this problem by utilizing anchor-free method. In this paper, we propose a gradient-free anchor augmentation method named AADI, which means Augmenting Anchors by the Detector Itself. AADI is not an anchor-free method, but it converts the scale and aspect ratio of anchors from a continuous space to a discrete space, which greatly alleviates the problem of anchors' designation. Furthermore, AADI does not add any parameters or hyper-parameters, which is beneficial for future research and downstream tasks. Extensive experiments on COCO dataset show that AADI has obvious advantages for both two-stage and single-stage methods, specifically, AADI achieves at least 2.1 AP improvements on Faster R-CNN and 1.6 AP improvements on RetinaNet, using ResNet-50 model. We hope that this simple and cost-efficient method can be widely used in object detection.",增加探测器本身的锚点。,很难确定基于锚点的对象检测方法的锚定比例和纵横比。当前的最新对象检测器要么根据数据集中的对象的形状和比例确定锚定参数，要么通过使用无锚方法来避免此问题。在本文中，我们提出了一种名为AADI的无梯度锚固方法，这意味着探测器本身增加锚点。AADI不是一种无锚方法，而是将锚定比的规模和纵横比从连续空间转换为离散空间，从而大大减轻了锚指定的问题。此外，AADI不会添加任何参数或超参数，这对未来的研究和下游任务有益。对可可数据集的广泛实验表明，AADI对于两阶段和单阶段方法具有明显的优势，具体来说，AADI使用Resnet-50模型可以至少对更快的R-CNN进行至少2.1 AP改进，并在视网膜上改善了1.6 AP。我们希望这种简单且具有成本效益的方法可以广泛用于对象检测中。,https://arxiv.org/abs/2105.14086,IJCAI,True,False,False,False
1822,Plurality Veto: A Simple Voting Rule Achieving Optimal Metric Distortion.,"Fatih Erdem Kizilkaya, David Kempe","The metric distortion framework posits that n voters and m candidates are jointly embedded in a metric space such that voters rank candidates that are closer to them higher. A voting rule's purpose is to pick a candidate with minimum total distance to the voters, given only the rankings, but not the actual distances. As a result, in the worst case, each deterministic rule picks a candidate whose total distance is at least three times larger than that of an optimal one, i.e., has distortion at least 3. A recent breakthrough result showed that achieving this bound of 3 is possible; however, the proof is non-constructive, and the voting rule itself is a complicated exhaustive search.   Our main result is an extremely simple voting rule, called Plurality Veto, which achieves the same optimal distortion of 3. Each candidate starts with a score equal to his number of first-place votes. These scores are then gradually decreased via an n-round veto process in which a candidate drops out when his score reaches zero. One after the other, voters decrement the score of their bottom choice among the standing candidates, and the last standing candidate wins. We give a one-paragraph proof that this voting rule achieves distortion 3. This rule is also immensely practical, and it only makes two queries to each voter, so it has low communication overhead.   We also generalize Plurality Veto into a class of randomized voting rules in the following way: Plurality veto is run only for k < n rounds; then, a candidate is chosen with probability proportional to his residual score. This general rule interpolates between Random Dictatorship (for k=0) and Plurality Veto (for k=n-1), and k controls the variance of the output. We show that for all k, this rule has distortion at most 3.",复数否决：实现最佳度量失真的简单投票规则。,该度量扭曲框架认为，n个选民和M候选人共同嵌入到公制空间中，因此选民对候选人的距离更高。投票规则的目的是仅鉴于排名，而不是实际距离，挑选了与选民总距离最小距离的候选人。结果，在最坏的情况下，每个确定性规则都选择了一个候选人，其总距离至少比最佳候选者大三倍，即至少有失真。最近的突破结果表明，达到3个边界可能但是，证明是非构造性的，投票规则本身是一个复杂的详尽搜索。我们的主要结果是一个非常简单的投票规则，称为多数否决权，它实现了相同的最佳失真为3。每个候选人的分数均以等于他的第一名选票数量的分数开始。然后，这些分数通过N回能的否决过程逐渐降低，在该过程中，当候选人得分达到零时，候选人会退出。一个接一个地，选民在常设候选人中降低了他们最不可能的选择，最后一位常设候选人赢得了胜利。我们给出一个单段证明，该投票规则实现了失真3.该规则也非常实用，它仅对每个选民进行两个疑问，因此它的沟通开销较低。我们还通过以下方式将多元化否决权否决为一类随机投票规则：复数否决仅针对k <n回合；然后，选择候选人的概率与他的剩余分数成正比。该一般规则在随机独裁统治（对于K = 0）和多数否决（对于K = N-1）之间，而K控制输出的方差。我们表明，对于所有k，该规则最多都会失真3。,https://arxiv.org/abs/2206.07098,IJCAI,True,False,False,False
1823,Plane Geometry Diagram Parsing.,"Ming-Liang Zhang, Fei Yin, Yi-Han Hao, Cheng-Lin Liu","Geometry diagram parsing plays a key role in geometry problem solving, wherein the primitive extraction and relation parsing remain challenging due to the complex layout and between-primitive relationship. In this paper, we propose a powerful diagram parser based on deep learning and graph reasoning. Specifically, a modified instance segmentation method is proposed to extract geometric primitives, and the graph neural network (GNN) is leveraged to realize relation parsing and primitive classification incorporating geometric features and prior knowledge. All the modules are integrated into an end-to-end model called PGDPNet to perform all the sub-tasks simultaneously. In addition, we build a new large-scale geometry diagram dataset named PGDP5K with primitive level annotations. Experiments on PGDP5K and an existing dataset IMP-Geometry3K show that our model outperforms state-of-the-art methods in four sub-tasks remarkably. Our code, dataset and appendix material are available at https://github.com/mingliangzhang2018/PGDP.",平面几何图解析。,几何图解析在几何问题解决中起关键作用，其中原始提取和关系解析由于复杂的布局和主要关系之间的关系而保持挑战。在本文中，我们提出了一个基于深度学习和图形推理的强大图表解析器。具体而言，提出了一种修改的实例分割方法来提取几何原始素，并利用图形神经网络（GNN）来实现关系解析和原始分类，并结合了几何特征和先验知识。所有模块都集成到称为PGDPNET的端到端模型中，以同时执行所有子任务。此外，我们构建了一个新的大规模几何图数据集，名为PGDP5K具有原始级别注释。在PGDP5K和现有数据集Imp-Ageometry3K上进行的实验表明，我们的模型在四个子任务中的最先进方法非常明显。我们的代码，数据集和附录材料可在https://github.com/mingliangzhang2018/pgdp上获得。,https://arxiv.org/abs/2205.09363,IJCAI,True,False,False,False
1824,Set Interdependence Transformer: Set-to-Sequence Neural Networks for Permutation Learning and Structure Prediction.,"Mateusz Jurewicz, Leon Derczynski","The task of learning to map an input set onto a permuted sequence of its elements is challenging for neural networks. Set-to-sequence problems occur in natural language processing, computer vision and structure prediction, where interactions between elements of large sets define the optimal output. Models must exhibit relational reasoning, handle varying cardinalities and manage combinatorial complexity. Previous attention-based methods require $n$ layers of their set transformations to explicitly represent $n$-th order relations. Our aim is to enhance their ability to efficiently model higher-order interactions through an additional interdependence component. We propose a novel neural set encoding method called the Set Interdependence Transformer, capable of relating the set's permutation invariant representation to its elements within sets of any cardinality. We combine it with a permutation learning module into a complete, 3-part set-to-sequence model and demonstrate its state-of-the-art performance on a number of tasks. These range from combinatorial optimization problems, through permutation learning challenges on both synthetic and established NLP datasets for sentence ordering, to a novel domain of product catalog structure prediction. Additionally, the network's ability to generalize to unseen sequence lengths is investigated and a comparative empirical analysis of the existing methods' ability to learn higher-order interactions is provided.",设置相互依存的变压器：用于置换学习和结构预测的设置到序列神经网络。,学习将输入集映射到其元素元素序列上的任务对于神经网络而言是一项挑战。设置到序列问题发生在自然语言处理，计算机视觉和结构预测中，其中大集合元素之间的相互作用定义了最佳输出。模型必须表现出关系推理，处理不同的基础性并管理组合复杂性。以前的基于注意力的方法需要$ n $层的设定转换，以明确表示$ n $ th订单关系。我们的目的是增强他们通过附加相互依赖组件有效地对高阶相互作用进行有效建模的能力。我们提出了一种新型的神经集编码方法，称为“集合相互依赖变压器”，能够将集合的置换不变表示与其在任何基数集合中的元素联系起来。我们将其与置换学习模块结合到一个完整的三部分设定模型中，并在许多任务上演示其最先进的性能。这些范围从组合优化问题，到在合成和已建立的NLP数据集上的置换学习挑战到句子排序的挑战，到产品目录结构预测的新颖领域。此外，研究了网络概括到看不见的序列长度的能力，并提供了现有方法学习高阶相互作用能力的比较经验分析。,https://arxiv.org/abs/2206.03720,IJCAI,True,False,False,False
1825,Imperceptible Backdoor Attack: From Input Space to Feature Representation.,"Nan Zhong, Zhenxing Qian, Xinpeng Zhang","Backdoor attacks are rapidly emerging threats to deep neural networks (DNNs). In the backdoor attack scenario, attackers usually implant the backdoor into the target model by manipulating the training dataset or training process. Then, the compromised model behaves normally for benign input yet makes mistakes when the pre-defined trigger appears. In this paper, we analyze the drawbacks of existing attack approaches and propose a novel imperceptible backdoor attack. We treat the trigger pattern as a special kind of noise following a multinomial distribution. A U-net-based network is employed to generate concrete parameters of multinomial distribution for each benign input. This elaborated trigger ensures that our approach is invisible to both humans and statistical detection. Besides the design of the trigger, we also consider the robustness of our approach against model diagnose-based defences. We force the feature representation of malicious input stamped with the trigger to be entangled with the benign one. We demonstrate the effectiveness and robustness against multiple state-of-the-art defences through extensive datasets and networks. Our trigger only modifies less than 1\% pixels of a benign image while the modification magnitude is 1. Our source code is available at https://github.com/Ekko-zn/IJCAI2022-Backdoor.",不可察觉的后门攻击：从输入空间到特征表示。,后门攻击是对深度神经网络（DNNS）的迅速发展的威胁。在后门攻击方案中，攻击者通常通过操纵训练数据集或训练过程将后门植入目标模型。然后，被妥协的模型通常对良性输入行为，但是当出现预定义的触发器时，造成错误。在本文中，我们分析了现有攻击方法的缺点，并提出了一种新颖的不可察觉的后门攻击。我们将触发模式视为多项式分布后的一种特殊的噪声。使用基于U-NET的网络来生成每个良性输入的多项式分布的混凝土参数。这种详细的触发因素确保我们的方法对人类和统计检测都是看不见的。除了设计触发器外，我们还考虑了我们的方法对基于模型诊断的防御的鲁棒性。我们强制强制用触发器盖章的恶意输入的特征表示，以纠缠于良性。我们通过广泛的数据集和网络证明了针对多个最新防御的有效性和鲁棒性。我们的触发器仅修改良性图像的1 \％像素，而修改幅度为1。我们的源代码可在https://github.com/ekko-zn/ijcai20222222-backdoor上找到。,https://arxiv.org/abs/2205.03190,IJCAI,True,False,False,False
1826,How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?,"Alvin Chan, Yew-Soon Ong, Clement Tan","Model robustness is vital for the reliable deployment of machine learning models in real-world applications. Recent studies have shown that data augmentation can result in model over-relying on features in the low-frequency domain, sacrificing performance against low-frequency corruptions, highlighting a connection between frequency and robustness. Here, we take one step further to more directly study the frequency bias of a model through the lens of its Jacobians and its implication to model robustness. To achieve this, we propose Jacobian frequency regularization for models' Jacobians to have a larger ratio of low-frequency components. Through experiments on four image datasets, we show that biasing classifiers towards low (high)-frequency components can bring performance gain against high (low)-frequency corruption and adversarial perturbation, albeit with a tradeoff in performance for low (high)-frequency corruption. Our approach elucidates a more direct connection between the frequency bias and robustness of deep learning models.",频率偏见如何影响神经图像分类器的鲁棒性，以防止常见的腐败和对抗性扰动？,模型鲁棒性对于在现实世界应用中可靠的机器学习模型的可靠部署至关重要。最近的研究表明，数据增强可以导致模型过度依赖低频域中的特征，从而牺牲了低频腐败的性能，从而突出了频率与稳健性之间的联系。在这里，我们迈出了进一步的一步，以更直接地研究模型的jacobians镜头及其对模型鲁棒性的影响。为了实现这一目标，我们建议模型的Jacobians的Jacobian频率正规化具有较大的低频组件比例。通过在四个图像数据集上的实验，我们表明分类器偏向低（高） - 频率组件可以为高（低） - 频率腐败和对抗性扰动带来绩效增益，尽管在低（高）的性能方面取决于低（高） - 频率 - 频率 - 频率 - 费用 - 频率损坏。我们的方法阐明了深度学习模型的频率偏差和鲁棒性之间的更直接联系。,https://arxiv.org/abs/2205.04533,IJCAI,True,False,False,False
1827,Learning degradation Uncertainty for unsupervised real-world image super-resolution,"l  q, ning, j, tang, f, wu, w, dong",nan,无监督的现实世界图像超级分辨率的学习降解不确定性, ,https://see.xidian.edu.cn/faculty/wsdong/,IJCAI,False,True,False,False
1828,A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model.,"Xin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu Wei, Houfeng Wang","Synthetic data construction of Grammatical Error Correction (GEC) for non-English languages relies heavily on human-designed and language-specific rules, which produce limited error-corrected patterns. In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pretrained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language. Our approach creates diverse parallel GEC data without any language-specific operations by taking the non-autoregressive translation generated by PXLM and the gold translation as error-corrected sentence pairs. Then, we reuse PXLM to initialize the GEC model and pretrain it with the synthetic data generated by itself, which yields further improvement. We evaluate our approach on three public benchmarks of GEC in different languages. It achieves the state-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains competitive performance on Falko-Merlin (German) and RULEC-GEC (Russian). Further analysis demonstrates that our data construction method is complementary to rule-based approaches.",通过预训练的跨语性语言模型进行多语言语法误差校正的统一策略。,非英语语言的语法误差校正（GEC）的合成数据构建在很大程度上依赖于人类设计的和语言特定的规则，这些规则产生了有限的错误校正模式。在本文中，我们为多语言GEC提出了一种通用和语言独立的策略，该策略可以有效地训练GEC系统，以便使用仅有两个易于访问的新的非英语语言来培训GEC系统：1）审核的跨语性语言模型（PXLM）和2）英语和语言之间的并行翻译数据。我们的方法通过采用PXLM生成的非自动性翻译和Gold Translation作为错误校正的句子对来创建不同的平行GEC数据，而无需任何特定语言的操作。然后，我们重复使用PXLM以初始化GEC模型并用自身生成的合成数据预处理，从而进一步改进。我们用不同语言的三个公共基准评估了我们的方法。它在NLPCC 2018 Task 2数据集（中文）上取得了最新的结果，并在Falko-Merlin（德语）和Rulec-Gec（俄罗斯）上获得了竞争性能。进一步的分析表明，我们的数据构建方法与基于规则的方法互补。,https://arxiv.org/abs/2201.10707,IJCAI,True,False,False,False
1829,Leveraging Class Abstraction for Commonsense Reinforcement Learning via Residual Policy Gradient Methods.,"Niklas Höpner, Ilaria Tiddi, Herke van Hoof","Enabling reinforcement learning (RL) agents to leverage a knowledge base while learning from experience promises to advance RL in knowledge intensive domains. However, it has proven difficult to leverage knowledge that is not manually tailored to the environment. We propose to use the subclass relationships present in open-source knowledge graphs to abstract away from specific objects. We develop a residual policy gradient method that is able to integrate knowledge across different abstraction levels in the class hierarchy. Our method results in improved sample efficiency and generalisation to unseen objects in commonsense games, but we also investigate failure modes, such as excessive noise in the extracted class knowledge or environments with little class structure.",通过剩余策略梯度方法利用类抽象来进行常识性增强学习。,使强化学习（RL）代理能够利用知识库，同时从经验中学习有望提高知识密集型领域的RL。但是，事实证明，它很难利用并非为环境而定制的知识。我们建议使用开源知识图中存在的子类关系来抽象远离特定对象。我们开发了一种剩余的策略梯度方法，该方法能够整合课堂层次结构中不同抽象级别的知识。我们的方法可提高样本效率和概括为常识游戏中看不见的对象，但我们还研究了故障模式，例如在提取的类知识中过度噪音或几乎没有类结构的环境。,https://arxiv.org/abs/2201.12126,IJCAI,True,False,False,False
1830,Unsupervised Context Aware Sentence Representation Pretraining for Multi-lingual Dense Retrieval.,"Ning Wu, Yaobo Liang, Houxing Ren, Linjun Shou, Nan Duan, Ming Gong, Daxin Jiang","Recent research demonstrates the effectiveness of using pretrained language models (PLM) to improve dense retrieval and multilingual dense retrieval. In this work, we present a simple but effective monolingual pretraining task called contrastive context prediction~(CCP) to learn sentence representation by modeling sentence level contextual relation. By pushing the embedding of sentences in a local context closer and pushing random negative samples away, different languages could form isomorphic structure, then sentence pairs in two different languages will be automatically aligned. Our experiments show that model collapse and information leakage are very easy to happen during contrastive training of language model, but language-specific memory bank and asymmetric batch normalization operation play an essential role in preventing collapsing and information leakage, respectively. Besides, a post-processing for sentence embedding is also very effective to achieve better retrieval performance. On the multilingual sentence retrieval task Tatoeba, our model achieves new SOTA results among methods without using bilingual data. Our model also shows larger gain on Tatoeba when transferring between non-English pairs. On two multi-lingual query-passage retrieval tasks, XOR Retrieve and Mr.TYDI, our model even achieves two SOTA results in both zero-shot and supervised setting among all pretraining models using bilingual data.",无监督的上下文意识到的句子表示，用于多语言密集检索。,最近的研究表明，使用预审前的语言模型（PLM）来改善密集检索和多语言密集检索。在这项工作中，我们提出了一个简单但有效的单语言预审前任务，称为对比性上下文预测〜（CCP），通过对句子级别的上下文关系进行建模来学习句子表示。通过将句子的嵌入在本地上下文中的嵌入并将随机的负面样本推开，不同的语言可以形成同构结构，然后将自动对齐两种不同语言的句子对。我们的实验表明，模型崩溃和信息泄漏在语言模型的对比培训期间很容易发生，但是语言特定的记忆库和不对称批处理正常化操作分别在防止崩溃和信息泄漏方面起着至关重要的作用。此外，嵌入句子的后处理对于获得更好的检索性能也非常有效。在多语言句子检索任务tatoeba上，我们的模型在不使用双语数据的情况下在方法中实现了新的SOTA结果。我们的模型在非英语对之间转移时还显示了tatoeba的增益。在两个多语性查询 - 填充检索任务（Xor检索和Tydi先生）中，我们的模型甚至可以实现两个SOTA，从而在使用双语数据的所有预训练模型中均零射击和监督设置。,https://arxiv.org/abs/2206.03281,IJCAI,True,False,False,False
1831,FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis.,"Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, Zhou Zhao","Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hindered their applications to speech synthesis. This paper proposes FastDiff, a fast conditional diffusion model for high-quality speech synthesis. FastDiff employs a stack of time-aware location-variable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. A noise schedule predictor is also adopted to reduce the sampling steps without sacrificing the generation quality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer, FastDiff-TTS, which generates high-fidelity speech waveforms without any intermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff demonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech samples. Also, FastDiff enables a sampling speed of 58x faster than real-time on a V100 GPU, making diffusion models practically applicable to speech synthesis deployment for the first time. We further show that FastDiff generalized well to the mel-spectrogram inversion of unseen speakers, and FastDiff-TTS outperformed other competing methods in end-to-end text-to-speech synthesis. Audio samples are available at \url{https://FastDiff.github.io/}.",FastDiff：高质量语音合成的快速条件扩散模型。,降级扩散概率模型（DDPM）最近在许多生成任务中都取得了领先的性能。但是，继承的迭代抽样过程成本阻碍了他们对语音综合的应用。本文提出了FastDiff，这是一种用于高质量语音合成的快速条件扩散模型。FastDiff采用了一系列具有时间感知的位置变化的卷积，以有效地模拟具有适应性条件的长期时间依赖性。还采用了噪声时间表预测器来减少采样步骤而不牺牲发电质量。基于FastDiff，我们设计了一个端到端文本到语音合成器FastDiff-TTS，该合成器FastDiff-TTS生成没有任何中间功能的高保真语音波形（例如MEL-SPECTROGRAM）。我们对FastDiff的评估证明了具有更高质量（MOS 4.28）语音样本的最新结果。此外，FastDiff在V100 GPU上的采样速度比实时速度快58倍，这使得扩散模型实际上是首次适用于语音合成部署。我们进一步表明，FastDiff概括地概括了未见扬声器的Mel-Spectrogragron反演，而FastDiff-TTS在端到端文本到语音合成中的其他竞争方法优于其他竞争方法。音频样本可在\ url {https://fastdiff.github.io/}上获得。,https://arxiv.org/abs/2204.09934,IJCAI,True,False,False,False
1832,None Class Ranking Loss for Document-Level Relation Extraction.,"Yang Zhou, Wee Sun Lee","Document-level relation extraction (RE) aims at extracting relations among entities expressed across multiple sentences, which can be viewed as a multi-label classification problem. In a typical document, most entity pairs do not express any pre-defined relation and are labeled as ""none"" or ""no relation"". For good document-level RE performance, it is crucial to distinguish such \textit{none} class instances (entity pairs) from those of pre-defined classes (relations). However, most existing methods only estimate the probability of pre-defined relations independently without considering the probability of ""no relation"". This ignores the context of entity pairs and the label correlations between the none class and pre-defined classes, leading to sub-optimal predictions. To address this problem, we propose a new multi-label loss that encourages large \textit{margins} of label confidence scores between each pre-defined class and the none class, which enables captured label correlations and context-dependent thresholding for label prediction. To gain further robustness against positive-negative imbalance and mislabeled data that could appear in real-world RE datasets, we propose a margin regularization and a margin shifting technique. Experimental results demonstrate that our method significantly outperforms existing multi-label losses for document-level RE and works well in other multi-label tasks such as emotion classification when none class instances are available for training.",文档级关系提取的无类排名损失。,文档级别的关系提取（RE）旨在提取跨多个句子表达的实体之间的关系，可以将其视为多标签分类问题。在典型的文档中，大多数实体对不表示任何预定义的关系，并将其标记为“无”或“无关系”。对于良好的文档级别的性能，将这种\ textit {none}类实例（实体对）与预定义类（关系）（关系）区分开是至关重要的。但是，大多数现有方法仅在不考虑“无关系”的概率的情况下独立估计预定关系的概率。这忽略了实体对的上下文以及无类和预定义类之间的标签相关性，从而导致了次优的预测。为了解决这个问题，我们提出了一种新的多标签损失，鼓励每个预定义的类和无类之间的标签置信度得分的大\ textit {margins}，该标签的标签相关性和上下文依赖于上下文的标签阈值进行标签预测。为了取得进一步的鲁棒性，以抵抗现实数据集中可能出现的积极阴性不平衡和标记错误的数据，我们提出了边缘正则化和边缘变化技术。实验结果表明，我们的方法显着优于文档级RE的现有多标签损失，并且在其他多标签任务（如情绪分类）中很好地运行时，当没有类实例可用于培训时。,https://arxiv.org/abs/2205.00476,IJCAI,True,False,False,False
1833,Automatic Noisy Label Correction for Fine-Grained Entity Typing.,"Weiran Pan, Wei Wei, Feida Zhu","Fine-grained entity typing (FET) aims to assign proper semantic types to entity mentions according to their context, which is a fundamental task in various entity-leveraging applications. Current FET systems usually establish on large-scale weakly-supervised/distantly annotation data, which may contain abundant noise and thus severely hinder the performance of the FET task. Although previous studies have made great success in automatically identifying the noisy labels in FET, they usually rely on some auxiliary resources which may be unavailable in real-world applications (e.g. pre-defined hierarchical type structures, human-annotated subsets). In this paper, we propose a novel approach to automatically correct noisy labels for FET without external resources. Specifically, it first identifies the potentially noisy labels by estimating the posterior probability of a label being positive or negative according to the logits output by the model, and then relabel candidate noisy labels by training a robust model over the remaining clean labels. Experiments on two popular benchmarks prove the effectiveness of our method. Our source code can be obtained from \url{https://github.com/CCIIPLab/DenoiseFET}.",对细粒实体键入的自动嘈杂标签校正。,细粒度实体键入（FET）旨在根据实体的上下文提及实体提及的适当的语义类型，这是各种实体式应用应用程序的基本任务。当前的FET系统通常在大规模弱监督/远的注释数据上建立，这可能包含丰富的噪声，因此严重阻碍了FET任务的性能。尽管以前的研究在自动识别FET中的嘈杂标签方面取得了巨大的成功，但它们通常依赖于一些在现实世界应用中无法获得的辅助资源（例如，预定义的层次类型结构，人类宣传的子集）。在本文中，我们提出了一种新颖的方法，可以自动校正没有外部资源的FET的嘈杂标签。具体而言，它首先通过根据模型的逻辑输出来估算标签的后验概率，然后通过在剩余的干净标签上训练可靠的模型来估算潜在的嘈杂标签。对两个流行基准测试的实验证明了我们方法的有效性。我们的源代码可以从\ url {https://github.com/cciiplab/denoisefet}获得。,https://arxiv.org/abs/2205.03011,IJCAI,True,False,False,False
1834,"Control Globally, Understand Locally: A Global-to-Local Hierarchical Graph Network for Emotional Support Conversation.","Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Yajing Sun, Yunpeng Li","Emotional support conversation aims at reducing the emotional distress of the help-seeker, which is a new and challenging task. It requires the system to explore the cause of help-seeker's emotional distress and understand their psychological intention to provide supportive responses. However, existing methods mainly focus on the sequential contextual information, ignoring the hierarchical relationships with the global cause and local psychological intention behind conversations, thus leads to a weak ability of emotional support. In this paper, we propose a Global-to-Local Hierarchical Graph Network to capture the multi-source information (global cause, local intentions and dialog history) and model hierarchical relationships between them, which consists of a multi-source encoder, a hierarchical graph reasoner, and a global-guide decoder. Furthermore, a novel training objective is designed to monitor semantic information of the global cause. Experimental results on the emotional support conversation dataset, ESConv, confirm that the proposed GLHG has achieved the state-of-the-art performance on the automatic and human evaluations.",全球控制，在本地理解：一种用于情感支持对话的全球到本地分层图网络。,情感支持对话旨在减少寻求帮助者的情绪困扰，这是一项新的挑战。它要求系统探索寻求帮助者的情绪困扰的原因，并了解他们提供支持反应的心理意图。但是，现有方法主要集中于顺序的上下文信息，忽略了与全球原因和对话背后的局部心理意图的等级关系，从而导致情绪支持的弱能力。在本文中，我们提出了一个全局到本地的层次图网络，以捕获多源信息（全球原因，本地意图和对话记录）和模型它们之间的层次结构关系，该关系由多源编码器，层次结构组成图形推理器和全球指南解码器。此外，一个新颖的培训目标旨在监视全球原因的语义信息。关于情绪支持对话数据集Esconv的实验结果证实，拟议的GLHG已在自动和人类评估方面取得了最新的表现。,https://arxiv.org/abs/2204.12749,IJCAI,True,False,False,False
1835,What is Right for Me is Not Yet Right for You: A Dataset for Grounding Relative Directions via Multi-Task Learning.,"Jae Hee Lee, Matthias Kerzel, Kyra Ahrens, Cornelius Weber, Stefan Wermter","Understanding spatial relations is essential for intelligent agents to act and communicate in the physical world. Relative directions are spatial relations that describe the relative positions of target objects with regard to the intrinsic orientation of reference objects. Grounding relative directions is more difficult than grounding absolute directions because it not only requires a model to detect objects in the image and to identify spatial relation based on this information, but it also needs to recognize the orientation of objects and integrate this information into the reasoning process. We investigate the challenging problem of grounding relative directions with end-to-end neural networks. To this end, we provide GRiD-3D, a novel dataset that features relative directions and complements existing visual question answering (VQA) datasets, such as CLEVR, that involve only absolute directions. We also provide baselines for the dataset with two established end-to-end VQA models. Experimental evaluations show that answering questions on relative directions is feasible when questions in the dataset simulate the necessary subtasks for grounding relative directions. We discover that those subtasks are learned in an order that reflects the steps of an intuitive pipeline for processing relative directions.",适合我的是不适合您的：用于通过多任务学习接地相对方向的数据集。,理解空间关系对于智能代理人在物理世界中采取行动和交流至关重要。相对方向是空间关系，描述了目标对象在参考对象的内在方向方面的相对位置。接地相对方向比接地绝对方向更加困难，因为它不仅需要模型来检测图像中的对象并基于此信息识别空间关系，而且还需要识别对象的方向并将此信息集成到推理中过程。我们研究了端到端神经网络基础相对方向的挑战性问题。为此，我们提供了Grid-3D，这是一个新颖的数据集，具有相对方向并补充了现有的视觉问题答案（VQA）数据集（例如CLEVR），仅涉及绝对方向。我们还为数据集提供了两个已建立的端到端VQA模型的基准。实验评估表明，当数据集中的问题模拟接地相对方向的必要子任务时，在相对方向上回答问题是可行的。我们发现这些子任务是按顺序学习的，该顺序反映了处理相对方向的直观管道的步骤。,https://arxiv.org/abs/2205.02671,IJCAI,True,False,False,False
1836,Search-Based Testing of Reinforcement Learning.,"Martin Tappler, Filip Cano Córdoba, Bernhard K. Aichernig, Bettina Könighofer","Evaluation of deep reinforcement learning (RL) is inherently challenging. Especially the opaqueness of learned policies and the stochastic nature of both agents and environments make testing the behavior of deep RL agents difficult. We present a search-based testing framework that enables a wide range of novel analysis capabilities for evaluating the safety and performance of deep RL agents. For safety testing, our framework utilizes a search algorithm that searches for a reference trace that solves the RL task. The backtracking states of the search, called boundary states, pose safety-critical situations. We create safety test-suites that evaluate how well the RL agent escapes safety-critical situations near these boundary states. For robust performance testing, we create a diverse set of traces via fuzz testing. These fuzz traces are used to bring the agent into a wide variety of potentially unknown states from which the average performance of the agent is compared to the average performance of the fuzz traces. We apply our search-based testing approach on RL for Nintendo's Super Mario Bros.",基于搜索的强化学习测试。,深入增强学习（RL）的评估本质上是具有挑战性的。尤其是学术政策的不透明性以及代理和环境的随机性使得对深度RL代理的行为进行了困难。我们提出了一个基于搜索的测试框架，该框架可实现广泛的新型分析功能，以评估深RL代理的安全性和性能。对于安全测试，我们的框架使用了一种搜索算法，该算法搜索了解决RL任务的参考跟踪。搜索的回溯状态称为边界状态，构成关键安全情况。我们创建安全测试套件，以评估RL代理在这些边界状态附近的安全关键情况下的逃脱状况。对于强大的性能测试，我们通过模糊测试创建了一套多样的痕迹。这些模糊轨迹用于将代理带入各种潜在的未知状态，将代理的平均性能与绒毛痕迹的平均性能进行比较。我们将基于搜索的测试方法应用于Nintendo的Super Mario Bros。,https://arxiv.org/abs/2205.04887,IJCAI,True,False,True,False
1837,Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion.,"Zhenwei Tang, Shichao Pei, Zhao Zhang, Yongchun Zhu, Fuzhen Zhuang, Robert Hoehndorf, Xiangliang Zhang","Most real-world knowledge graphs (KG) are far from complete and comprehensive. This problem has motivated efforts in predicting the most plausible missing facts to complete a given KG, i.e., knowledge graph completion (KGC). However, existing KGC methods suffer from two main issues, 1) the false negative issue, i.e., the candidates for sampling negative training instances include potential true facts; and 2) the data sparsity issue, i.e., true facts account for only a tiny part of all possible facts. To this end, we propose positive-unlabeled learning with adversarial data augmentation (PUDA) for KGC. In particular, PUDA tailors positive-unlabeled risk estimator for the KGC task to deal with the false negative issue. Furthermore, to address the data sparsity issue, PUDA achieves a data augmentation strategy by unifying adversarial training and positive-unlabeled learning under the positive-unlabeled minimax game. Extensive experimental results demonstrate its effectiveness and compatibility.",带有积极的未标记学习，具有对抗性数据增强，以完成知识图完成。,大多数真实的知识图（kg）远非完整和全面。这个问题激发了预测最合理的缺失事实以完成给定的kg，即知识图完成（KGC）。但是，现有的kgc方法遇到了两个主要问题，1）虚假的负面问题，即，对负面培训实例进行抽样的候选人包括潜在的真实事实；2）数据稀疏问题，即真实事实仅解释了所有可能事实的一小部分。为此，我们提出了针对KGC的对抗数据增强（PUDA）的积极未标记的学习。特别是，PUDA针对KGC任务量身定制了正标记的风险估计器，以解决虚假的负面问题。此外，为了解决数据稀疏问题，PUDA通过在积极的无标记的Minimax游戏中统一对抗性培训和积极的未标记学习来实现数据增强策略。广泛的实验结果证明了其有效性和兼容性。,https://arxiv.org/abs/2205.00904,IJCAI,True,False,False,False
1838,A Simple yet Effective Method for Graph Classification.,"Junran Wu, Shangzhe Li, Jianhao Li, Yicheng Pan, Ke Xu","In deep neural networks, better results can often be obtained by increasing the complexity of previously developed basic models. However, it is unclear whether there is a way to boost performance by decreasing the complexity of such models. Intuitively, given a problem, a simpler data structure comes with a simpler algorithm. Here, we investigate the feasibility of improving graph classification performance while simplifying the learning process. Inspired by structural entropy on graphs, we transform the data sample from graphs to coding trees, which is a simpler but essential structure for graph data. Furthermore, we propose a novel message passing scheme, termed hierarchical reporting, in which features are transferred from leaf nodes to root nodes by following the hierarchical structure of coding trees. We then present a tree kernel and a convolutional network to implement our scheme for graph classification. With the designed message passing scheme, the tree kernel and convolutional network have a lower runtime complexity of $O(n)$ than Weisfeiler-Lehman subtree kernel and other graph neural networks of at least $O(hm)$. We empirically validate our methods with several graph classification benchmarks and demonstrate that they achieve better performance and lower computational consumption than competing approaches.",一种简单但有效的图形分类方法。,在深层神经网络中，通常可以通过增加先前开发的基本模型的复杂性来获得更好的结果。但是，目前尚不清楚是否有一种方法可以通过降低此类模型的复杂性来提高性能。直观地，考虑到一个问题，更简单的数据结构带有更简单的算法。在这里，我们研究了改善图形分类性能的可行性，同时简化学习过程。受图形结构熵的启发，我们将数据样本从图形转换为编码树，这是图形数据的更简单但必不可少的结构。此外，我们提出了一个新的消息传递方案，称为层次报告，其中特征通过遵循编码树的层次结构从叶子节点传递到根节点。然后，我们提出一个树内核和一个卷积网络，以实现我们的图形分类方案。使用设计的消息传递方案，Tree内核和卷积网络的运行时复杂性较低，$ O（n）$比Weisfeiler-Lehman子树子树和其他至少$ O（HM）$的其他图神经网络。我们通过几种图形分类基准来验证我们的方法，并证明它们比竞争方法更好地实现了更好的性能和更低的计算消耗。,https://arxiv.org/abs/2206.02404,IJCAI,True,False,False,False
1839,Shielding Federated Learning: Robust Aggregation with Adaptive Client Selection.,"Wei Wan, Shengshan Hu, Jianrong Lu, Leo Yu Zhang, Hai Jin, Yuanyuan He","Federated learning (FL) enables multiple clients to collaboratively train an accurate global model while protecting clients' data privacy. However, FL is susceptible to Byzantine attacks from malicious participants. Although the problem has gained significant attention, existing defenses have several flaws: the server irrationally chooses malicious clients for aggregation even after they have been detected in previous rounds; the defenses perform ineffectively against sybil attacks or in the heterogeneous data setting.   To overcome these issues, we propose MAB-RFL, a new method for robust aggregation in FL. By modelling the client selection as an extended multi-armed bandit (MAB) problem, we propose an adaptive client selection strategy to choose honest clients that are more likely to contribute high-quality updates. We then propose two approaches to identify malicious updates from sybil and non-sybil attacks, based on which rewards for each client selection decision can be accurately evaluated to discourage malicious behaviors. MAB-RFL achieves a satisfying balance between exploration and exploitation on the potential benign clients. Extensive experimental results show that MAB-RFL outperforms existing defenses in three attack scenarios under different percentages of attackers.",屏蔽联合学习：适合自适应客户选择的强大聚合。,联合学习（FL）使多个客户能够协作培训准确的全球模型，同时保护客户的数据隐私。但是，FL容易受到恶意参与者的拜占庭攻击。尽管问题引起了很大的关注，但现有的防御措施有几个缺陷：服务器非理性地选择了恶意客户端进行聚合，即使在前一轮中被检测到。防御能力对SYBIL攻击或在异质数据设置中无效。为了克服这些问题，我们提出了MAB-RFL，这是一种在FL中鲁棒聚集的新方法。通过将客户选择建模为扩展的多臂强盗（MAB）问题，我们提出了一种自适应客户选择策略，以选择诚实的客户，这些客户更有可能贡献高质量的更新。然后，我们提出了两种方法，以确定Sybil和非陪同攻击中的恶意更新，基于每个客户选择决策的奖励，可以准确评估以阻止恶意行为。MAB-RFL在对潜在良性客户的探索和开发之间达到了令人满意的平衡。广泛的实验结果表明，在不同百分比的攻击者中，MAB-RFL在三种攻击方案中的表现优于现有防御。,https://arxiv.org/abs/2204.13256,IJCAI,True,False,False,False
1840,Anomaly Detection by Leveraging Incomplete Anomalous Knowledge with Anomaly-Aware Bidirectional GANs.,"Bowen Tian, Qinliang Su, Jian Yin","The goal of anomaly detection is to identify anomalous samples from normal ones. In this paper, a small number of anomalies are assumed to be available at the training stage, but they are assumed to be collected only from several anomaly types, leaving the majority of anomaly types not represented in the collected anomaly dataset at all. To effectively leverage this kind of incomplete anomalous knowledge represented by the collected anomalies, we propose to learn a probability distribution that can not only model the normal samples, but also guarantee to assign low density values for the collected anomalies. To this end, an anomaly-aware generative adversarial network (GAN) is developed, which, in addition to modeling the normal samples as most GANs do, is able to explicitly avoid assigning probabilities for collected anomalous samples. Moreover, to facilitate the computation of anomaly detection criteria like reconstruction error, the proposed anomaly-aware GAN is designed to be bidirectional, attaching an encoder for the generator. Extensive experimental results demonstrate that our proposed method is able to effectively make use of the incomplete anomalous information, leading to significant performance gains compared to existing methods.",通过利用异常知识的异常知识的异常检测。,异常检测的目的是鉴定来自正常样本的异常样本。在本文中，假定少数异常在训练阶段可用，但假定它们仅是从几种异常类型中收集的，这使得在收集到的异常数据集中根本没有表示的大多数异常类型。为了有效利用收集到的异常代表的这种不完全的异常知识，我们建议学习一个概率分布，不仅可以对正常样本进行建模，而且可以保证为收集的异常分配低密度值。为此，开发了一个异常感知的生成对抗网络（GAN），除了像大多数GAN一样对正常样品进行建模外，还可以明确避免为收集的异常样品分配概率。此外，为了促进计算异常检测标准（如重建误差），提出的异常感知的gan被设计为双向，并为发电机附加编码器。广泛的实验结果表明，我们提出的方法能够有效地利用不完整的异常信息，与现有方法相比，导致了显着的性能增长。,https://arxiv.org/abs/2204.13335,IJCAI,True,False,False,False
1841,Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast.,"Boqing Zhu, Kele Xu, Changjian Wang, Zheng Qin, Tao Sun, Huaimin Wang, Yuxing Peng","We present an approach to learn voice-face representations from the talking face videos, without any identity labels. Previous works employ cross-modal instance discrimination tasks to establish the correlation of voice and face. These methods neglect the semantic content of different videos, introducing false-negative pairs as training noise. Furthermore, the positive pairs are constructed based on the natural correlation between audio clips and visual frames. However, this correlation might be weak or inaccurate in a large amount of real-world data, which leads to deviating positives into the contrastive paradigm. To address these issues, we propose the cross-modal prototype contrastive learning (CMPC), which takes advantage of contrastive methods and resists adverse effects of false negatives and deviate positives. On one hand, CMPC could learn the intra-class invariance by constructing semantic-wise positives via unsupervised clustering in different modalities. On the other hand, by comparing the similarities of cross-modal instances from that of cross-modal prototypes, we dynamically recalibrate the unlearnable instances' contribution to overall loss. Experiments show that the proposed approach outperforms state-of-the-art unsupervised methods on various voice-face association evaluation protocols. Additionally, in the low-shot supervision setting, our method also has a significant improvement compared to previous instance-wise contrastive learning.",通过跨模式原型对比的无监督的语音表面表示。,我们提出了一种方法，可以从谈话面部视频中学习语音表面表示，而没有任何身份标签。先前的作品采用跨模式实例歧视任务来建立声音和面部的相关性。这些方法忽略了不同视频的语义内容，将假阴性对作为训练噪声。此外，正对基于音频夹和视觉帧之间的自然相关性构建。但是，这种相关性可能在大量的现实数据中弱或不准确，从而导致阳性偏离对比范式。为了解决这些问题，我们提出了跨模式原型对比度学习（CMPC），该学习利用了对比方法，并抵抗了错误的负面影响并偏离阳性。一方面，CMPC可以通过以不同方式通过无监督的聚类来构建语义阳性来学习类内的不变性。另一方面，通过比较跨模式原型的跨模式实例的相似性，我们可以动态地重新校准未完成的实例对整体损失的贡献。实验表明，所提出的方法在各种语音面积关联评估方案上都优于最先进的无监督方法。此外，与以前的实例对比度学习相比，在低射击监督环境中，我们的方法还具有显着改善。,https://arxiv.org/abs/2204.14057,IJCAI,True,False,False,False
1842,Towards Joint Intent Detection and Slot Filling via Higher-order Attention.,"Dongsheng Chen, Zhiqi Huang, Xian Wu, Shen Ge, Yuexian Zou","Intent detection (ID) and Slot filling (SF) are two major tasks in spoken language understanding (SLU). Recently, attention mechanism has been shown to be effective in jointly optimizing these two tasks in an interactive manner. However, latest attention-based works concentrated only on the first-order attention design, while ignoring the exploration of higher-order attention mechanisms. In this paper, we propose a BiLinear attention block, which leverages bilinear pooling to simultaneously exploit both the contextual and channel-wise bilinear attention distributions to capture the second-order interactions between the input intent or slot features. Higher and even infinity order interactions are built by stacking numerous blocks and assigning Exponential Linear Unit (ELU) to blocks. Before the decoding stage, we introduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot information in a more effective way. Technically, instead of simply concatenating intent and slot features, we first compute two correlation matrices to weight on two features. Furthermore, we present Higher-order Attention Network for the SLU tasks. Experiments on two benchmark datasets show that our approach yields improvements compared with the state-of-the-art approach. We also provide discussion to demonstrate the effectiveness of the proposed approach.",通过更高阶段的注意力进行关节意图检测和插槽填充。,意图检测（ID）和插槽填充（SF）是口语理解（SLU）的两个主要任务。最近，注意机制已被证明可以有效地以交互式方式共同优化这两个任务。但是，最新的基于注意力的作品仅集中在一阶注意设计上，同时忽略了高阶注意机制的探索。在本文中，我们提出了一个双线性注意块，该块利用双线性池以同时利用上下文和频道双线性注意分布来捕获输入意图或插槽特征之间的二阶相互作用。通过堆叠众多块并将指数线性单元（ELU）分配给块来构建较高甚至无限订单的交互。在解码阶段之前，我们介绍了动态特征融合层，以更有效的方式隐式融合意图和插槽信息。从技术上讲，我们首先将两个相关矩阵重量计算在两个功能上，而不是简单地串联意图和插槽功能。此外，我们为SLU任务提供了高阶注意网络。两个基准数据集的实验表明，与最先进的方法相比，我们的方法可以改善。我们还提供讨论以证明拟议方法的有效性。,https://arxiv.org/abs/2109.08890,IJCAI,True,False,False,False
1843,UM4: Unified Multilingual Multiple Teacher-Student Model for Zero-Resource Neural Machine Translation.,"Jian Yang, Yuwei Yin, Shuming Ma, Dongdong Zhang, Shuangzhi Wu, Hongcheng Guo, Zhoujun Li, Furu Wei","Most translation tasks among languages belong to the zero-resource translation problem where parallel corpora are unavailable. Multilingual neural machine translation (MNMT) enables one-pass translation using shared semantic space for all languages compared to the two-pass pivot translation but often underperforms the pivot-based method. In this paper, we propose a novel method, named as Unified Multilingual Multiple teacher-student Model for NMT (UM4). Our method unifies source-teacher, target-teacher, and pivot-teacher models to guide the student model for the zero-resource translation. The source teacher and target teacher force the student to learn the direct source to target translation by the distilled knowledge on both source and target sides. The monolingual corpus is further leveraged by the pivot-teacher model to enhance the student model. Experimental results demonstrate that our model of 72 directions significantly outperforms previous methods on the WMT benchmark.",UM4：用于零资源神经机器翻译的统一多语言多语言多语言多语言教师模型。,语言之间的大多数翻译任务都属于无法使用的零资源翻译问题。与两种通用枢轴翻译相比，多语言神经机器翻译（MNMT）可以使用所有语言的共享语义空间进行一通翻译，但通常表现不佳的基于枢轴的方法。在本文中，我们提出了一种新颖的方法，称为NMT（UM4）的统一多语言多语言多种教师模型。我们的方法统一了来源教师，目标老师和枢轴教师模型，以指导零资源翻译的学生模型。来源老师和目标教师迫使学生学习直接来源，以通过蒸馏知识和目标方面的蒸馏知识进行目标翻译。枢轴教师模型进一步利用单语语料库来增强学生模型。实验结果表明，我们的72个方向模型在WMT基准测试上明显优于先前的方法。,https://arxiv.org/abs/2207.04900,IJCAI,True,False,False,False
1844,Frontiers and Exact Learning of ELI Queries under DL-Lite Ontologies.,"Maurice Funk, Jean Christoph Jung, Carsten Lutz","We study ELI queries (ELIQs) in the presence of ontologies formulated in the description logic DL-Lite. For the dialect DL-LiteH, we show that ELIQs have a frontier (set of least general generalizations) that is of polynomial size and can be computed in polynomial time. In the dialect DL-LiteF, in contrast, frontiers may be infinite. We identify a natural syntactic restriction that enables the same positive results as for DL-LiteH. We use out results on frontiers to show that ELIQs are learnable in polynomial time in the presence of a DL-LiteH / restricted DL-LiteF ontology in Angluin's framework of exact learning with only membership queries.",边界和在DL-Lite本体下的ELI查询的精确学习。,我们在描述逻辑dl-lite中提出的本体论中研究了ELI查询（ELIQ）。对于方言DL-LiteH，我们表明ELIQ具有多项式大小的边界（最不通用的概括），可以在多项式时间内计算。相反，在方言DL-Litef中，边界可能是无限的。我们确定了一种自然的句法限制，该限制能够带来与DL-LiteH相同的积极结果。我们在边界上使用结果表明，在Angluin的Angluin精确学习框架中，只有会员查询的dl-liteh /受限DL-LiteF本体在多项式时间内可以学习ELIQ。,https://arxiv.org/abs/2204.14172,IJCAI,True,False,False,False
1845,Fixing Knockout Tournaments With Seeds.,"Pasin Manurangsi, Warut Suksompong","Knockout tournaments constitute a popular format for organizing sports competitions. While prior results have shown that it is often possible to manipulate a knockout tournament by fixing the bracket, these results ignore the prevalent aspect of player seeds, which can significantly constrain the chosen bracket. We show that certain structural conditions that guarantee that a player can win a knockout tournament without seeds are no longer sufficient in light of seed constraints. On the other hand, we prove that when the pairwise match outcomes are generated randomly, all players are still likely to be knockout winners under the same probability threshold with seeds as without seeds. In addition, we investigate the complexity of deciding whether a manipulation is possible when seeds are present.",用种子固定淘汰赛。,淘汰赛构成了组织体育比赛的流行格式。虽然先前的结果表明，通常可以通过固定支架来操纵淘汰赛，但这些结果忽略了玩家种子的普遍方面，这可能会严重限制所选的支架。我们表明，某些确保玩家可以在没有种子的情况下赢得淘汰赛的结构条件不再足够。另一方面，我们证明，随机生成成对匹配结果时，所有玩家仍然可能是与没有种子的种子相同的概率阈值下淘汰赛冠军。此外，我们研究了在存在种子时确定是否可能进行操作的复杂性。,https://arxiv.org/abs/2204.11171,IJCAI,True,False,False,False
1846,Learning Label Initialization for Time-Dependent Harmonic Extension.,Amitoz Azad,"Node classification on graphs can be formulated as the Dirichlet problem on graphs where the signal is given at the labeled nodes, and the harmonic extension is done on the unlabeled nodes. This paper considers a time-dependent version of the Dirichlet problem on graphs and shows how to improve its solution by learning the proper initialization vector on the unlabeled nodes. Further, we show that the improved solution is at par with state-of-the-art methods used for node classification. Finally, we conclude this paper by discussing the importance of parameter t, pros, and future directions.",学习标签初始化，用于时间相关的谐波扩展。,图上的节点分类可以作为图表上的dirichlet问题进行表述，在该图上给出了标记的节点处的信号，并且在未标记的节点上进行了谐波扩展。本文考虑了图形上的Dirichlet问题的时间相关版本，并通过学习未标记的节点上的适当初始化向量来改善其解决方案。此外，我们表明改进的解决方案与用于节点分类的最新方法相提并论。最后，我们通过讨论参数t，专业和未来方向的重要性来结束本文。,https://arxiv.org/abs/2205.01358,IJCAI,True,False,False,False
1847,RMGN: A Regional Mask Guided Network for Parser-free Virtual Try-on.,"Chao Lin, Zhao Li, Sheng Zhou, Shichang Hu, Jialun Zhang, Linhao Luo, Jiarun Zhang, Longtao Huang, Yuan He","Virtual try-on(VTON) aims at fitting target clothes to reference person images, which is widely adopted in e-commerce.Existing VTON approaches can be narrowly categorized into Parser-Based(PB) and Parser-Free(PF) by whether relying on the parser information to mask the persons' clothes and synthesize try-on images. Although abandoning parser information has improved the applicability of PF methods, the ability of detail synthesizing has also been sacrificed. As a result, the distraction from original cloth may persistin synthesized images, especially in complicated postures and high resolution applications. To address the aforementioned issue, we propose a novel PF method named Regional Mask Guided Network(RMGN). More specifically, a regional mask is proposed to explicitly fuse the features of target clothes and reference persons so that the persisted distraction can be eliminated. A posture awareness loss and a multi-level feature extractor are further proposed to handle the complicated postures and synthesize high resolution images. Extensive experiments demonstrate that our proposed RMGN outperforms both state-of-the-art PB and PF methods.Ablation studies further verify the effectiveness ofmodules in RMGN.",RMGN：用于无解析器虚拟试验的区域蒙版指导网络。,虚拟试验（VTON）旨在将目标服装拟合到参考人员图像，这在电子商务中广泛采用。使用VTON方法可以通过是否依赖于基于Parser（PB）和Parser-Free（PF）将VTON方法狭窄地分类为在解析器的信息上，掩盖了人员的衣服并合成尝试图像。尽管放弃的解析器信息改善了PF方法的适用性，但细节合成的能力也已被牺牲。结果，原始布的分散注意力可能会持续合成图像，尤其是在复杂的姿势和高分辨率应用中。为了解决上述问题，我们提出了一种名为“区域掩码指导网络（RMGN）”的新型PF方法。更具体地说，提出了区域口罩，以明确融合目标衣服和参考人员的特征，以便可以消除持续的干扰。进一步提出了姿势意识丧失和多级特征提取器，以处理复杂的姿势并合成高分辨率图像。广泛的实验表明，我们提出的RMGN胜过最新的PB和PF方法。ABLITATION研究进一步验证了Modules在RMGN中的有效性。,https://arxiv.org/abs/2204.11258,IJCAI,True,False,False,False
1848,Bounded Memory Adversarial Bandits with Composite Anonymous Delayed Feedback.,"Zongqi Wan, Xiaoming Sun, Jialin Zhang","We study the adversarial bandit problem with composite anonymous delayed feedback. In this setting, losses of an action are split into $d$ components, spreading over consecutive rounds after the action is chosen. And in each round, the algorithm observes the aggregation of losses that come from the latest $d$ rounds. Previous works focus on oblivious adversarial setting, while we investigate the harder non-oblivious setting. We show non-oblivious setting incurs $\Omega(T)$ pseudo regret even when the loss sequence is bounded memory. However, we propose a wrapper algorithm which enjoys $o(T)$ policy regret on many adversarial bandit problems with the assumption that the loss sequence is bounded memory. Especially, for $K$-armed bandit and bandit convex optimization, we have $\mathcal{O}(T^{2/3})$ policy regret bound. We also prove a matching lower bound for $K$-armed bandit. Our lower bound works even when the loss sequence is oblivious but the delay is non-oblivious. It answers the open problem proposed in \cite{wang2021adaptive}, showing that non-oblivious delay is enough to incur $\tilde{\Omega}(T^{2/3})$ regret.",带有复合匿名延迟反馈的有界内存对抗匪。,我们研究了对抗匿名延迟反馈的对抗性匪徒问题。在这种情况下，操作的损失分为$ d $组件，在选择动作后连续一轮散布。在每回合中，该算法都观察到来自最新$ d $回合的损失的汇总。以前的作品着重于遗忘的对抗环境，同时我们研究了更艰难的非欺骗环境。我们展示的不掩饰设置会引起$ \ omega（t）$ pseudo遗憾，即使损失序列是有限的内存也是如此。但是，我们提出了一种包装算法，该算法在许多对抗性强盗问题上都享受$ o（t）$ policy遗憾，假设损失顺序是有限的记忆。特别是，对于$ k $ armed的强盗和强盗凸优化，我们有$ \ mathcal {o}（t^{2/3}）$ policy policy selford绑定。我们还证明了$ k $武装的强盗的匹配下限。即使损失序列是遗忘的，我们的下限也起作用，但延迟是非掩饰的。它回答了\ cite {wang2021Adaptive}中提出的开放问题，表明非欺骗性延迟足以产生$ \ tilde {\ omega}（\ omega}（t^{2/3}）$遗憾。,https://arxiv.org/abs/2204.12764,IJCAI,True,False,False,False
1849,Network Creation with Homophilic Agents.,"Martin Bullinger, Pascal Lenzner, Anna Melnichenko","Network Creation Games are an important framework for understanding the formation of real-world networks. These games usually assume a set of indistinguishable agents strategically buying edges at a uniform price leading to a network among them. However, in real life, agents are heterogeneous and their relationships often display a bias towards similar agents, say of the same ethnic group. This homophilic behavior on the agent level can then lead to the emergent global phenomenon of social segregation. We initiate the study of Network Creation Games with multiple types of homophilic agents and non-uniform edge cost. Specifically, we introduce and compare two models, focusing on the perception of same-type and different-type neighboring agents, respectively. Despite their different initial conditions, both our theoretical and experimental analysis show that the resulting stable networks are almost identical in the two models, indicating a robust structure of social networks under homophily. Moreover, we investigate the segregation strength of the formed networks and thereby offer new insights on understanding segregation.",与同粒子代理的网络创建。,网络创建游戏是理解现实世界网络形成的重要框架。这些游戏通常会以一套不可区分的代理商在战略上以统一的价格购买边缘，从而导致网络中的网络。但是，在现实生活中，代理人是异质的，他们的关系经常对相似的代理人表现出偏见，例如同一族裔。然后，这种代理水平上的这种同质行为可以导致社会种族隔离的新兴全球现象。我们启动具有多种类型的同粒子代理和不均匀边缘成本的网络创建游戏的研究。具体而言，我们介绍和比较了两个模型，分别着重于对相同型和不同型相邻代理的感知。尽管它们的初始条件不同，但我们的理论和实验分析都表明，在这两个模型中，所得稳定的网络几乎相同，表明同质下的社交网络结构可靠。此外，我们研究了形成的网络的隔离强度，从而为理解种族隔离提供了新的见解。,https://arxiv.org/abs/2204.13757,IJCAI,True,False,False,False
1850,"Placing Green Bridges Optimally, with Habitats Inducing Cycles.","Maike Herkenrath, Till Fluschnik, Francesco Grothe, Leon Kellerhals","Choosing the placement of wildlife crossings (i.e., green bridges) to reconnect animal species' fragmented habitats is among the 17 goals towards sustainable development by the UN. We consider the following established model: Given a graph whose vertices represent the fragmented habitat areas and whose weighted edges represent possible green bridge locations, as well as the habitable vertex set for each species, find the cheapest set of edges such that each species' habitat is connected. We study this problem from a theoretical (algorithms and complexity) and an experimental perspective, while focusing on the case where habitats induce cycles. We prove that the NP-hardness persists in this case even if the graph structure is restricted. If the habitats additionally induce faces in plane graphs however, the problem becomes efficiently solvable. In our empirical evaluation we compare this algorithm as well as ILP formulations for more general variants and an approximation algorithm with another. Our evaluation underlines that each specialization is beneficial in terms of running time, whereas the approximation provides highly competitive solutions in practice.",最佳地放置绿色桥梁，并具有诱导周期的栖息地。,选择野生动植物过境点（即绿桥）重新连接动物物种零散的栖息地是联合国可持续发展的17个目标之一。我们考虑以下既定模型：给定的图表，其顶点代表零散的栖息地区域，其加权边缘代表了可能的绿色桥梁位置以及每个物种的可居住顶点，找到了最便宜的边缘，以使每个物种的栖息地都可以已连接。我们从理论（算法和复杂性）和实验性观点研究了这个问题，同时着重于栖息地诱导周期的情况。我们证明，即使图形结构受到限制，在这种情况下，NP硬度仍然存在。但是，如果栖息地还在平面图中诱导面孔，则该问题将有效地解决。在我们的经验评估中，我们比较了该算法以及ILP公式的更通用的变体和近似算法与另一种算法。我们的评估强调，每个专业化在运行时间方面都是有益的，而近似值在实践中提供了高度竞争的解决方案。,https://arxiv.org/abs/2201.12273,IJCAI,True,False,False,False
1851,Ridgeless Regression with Random Features.,"Jian Li, Yong Liu, Yingying Zhang","Recent theoretical studies illustrated that kernel ridgeless regression can guarantee good generalization ability without an explicit regularization. In this paper, we investigate the statistical properties of ridgeless regression with random features and stochastic gradient descent. We explore the effect of factors in the stochastic gradient and random features, respectively. Specifically, random features error exhibits the double-descent curve. Motivated by the theoretical findings, we propose a tunable kernel algorithm that optimizes the spectral density of kernel during training. Our work bridges the interpolation theory and practical algorithm.",具有随机特征的无骑线回归。,最近的理论研究表明，内核无脊回归可以保证良好的概括能力而无需明确的正则化。在本文中，我们研究了具有随机特征和随机梯度下降的无乘车回归的统计特性。我们分别探讨了随机梯度和随机特征中因素的影响。具体而言，随机特征误差表现出双重曲线。在理论发现的激励下，我们提出了一种可调核算法，该算法优化了训练过程中内核的光谱密度。我们的工作桥接了插值理论和实用算法。,https://arxiv.org/abs/2205.00477,IJCAI,True,False,False,False
1852,Robustness Guarantees for Credal Bayesian Networks via Constraint Relaxation over Probabilistic Circuits.,"Hjalmar Wijk, Benjie Wang, Marta Kwiatkowska","In many domains, worst-case guarantees on the performance (e.g., prediction accuracy) of a decision function subject to distributional shifts and uncertainty about the environment are crucial. In this work we develop a method to quantify the robustness of decision functions with respect to credal Bayesian networks, formal parametric models of the environment where uncertainty is expressed through credal sets on the parameters. In particular, we address the maximum marginal probability (MARmax) problem, that is, determining the greatest probability of an event (such as misclassification) obtainable for parameters in the credal set. We develop a method to faithfully transfer the problem into a constrained optimization problem on a probabilistic circuit. By performing a simple constraint relaxation, we show how to obtain a guaranteed upper bound on MARmax in linear time in the size of the circuit. We further theoretically characterize this constraint relaxation in terms of the original Bayesian network structure, which yields insight into the tightness of the bound. We implement the method and provide experimental evidence that the upper bound is often near tight and demonstrates improved scalability compared to other methods.",通过对概率电路的限制放松，稳健性可以保证信用贝叶斯网络。,在许多领域中，最坏情况可以保证受到分布转移和环境不确定性的决策功能的性能（例如预测准确性）至关重要。在这项工作中，我们开发了一种方法来量化对信用贝叶斯网络的决策函数的鲁棒性，即环境的形式参数模型，在该模型中，通过参数的信用集表示不确定性。特别是，我们解决了最大边缘概率（MARMAX）问题，即确定可用于信用集中参数可获得的事件的最大概率（例如错误分类）。我们开发了一种将问题忠实地转移到概率电路上的约束优化问题中的方法。通过执行简单的约束松弛，我们展示了如何在电路尺寸的线性时间内获得保证的上限。我们从理论上进一步描述了这种约束放松，从原始的贝叶斯网络结构角度来看，这可以深入了解界限的紧密度。我们实施了该方法，并提供了实验证据，表明上限通常接近紧密，并且与其他方法相比证明了可伸缩性的提高。,https://arxiv.org/abs/2205.05793,IJCAI,True,False,False,False
1853,Insight into Voting Problem Complexity Using Randomized Classes.,"Zack Fitzsimmons, Edith Hemaspaandra","The first step in classifying the complexity of an NP problem is typically showing the problem in P or NP-complete. This has been a successful first step for many problems, including voting problems. However, in this paper we show that this may not always be the best first step. We consider the problem of constructive control by replacing voters (CCRV) introduced by Loreggia et al. (2015) for the scoring rule First-Last, which is defined by $\langle 1, 0, \dots, 0, -1\rangle$. We show that this problem is equivalent to Exact Perfect Bipartite Matching, and so CCRV for First-Last can be determined in random polynomial time. So on the one hand, if CCRV for First-Last is NP-complete then RP = NP, which is extremely unlikely. On the other hand, showing that CCRV for First-Last is in P would also show that Exact Perfect Bipartite Matching is in P, which would solve a well-studied 40-year-old open problem.   By considering RP as an option we also gain insight into the complexity of CCRV for 2-Approval, ultimately showing it in P, which settles the complexity of the sole open problem in the comprehensive table from Erd\'{e}lyi et al. (2021).",使用随机类洞察投票问题复杂性。,对NP问题的复杂性进行分类的第一步通常是在P或NP完成中显示该问题。对于许多问题，包括投票问题，这是成功的第一步。但是，在本文中，我们表明这可能并不总是是最好的第一步。我们考虑了Loreggia等人提出的替换选民（CCRV）来考虑建设性控制的问题。（2015年）对于得分规则，由$ \ langle 1，0，\ dots，0，-1 \ rangle $定义。我们表明，这个问题等同于精确的完美两分匹配，因此可以在随机多项式时间内确定第一last的CCRV。因此，一方面，如果用于第一last的CCRV是NP完整的，则RP = NP，这是极不可能的。另一方面，表明第一持久的CCRV在P中也将表明P Perfect Perfect piptite匹配是在P中，这将解决一个经过良好研究的40年历史的开放问题。通过将RP作为一种选择，我们还可以深入了解2批准的CCRV的复杂性，最终在P中显示了CCRV的复杂性，该ccrv在p \'{e} lyi等人的综合表中解决了唯一开放问题的复杂性。（2021）。,https://arxiv.org/abs/2204.12856,IJCAI,True,False,False,False
1854,Real-Time BDI Agents: A Model and Its Implementation.,"Andrea Traldi, Francesco Bruschetti, Marco Robol, Marco Roveri, Paolo Giorgini","The BDI model proved to be effective for developing applications requiring high-levels of autonomy and to deal with the complexity and unpredictability of real-world scenarios. The model, however, has significant limitations in reacting and handling contingencies within the given real-time constraints. Without an explicit representation of time, existing real-time BDI implementations overlook the temporal implications during the agent's decision process that may result in delays or unresponsiveness of the system when it gets overloaded. In this paper, we redefine the BDI agent control loop inspired by well established algorithms for real-time systems to ensure a proper reaction of agents and their effective application in typical real-time domains. Our model proposes an effective real-time management of goals, plans, and actions with respect to time constraints and resources availability. We propose an implementation of the model for a resource-collection video-game and we validate the approach against a set of significant scenarios.",实时BDI代理：模型及其实施。,事实证明，BDI模型对于开发需要高级自治的应用程序有效，并应对现实情况的复杂性和不可预测性。但是，该模型在给定实时限制内的反应和处理意外事件方面具有重大局限性。如果没有明确表示时间，现有的实时BDI实现忽略了代理决策过程中的时间含义，这些含义可能会导致系统过载时的延迟或反应。在本文中，我们重新定义了BDI代理控制环的灵感，灵感来自实时系统的良好算法，以确保代理的适当反应及其在典型的实时域中的有效应用。我们的模型提出了有关时间限制和资源可用性的有效实时管理。我们建议对资源收集视频游戏的模型实施，并根据一组重要场景验证该方法。,https://arxiv.org/abs/2205.00979,IJCAI,True,False,False,False
1855,Approval with Runoff.,"Théo Delemazure, Jérôme Lang, Jean-François Laslier, Remzi Sanver","We define a family of runoff rules that work as follows: voters cast approval ballots over candidates; two finalists are selected; and the winner is decided by majority. With approval-type ballots, there are various ways to select the finalists. We leverage known approval-based committee rules and study the obtained runoff rules from an axiomatic point of view. Then we analyze the outcome of these rules on single-peaked profiles, and on real data.",批准径流。,我们定义了一系列的径流规则，如下所示：选民对候选人进行批准选票；选出两名决赛选手；获胜者由多数派决定。有了批准式选票，有多种选择决赛选手的方法。我们利用已知的基于批准的委员会规则，并从公理角度研究获得的径流规则。然后，我们分析了这些规则在单峰配置文件和实际数据上的结果。,https://arxiv.org/abs/2203.02343,IJCAI,True,False,False,False
1856,Multi-Armed Bandit Problem with Temporally-Partitioned Rewards: When Partial Feedback Counts.,"Giulia Romano, Andrea Agostini, Francesco Trovò, Nicola Gatti, Marcello Restelli","There is a rising interest in industrial online applications where data becomes available sequentially. Inspired by the recommendation of playlists to users where their preferences can be collected during the listening of the entire playlist, we study a novel bandit setting, namely Multi-Armed Bandit with Temporally-Partitioned Rewards (TP-MAB), in which the stochastic reward associated with the pull of an arm is partitioned over a finite number of consecutive rounds following the pull. This setting, unexplored so far to the best of our knowledge, is a natural extension of delayed-feedback bandits to the case in which rewards may be dilated over a finite-time span after the pull instead of being fully disclosed in a single, potentially delayed round. We provide two algorithms to address TP-MAB problems, namely, TP-UCB-FR and TP-UCB-EW, which exploit the partial information disclosed by the reward collected over time. We show that our algorithms provide better asymptotical regret upper bounds than delayed-feedback bandit algorithms when a property characterizing a broad set of reward structures of practical interest, namely alpha-smoothness, holds. We also empirically evaluate their performance across a wide range of settings, both synthetically generated and from a real-world media recommendation problem.",多臂匪徒问题带有时间分配的奖励：当部分反馈计数时。,对工业在线应用程序的兴趣不大，其中数据依次可用。受播放列表的推荐启发给用户，在整个播放列表的聆听期间可以收集他们的偏好，我们研究了一个新颖的强盗设置，即带有时间分配的奖励（TP-MAB）的多军匪徒，在其中，随机奖励与手臂的拉力相关联，在拉动之后，将连续数量有限的连续回合分开。到目前为止，这种设置尚未探索到我们所知，这是对延迟反馈土匪的自然延伸，在拉力之后可以在有限的时间内扩张奖励，而不是在单个，潜在的，潜在的，可能完全披露延迟回合。我们提供两种算法来解决TP-MAB问题，即TP-UCB-FR和TP-UCB-EW，这些算法利用了随着时间的推移收集的奖励所披露的部分信息。我们表明，当一个属性表征了一系列具有实际兴趣的奖励结构（即Alpha-Smoothness）时，我们的算法比延迟反馈的强盗算法提供了更好的渐近遗憾上限。我们还从合成生成和现实世界中的媒体推荐问题中进行了经验评估它们在广泛的环境中的性能。,https://arxiv.org/abs/2206.00586,IJCAI,True,False,False,False
1857,Limits and Possibilities of Forgetting in Abstract Argumentation,"R Baumann, M Berthold",nan,在抽象论证中忘记忘记的限制和可能性, ,https://scholar.google.com.hk,IJCAI,False,True,False,False
1858,Robust Solutions for Multi-Defender Stackelberg Security Games.,"Dolev Mutzari, Yonatan Aumann, Sarit Kraus","Multi-defender Stackelberg Security Games (MSSG) have recently gained increasing attention in the literature. However, the solutions offered to date are highly sensitive, wherein even small perturbations in the attacker's utility or slight uncertainties thereof can dramatically change the defenders' resulting payoffs and alter the equilibrium. In this paper, we introduce a robust model for MSSGs, which admits solutions that are resistant to small perturbations or uncertainties in the game's parameters. First, we formally define the notion of robustness, as well as the robust MSSG model. Then, for the non-cooperative setting, we prove the existence of a robust approximate equilibrium in any such game, and provide an efficient construction thereof. For the cooperative setting, we show that any such game admits a robust approximate alpha-core, provide an efficient construction thereof, and prove that stronger types of the core may be empty. Interestingly, the robust solutions can substantially increase the defenders' utilities over those of the non-robust ones.",多次防御者Stackelberg安全游戏的强大解决方案。,多人Stackelberg安全游戏（MSSG）最近在文献中引起了人们的关注。但是，迄今为止提供的解决方案非常敏感，即使在攻击者的效用或其轻微的不确定性中，即使是微小的扰动也可以极大地改变捍卫者的结果并改变平衡。在本文中，我们为MSSG介绍了一个强大的模型，该模型承认了对游戏参数中对小扰动或不确定性有抵抗力的解决方案。首先，我们正式定义了鲁棒性的概念以及鲁棒的MSSG模型。然后，对于非合作设置，我们证明在任何此类游戏中都存在稳健的近似平衡，并提供了有效的结构。在合作环境中，我们表明任何这样的游戏都承认了强大的近似α核，提供了有效的结构，并证明了更强大的核心类型可能是空的。有趣的是，强大的解决方案可以大大提高捍卫者的公用事业，而不是非凡的解决方案。,https://arxiv.org/abs/2204.14000,IJCAI,True,False,False,False
1859,Contests to Incentivize a Target Group.,"Edith Elkind, Abheek Ghosh, Paul Goldberg","We study how to incentivize agents in a target group to produce a higher output in the context of incomplete information, by means of rank-order allocation contests. We describe a symmetric Bayes--Nash equilibrium for contests that have two types of rank-based prizes: prizes that are accessible only to the agents in the target group; prizes that are accessible to everyone. We also specialize this equilibrium characterization to two important sub-cases: (i) contests that do not discriminate while awarding the prizes, i.e., only have prizes that are accessible to everyone; (ii) contests that have prize quotas for the groups, and each group can compete only for prizes in their share. For these models, we also study the properties of the contest that maximizes the expected total output by the agents in the target group.",激励目标群体的比赛。,我们研究如何通过等级分配竞赛在不完整信息的情况下激励目标群中的代理，以产生更高的产出。我们描述了具有两种基于排名的奖品的竞赛的对称贝叶斯 - 均衡：奖品仅适用于目标群体中的代理商；每个人都可以获得的奖品。我们还将这种均衡表征专门针对两个重要的子案例：（i）在授予奖品时没有区分的竞赛，即只有每个人都可以获得的奖品；（ii）为小组提供奖品配额的竞赛，每个小组只能争夺其份额的奖品。对于这些模型，我们还研究了竞赛的特性，该特性最大程度地提高了目标组中代理的预期总产出。,https://arxiv.org/abs/2204.14051,IJCAI,True,False,False,False
1860,"Taylor, Can You Hear Me Now? A Taylor-Unfolding Framework for Monaural Speech Enhancement.","Andong Li, Shan You, Guochen Yu, Chengshi Zheng, Xiaodong Li","While the deep learning techniques promote the rapid development of the speech enhancement (SE) community, most schemes only pursue the performance in a black-box manner and lack adequate model interpretability. Inspired by Taylor's approximation theory, we propose an interpretable decoupling-style SE framework, which disentangles the complex spectrum recovery into two separate optimization problems \emph{i.e.}, magnitude and complex residual estimation. Specifically, serving as the 0th-order term in Taylor's series, a filter network is delicately devised to suppress the noise component only in the magnitude domain and obtain a coarse spectrum. To refine the phase distribution, we estimate the sparse complex residual, which is defined as the difference between target and coarse spectra, and measures the phase gap. In this study, we formulate the residual component as the combination of various high-order Taylor terms and propose a lightweight trainable module to replace the complicated derivative operator between adjacent terms. Finally, following Taylor's formula, we can reconstruct the target spectrum by the superimposition between 0th-order and high-order terms. Experimental results on two benchmark datasets show that our framework achieves state-of-the-art performance over previous competing baselines in various evaluation metrics. The source code is available at github.com/Andong-Lispeech/TaylorSENet.",泰勒，你现在能听到我的声音吗？泰勒无编码框架，用于增强单声道语音。,尽管深度学习技术促进了演讲增强（SE）社区的快速发展，但大多数方案仅以黑盒方式追求性能，并且缺乏足够的模型可解释性。受泰勒（Taylor）的近似理论的启发，我们提出了一个可解释的脱钩式SE框架，该框架将复杂的频谱恢复分为两个单独的优化问题\ emph {i.e。}，幅度和复杂的残差估计。具体而言，用作泰勒系列中的0阶项，精巧地设计了一个滤波器网络，以抑制幅度域中的噪声分量并获得粗频谱。为了完善相分布，我们估计稀疏的复合物残差，该残差定义为目标和粗光谱之间的差异，并测量相位间隙。在这项研究中，我们将残留成分作为各种高阶泰勒术语的组合制定，并提出一个可轻质训练的模块，以替代相邻项之间复杂的衍生算子。最后，按照泰勒的公式，我们可以通过0阶和高阶项之间的叠加来重建目标光谱。两个基准数据集的实验结果表明，在各种评估指标中，我们的框架比以前的竞争基线实现了最新的性能。源代码可在github.com/andong-lispeech/taylorsenet上获得。,https://arxiv.org/abs/2205.00206,IJCAI,True,False,False,False
1861,Multiwinner Elections under Minimax Chamberlin-Courant Rule in Euclidean Space.,"Chinmay Sonar, Subhash Suri, Jie Xue","We consider multiwinner elections in Euclidean space using the minimax Chamberlin-Courant rule. In this setting, voters and candidates are embedded in a $d$-dimensional Euclidean space, and the goal is to choose a committee of $k$ candidates so that the rank of any voter's most preferred candidate in the committee is minimized. (The problem is also equivalent to the ordinal version of the classical $k$-center problem.) We show that the problem is NP-hard in any dimension $d \geq 2$, and also provably hard to approximate. Our main results are three polynomial-time approximation schemes, each of which finds a committee with provably good minimax score. In all cases, we show that our approximation bounds are tight or close to tight. We mainly focus on the $1$-Borda rule but some of our results also hold for the more general $r$-Borda.",在欧几里得空间中，在最小的Chamberlin-Courant规则下的多翼大选举。,我们考虑使用Minimax Chamberlin-Courant规则来考虑欧几里得空间中的多翼大选举。在这种情况下，选民和候选人嵌入了$ d $维的欧几里得空间中，目标是选择一个$ k $候选人的委员会，以便将任何选民最喜欢的候选人的职级最小化。（问题也等同于经典$ k $中心问题的序数版。）我们表明，该问题在任何维度$ d \ geq 2 $中都是np-hard，也很难近似。我们的主要结果是三个多项式时间近似方案，每个方案都发现一个委员会证明具有良好的最小分数。在所有情况下，我们都表明我们的近似范围紧密或接近紧密。我们主要关注$ 1 $  -  borda规则，但我们的一些结果也适用于更通用的$ r $ boda。,https://arxiv.org/abs/2205.13598,IJCAI,True,False,False,False
1862,Training Naturalized Semantic Parsers with Very Little Data.,"Subendhu Rongali, Konstantine Arkoudas, Melanie Rubino, Wael Hamza","Semantic parsing is an important NLP problem, particularly for voice assistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic parsers are seq2seq architectures based on large language models that have been pretrained on vast amounts of text. To better leverage that pretraining, recent work has explored a reformulation of semantic parsing whereby the output sequences are themselves natural language sentences, but in a controlled fragment of natural language. This approach delivers strong results, particularly for few-shot semantic parsing, which is of key importance in practice and the focus of our paper. We push this line of work forward by introducing an automated methodology that delivers very significant additional improvements by utilizing modest amounts of unannotated data, which is typically easy to obtain. Our method is based on a novel synthesis of four techniques: joint training with auxiliary unsupervised tasks; constrained decoding; self-training; and paraphrasing. We show that this method delivers new SOTA few-shot performance on the Overnight dataset, particularly in very low-resource settings, and very compelling few-shot results on a new semantic parsing dataset.",培训归化语义解析器的数据很少。,语义解析是一个重要的NLP问题，特别是对于Alexa和Google Assistant等语音助手而言。最新的（SOTA）语义解析器是基于大量文本鉴定的大语言模型的SEQ2SEQ架构。为了更好地利用这种预处理，最近的工作探讨了语义解析的重新印象，从而使输出序列本身是自然语言句子，但以自然语言的受控片段。这种方法可提供强大的结果，尤其是对于少量的语义解析，这在实践和论文的重点中至关重要。我们通过引入一种自动化方法来推动这一工作线向前推进，该方法通过利用适量的未注释的数据来提供非常重大的改进，这通常很容易获得。我们的方法基于对四种技术的新型合成：具有无监督任务的联合培训；约束解码；自我训练；和释义。我们表明，这种方法在隔夜数据集中提供了新的SOTA少数性能，尤其是在非常低的资源设置中，并且在新的语义解析数据集中非常引人注目的结果非常引人注目。,https://arxiv.org/abs/2204.14243,IJCAI,True,False,False,False
1863,Efficient Algorithms for Monotone Non-Submodular Maximization with Partition Matroid Constraint.,"Lan N. Nguyen, My T. Thai","In this work, we study the problem of monotone non-submodular maximization with partition matroid constraint. Although a generalization of this problem has been studied in literature, our work focuses on leveraging properties of partition matroid constraint to (1) propose algorithms with theoretical bound and efficient query complexity; and (2) provide better analysis on theoretical performance guarantee of some existing techniques. We further investigate those algorithms' performance in two applications: Boosting Influence Spread and Video Summarization. Experiments show our algorithms return comparative results to the state-of-the-art algorithms while taking much fewer queries.",单调非管制最大化的有效算法，分区矩阵约束。,在这项工作中，我们研究了单调非管制最大化的问题，并通过分区矩阵约束。尽管在文献中已经研究了该问题的概括，但我们的工作着重于将分区矩阵约束的属性利用（1）提出具有理论界限和有效的查询复杂性的算法；（2）对某些现有技术的理论性能保证提供更好的分析。我们进一步研究了这些算法在两个应用中的性能：增强影响力传播和视频摘要。实验表明我们的算法将比较结果返回到最新算法的同时，同时进行了更少的查询。,https://arxiv.org/abs/2204.13832,IJCAI,True,False,False,False
1864,Incentives in Social Decision Schemes with Pairwise Comparison Preferences.,"Felix Brandt, Patrick Lederer, Warut Suksompong","Social decision schemes (SDSs) map the preferences of individual voters over multiple alternatives to a probability distribution over the alternatives. In order to study properties such as efficiency, strategyproofness, and participation for SDSs, preferences over alternatives are typically lifted to preferences over lotteries using the notion of stochastic dominance (SD). However, requiring strategyproofness or participation with respect to this preference extension only leaves room for rather undesirable SDSs such as random dictatorships. Hence, we focus on the natural but little understood pairwise comparison (PC) preference extension, which postulates that one lottery is preferred to another if the former is more likely to return a preferred outcome. In particular, we settle three open questions raised by Brandt (2017): (i) there is no Condorcet-consistent SDS that satisfies PC-strategyproofness; (ii) there is no anonymous and neutral SDS that satisfies PC-efficiency and PC-strategyproofness; and (iii) there is no anonymous and neutral SDS that satisfies PC-efficiency and strict PC-participation. All three impossibilities require m >= 4 alternatives and turn into possibilities when m <= 3.",具有成对比较偏好的社会决策计划中的激励措施。,社会决策计划（SDSS）绘制了单个选民而不是多种替代方案的偏好。为了研究效率，防策略性和对SDSS的参与等属性，通常使用随机优势（SD）概念来提起替代方案的偏好。但是，需要对这种偏好扩展的战略性或参与只会为不受欢迎的SDS（例如随机独裁政府）留出空间。因此，我们专注于天然但鲜为人知的成对比较（PC）偏好扩展，如果前者更有可能返回首选结果，则假定一个彩票优先于另一个彩票。特别是，我们解决了Brandt（2017）提出的三个公开问题：（i）没有Condorcet一致的SD可满足PC-Strategateproverness；（ii）没有匿名和中性的SD可满足PC效率和PC-STRATECYPROX；（iii）没有满足PC效率和严格的PC参与的匿名和中性SD。所有三个不可能都需要m> = 4替代方案，并在m <= 3时变成可能性。,https://arxiv.org/abs/2204.12436,IJCAI,True,False,False,False
1865,Towards Robust Unsupervised Disentanglement of Sequential Data - A Case Study Using Music Audio.,"Yin-Jyun Luo, Sebastian Ewert, Simon Dixon","Disentangled sequential autoencoders (DSAEs) represent a class of probabilistic graphical models that describes an observed sequence with dynamic latent variables and a static latent variable. The former encode information at a frame rate identical to the observation, while the latter globally governs the entire sequence. This introduces an inductive bias and facilitates unsupervised disentanglement of the underlying local and global factors. In this paper, we show that the vanilla DSAE suffers from being sensitive to the choice of model architecture and capacity of the dynamic latent variables, and is prone to collapse the static latent variable. As a countermeasure, we propose TS-DSAE, a two-stage training framework that first learns sequence-level prior distributions, which are subsequently employed to regularise the model and facilitate auxiliary objectives to promote disentanglement. The proposed framework is fully unsupervised and robust against the global factor collapse problem across a wide range of model configurations. It also avoids typical solutions such as adversarial training which usually involves laborious parameter tuning, and domain-specific data augmentation. We conduct quantitative and qualitative evaluations to demonstrate its robustness in terms of disentanglement on both artificial and real-world music audio datasets.",为了强大的无监督分散数据 - 使用音乐音频的案例研究。,解开的顺序自动编码器（DSAE）代表一类概率图形模型，该模型描述了具有动态潜在变量和静态潜在变量的观察到的序列。前者以与观测值相同的帧速率编码信息，而后者在全球范围内控制整个序列。这引入了归纳偏见，并促进了基础本地和全球因素的无监督分解。在本文中，我们表明，香草dsae对动态潜在变量的模型结构和容量的选择敏感，并且容易折叠静态潜在变量。作为对策，我们提出了TS-DSAE，这是一个两阶段的培训框架，首先学习序列级别的先验分布，随后将其用于正规化该模型并促进辅助目标以促进分解。在广泛的模型配置中，对全局因子崩溃问题进行了完全无监督和强大的框架。它还避免了典型的解决方案，例如通常涉及费力参数调整和特定于域的数据增强的对抗训练。我们进行定量和定性评估，以证明其在人工音乐和现实音乐音频数据集上的分离方面的鲁棒性。,https://arxiv.org/abs/2205.05871,IJCAI,True,False,False,False
1866,Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?,"Hwanil Choi, Wonjoon Chang, Jaesik Choi","Even though image generation with Generative Adversarial Networks has been showing remarkable ability to generate high-quality images, GANs do not always guarantee photorealistic images will be generated. Sometimes they generate images that have defective or unnatural objects, which are referred to as 'artifacts'. Research to determine why the artifacts emerge and how they can be detected and removed has not been sufficiently carried out. To analyze this, we first hypothesize that rarely activated neurons and frequently activated neurons have different purposes and responsibilities for the progress of generating images. By analyzing the statistics and the roles for those neurons, we empirically show that rarely activated neurons are related to failed results of making diverse objects and lead to artifacts. In addition, we suggest a correction method, called 'sequential ablation', to repair the defective part of the generated images without complex computational cost and manual efforts.",我们可以在深层生成网络中找到引起不切实际图像的神经元吗？,尽管具有生成对抗网络的图像生成已经显示出出色的产生高质量图像的能力，但GAN并不总是保证会生成逼真的图像。有时，它们会产生具有缺陷或不自然对象的图像，这些对象称为“工件”。研究确定为什么出现伪影以及如何被检测和去除它们的研究尚未得到充分进行。为了分析这一点，我们首先假设很少激活的神经元和经常激活的神经元具有不同的目的和责任，以实现生成图像的进展。通过分析这些神经元的统计数据和作用，我们从经验上表明，很少激活的神经元与制造多种物体和导致伪影的结果失败有关。此外，我们建议一种称为“顺序消融”的校正方法，以修复生成的图像的有缺陷部分，而无需复杂的计算成本和手动工作。,https://arxiv.org/abs/2201.06346,IJCAI,True,False,False,False
1867,A Polynomial-time Decentralised Algorithm for Coordinated Management of Multiple Intersections.,"Tatsuya Iwase, Sebastian Stein, Enrico H. Gerding, Archie Chapman","Autonomous intersection management has the potential to reduce road traffic congestion and energy consumption. To realize this potential, efficient algorithms are needed. However, most existing studies locally optimize one intersection at a time, and this can cause negative externalities on the traffic network as a whole. Here, we focus on coordinating multiple intersections, and formulate the problem as a distributed constraint optimisation problem (DCOP). We consider three utility design approaches that trade off efficiency and fairness. Our polynomial-time algorithm for coordinating multiple intersections reduces the traffic delay by about 41% compared to independent single intersection management approaches.",多项式分散算法，用于多个交集的协调管理。,自主交叉路口管理有可能减少道路交通拥堵和能源消耗。为了实现这种潜在的，有效的算法。但是，大多数现有研究一次都在本地优化一个交叉点，这可能会在整个交通网络上引起负面外部性。在这里，我们专注于协调多个交叉点，并将问题作为分布式约束优化问题（DCOP）提出。我们考虑了三种实用设计方法，可以权衡效率和公平性。与独立的单个交叉路口管理方法相比，我们用于协调多个交叉路口的多项式时间算法可将交通延迟减少约41％。,https://arxiv.org/abs/2205.00877,IJCAI,True,False,False,False
1868,Community Question Answering Entity Linking via Leveraging Auxiliary Data.,"Yuhan Li, Wei Shen, Jianbo Gao, Yadong Wang","Community Question Answering (CQA) platforms contain plenty of CQA texts (i.e., questions and answers corresponding to the question) where named entities appear ubiquitously. In this paper, we define a new task of CQA entity linking (CQAEL) as linking the textual entity mentions detected from CQA texts with their corresponding entities in a knowledge base. This task can facilitate many downstream applications including expert finding and knowledge base enrichment. Traditional entity linking methods mainly focus on linking entities in news documents, and are suboptimal over this new task of CQAEL since they cannot effectively leverage various informative auxiliary data involved in the CQA platform to aid entity linking, such as parallel answers and two types of meta-data (i.e., topic tags and users). To remedy this crucial issue, we propose a novel transformer-based framework to effectively harness the knowledge delivered by different kinds of auxiliary data to promote the linking performance. We validate the superiority of our framework through extensive experiments over a newly released CQAEL data set against state-of-the-art entity linking methods.",社区问题回答实体通过利用辅助数据链接。,社区问题回答（CQA）平台包含大量CQA文本（即与问题相对应的问题和答案），其中命名实体无处不在。在本文中，我们将CQA实体链接（CQAEL）的新任务定义为链接文本实体，从CQA文本中链接了及其在知识库中的相应实体的检测到。这项任务可以促进许多下游应用程序，包括专家发现和知识库丰富。传统实体链接方法主要集中于链接新闻文档中的实体，并且在CQAEL的这一新任务上是最佳选择-data（即主题标签和用户）。为了解决这个关键问题，我们提出了一个基于变压器的新型框架，以有效利用不同种类的辅助数据提供的知识来促进链接性能。我们通过广泛的实验比新发布的CQAEL数据集验证框架的优势，相对于最新的实体链接方法。,https://arxiv.org/abs/2205.11917,IJCAI,True,False,False,False
1869,Offline Vehicle Routing Problem with Online Bookings: A Novel Problem Formulation with Applications to Paratransit.,"Amutheezan Sivagnanam, Salah Uddin Kadir, Ayan Mukhopadhyay, Philip Pugliese, Abhishek Dubey, Samitha Samaranayake, Aron Laszka","Vehicle routing problems (VRPs) can be divided into two major categories: offline VRPs, which consider a given set of trip requests to be served, and online VRPs, which consider requests as they arrive in real-time. Based on discussions with public transit agencies, we identify a real-world problem that is not addressed by existing formulations: booking trips with flexible pickup windows (e.g., 3 hours) in advance (e.g., the day before) and confirming tight pickup windows (e.g., 30 minutes) at the time of booking. Such a service model is often required in paratransit service settings, where passengers typically book trips for the next day over the phone. To address this gap between offline and online problems, we introduce a novel formulation, the offline vehicle routing problem with online bookings. This problem is very challenging computationally since it faces the complexity of considering large sets of requests -- similar to offline VRPs -- but must abide by strict constraints on running time -- similar to online VRPs. To solve this problem, we propose a novel computational approach, which combines an anytime algorithm with a learning-based policy for real-time decisions. Based on a paratransit dataset obtained from our partner transit agency, we demonstrate that our novel formulation and computational approach lead to significantly better outcomes in this service setting than existing algorithms.",在线预订中的离线车辆路由问题：一种新的问题提出，并应用于副交易。,车辆路由问题（VRP）可以分为两个主要类别：离线VRP，它考虑要服务的给定旅行请求以及在线VRP，它们在实时到达时考虑请求。基于与公共交通机构的讨论，我们确定了现有配方未解决的现实世界问题：预订具有灵活的拾取窗口（例如3小时）（例如，前一天）的预订旅行，并确认紧密的拾取窗口（例如，预订时30分钟）。这种服务模型通常是在Paratransit服务设置中需要的，乘客通常会在第二天通过电话预订旅行。为了解决离线问题和在线问题之间的差距，我们引入了一种新颖的配方，即在线预订中的离线车辆路由问题。这个问题在计算上非常具有挑战性，因为它面临着考虑大量请求的复杂性（类似于离线VRP），但必须遵守与在线VRP相似的严格限制。为了解决这个问题，我们提出了一种新颖的计算方法，该方法将任何时间算法与实时决策的基于学习的策略相结合。基于从我们的合作伙伴运输机构获得的副译本数据集，我们证明，与现有算法相比，我们的新颖配方和计算方法在此服务环境中的结果明显更好。,https://arxiv.org/abs/2204.11992,IJCAI,True,False,False,False
1870,Invasion Dynamics in the Biased Voter Process.,"Loke Durocher, Panagiotis Karras, Andreas Pavlogiannis, Josef Tkadlec","The voter process is a classic stochastic process that models the invasion of a mutant trait $A$ (e.g., a new opinion, belief, legend, genetic mutation, magnetic spin) in a population of agents (e.g., people, genes, particles) who share a resident trait $B$, spread over the nodes of a graph. An agent may adopt the trait of one of its neighbors at any time, while the invasion bias $r\in(0,\infty)$ quantifies the stochastic preference towards ($r>1$) or against ($r<1$) adopting $A$ over $B$. Success is measured in terms of the fixation probability, i.e., the probability that eventually all agents have adopted the mutant trait $A$. In this paper we study the problem of fixation probability maximization under this model: given a budget $k$, find a set of $k$ agents to initiate the invasion that maximizes the fixation probability. We show that the problem is NP-hard for both $r>1$ and $r<1$, while the latter case is also inapproximable within any multiplicative factor. On the positive side, we show that when $r>1$, the optimization function is submodular and thus can be greedily approximated within a factor $1-1/e$. An experimental evaluation of some proposed heuristics corroborates our results.",在偏见的选民过程中的入侵动态。,选民过程是一个经典的随机过程，它模拟了突变特质$ a $（例如，新的意见，信念，传说，遗传突变，磁性旋转）的入侵（例如，人，基因，粒子）共享居民特质$ b $的人，分布在图表的节点上。代理商可以随时采用其一个邻居的特征，而入侵偏见$ r \ in（0，\ infty）$量化了对（$ r> 1 $）或反对（$ r <1 $）的随机偏好）采用$ a $ $ b $。成功是根据固定概率来衡量的，即最终所有代理商都采用了突变特质$ a $的概率。在本文中，我们研究了此模型下的固定概率最大化问题：鉴于预算$ K $，请找到一组$ k $的代理商来启动入侵，从而最大程度地提高固定概率。我们表明，对于$ r> 1 $和$ r <1 $的问题是NP-HARD，而后一种情况在任何乘法因素中也不适合。从积极的一面来看，我们表明，当$ r> 1 $时，优化功能是suppular的，因此可以在$ 1-1/e $的因子内贪婪地近似。对一些提出的启发式方法的实验评估证实了我们的结果。,https://arxiv.org/abs/2201.08207,IJCAI,True,False,False,False
1871,Emotion-Controllable Generalized Talking Face Generation.,"Sanjana Sinha, Sandika Biswas, Ravindra Yadav, Brojeshwar Bhowmick","Despite the significant progress in recent years, very few of the AI-based talking face generation methods attempt to render natural emotions. Moreover, the scope of the methods is majorly limited to the characteristics of the training dataset, hence they fail to generalize to arbitrary unseen faces. In this paper, we propose a one-shot facial geometry-aware emotional talking face generation method that can generalize to arbitrary faces. We propose a graph convolutional neural network that uses speech content feature, along with an independent emotion input to generate emotion and speech-induced motion on facial geometry-aware landmark representation. This representation is further used in our optical flow-guided texture generation network for producing the texture. We propose a two-branch texture generation network, with motion and texture branches designed to consider the motion and texture content independently. Compared to the previous emotion talking face methods, our method can adapt to arbitrary faces captured in-the-wild by fine-tuning with only a single image of the target identity in neutral emotion.",可控制的通用谈话面部产生。,尽管近年来取得了重大进展，但基于AI的会说话的面部生成方法很少试图引起自然情绪。此外，该方法的范围主要限于训练数据集的特征，因此它们未能推广到任意看不见的面孔。在本文中，我们提出了一种单一的面部几何学意识到的情感说话面部生成方法，可以推广到任意面孔。我们提出了一个使用语音内容特征的图形卷积神经网络，以及独立的情感输入来产生情感和语音引起的运动在面部几何学意识到的地标表示上。该表示形式在我们的光流引导纹理生成网络中进一步用于产生纹理。我们提出了一个两分支的纹理生成网络，其运动和纹理分支旨在独立考虑运动和纹理内容。与以前的情感说话的面部方法相比，我们的方法可以通过仅在中性情绪中使用目标身份的单个图像进行微调来适应野外捕获的任意面孔。,https://arxiv.org/abs/2205.01155,IJCAI,True,False,False,False
1872,COMET Flows: Towards Generative Modeling of Multivariate Extremes and Tail Dependence.,"Andrew McDonald, Pang-Ning Tan, Lifeng Luo","Normalizing flows, a popular class of deep generative models, often fail to represent extreme phenomena observed in real-world processes. In particular, existing normalizing flow architectures struggle to model multivariate extremes, characterized by heavy-tailed marginal distributions and asymmetric tail dependence among variables. In light of this shortcoming, we propose COMET (COpula Multivariate ExTreme) Flows, which decompose the process of modeling a joint distribution into two parts: (i) modeling its marginal distributions, and (ii) modeling its copula distribution. COMET Flows capture heavy-tailed marginal distributions by combining a parametric tail belief at extreme quantiles of the marginals with an empirical kernel density function at mid-quantiles. In addition, COMET Flows capture asymmetric tail dependence among multivariate extremes by viewing such dependence as inducing a low-dimensional manifold structure in feature space. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of COMET Flows in capturing both heavy-tailed marginals and asymmetric tail dependence compared to other state-of-the-art baseline architectures. All code is available on GitHub at https://github.com/andrewmcdonald27/COMETFlows.",彗星流：朝着多元极端和尾部依赖性的生成建模。,正常化的流量是一种流行的深层生成模型，通常无法代表在现实世界过程中观察到的极端现象。尤其是，现有的归一化流架构难以建模多变量极端，其特征是变量之间的重尾边缘分布和不对称的尾巴依赖性。鉴于这一缺点，我们提出了彗星（Copula多元极端）流，该彗星分解了将联合分布建模为两个部分的过程：（i）对其边际分布进行建模，以及（ii）对其copula分布进行建模。彗星流通过将边际极端分位数的参数尾信仰与中质量的经验核密度函数结合在一起，捕获了重尾的边际分布。此外，彗星流通过查看诸如诱导特征空间中低维歧管结构的依赖性来捕获多元极端之间的不对称尾巴依赖性。与其他最先进的基线架构相比，合成和现实世界数据集的实验结果证明了彗星流在捕获重尾边缘和不对称尾部依赖性方面的有效性。所有代码均可在https://github.com/andrewmcdonald27/cometflows上在github上找到。,https://arxiv.org/abs/2205.01224,IJCAI,True,False,False,False
1873,Thompson Sampling for Bandit Learning in Matching Markets.,"Fang Kong, Junming Yin, Shuai Li","The problem of two-sided matching markets has a wide range of real-world applications and has been extensively studied in the literature. A line of recent works have focused on the problem setting where the preferences of one-side market participants are unknown \emph{a priori} and are learned by iteratively interacting with the other side of participants. All these works are based on explore-then-commit (ETC) and upper confidence bound (UCB) algorithms, two common strategies in multi-armed bandits (MAB). Thompson sampling (TS) is another popular approach, which attracts lots of attention due to its easier implementation and better empirical performances. In many problems, even when UCB and ETC-type algorithms have already been analyzed, researchers are still trying to study TS for its benefits. However, the convergence analysis of TS is much more challenging and remains open in many problem settings. In this paper, we provide the first regret analysis for TS in the new setting of iterative matching markets. Extensive experiments demonstrate the practical advantages of the TS-type algorithm over the ETC and UCB-type baselines.",汤普森在匹配市场中进行匪徒学习的抽样。,双面匹配市场的问题具有广泛的现实应用程序，并在文献中进行了广泛的研究。最近的一系列作品集中在问题设置上，其中一面市场参与者的偏好是未知的\ emph {a先验}，并且通过与参与者的另一端进行迭代互动而学习。所有这些作品均基于探索-Commit（ETC）和上置信度结合（UCB）算法，这是多臂BANDITS（MAB）中的两种常见策略。汤普森采样（TS）是另一种流行的方法，由于其更容易实施和更好的经验表现，它引起了很多关注。在许多问题中，即使已经分析了UCB和ETC型算法，研究人员仍在尝试研究TS的好处。但是，TS的收敛分析更具挑战性，并且在许多问题环境中仍然开放。在本文中，我们在迭代匹配市场的新环境中为TS提供了第一个遗憾分析。广泛的实验证明了TS型算法比ETC和UCB型基线的实际优势。,https://arxiv.org/abs/2204.12048,IJCAI,True,False,False,False
1874,IMO^3: Interactive Multi-Objective Off-Policy Optimization.,"Nan Wang, Hongning Wang, Maryam Karimzadehgan, Branislav Kveton, Craig Boutilier","Most real-world optimization problems have multiple objectives. A system designer needs to find a policy that trades off these objectives to reach a desired operating point. This problem has been studied extensively in the setting of known objective functions. We consider a more practical but challenging setting of unknown objective functions. In industry, this problem is mostly approached with online A/B testing, which is often costly and inefficient. As an alternative, we propose interactive multi-objective off-policy optimization (IMO^3). The key idea in our approach is to interact with a system designer using policies evaluated in an off-policy fashion to uncover which policy maximizes her unknown utility function. We theoretically show that IMO^3 identifies a near-optimal policy with high probability, depending on the amount of feedback from the designer and training data for off-policy estimation. We demonstrate its effectiveness empirically on multiple multi-objective optimization problems.",IMO^3：交互式多目标非政策优化。,大多数实际优化问题都有多种目标。系统设计师需要找到一项策略，以交易这些目标才能达到所需的操作点。在已知的目标函数的设置中，对此问题进行了广泛的研究。我们认为未知目标功能的更实际但具有挑战性的环境。在行业中，此问题主要通过在线A/B测试来解决，这通常是昂贵且效率低下的。作为替代方案，我们提出了交互式多目标非政策优化（IMO^3）。我们方法中的关键思想是使用以非政策方式评估的策略与系统设计师进行交互，以发现最大化她未知的实用程序功能的政策。从理论上讲，IMO^3表明IMO^3具有很高的可能性，具体取决于设计师的反馈和培训数据的反馈量，以进行非政策估计。我们在多个多目标优化问题上从经验上证明了其有效性。,https://arxiv.org/abs/2201.09798,IJCAI,True,False,False,False
1875,Disentangling the Computational Complexity of Network Untangling.,"Vincent Froese, Pascal Kunz, Philipp Zschoche","We study the network untangling problem introduced by Rozenshtein, Tatti, and Gionis [DMKD 2021], which is a variant of Vertex Cover on temporal graphs -- graphs whose edge set changes over discrete time steps. They introduce two problem variants. The goal is to select at most $k$ time intervals for each vertex such that all time-edges are covered and (depending on the problem variant) either the maximum interval length or the total sum of interval lengths is minimized. This problem has data mining applications in finding activity timelines that explain the interactions of entities in complex networks.   Both variants of the problem are NP-hard. In this paper, we initiate a multivariate complexity analysis involving the following parameters: number of vertices, lifetime of the temporal graph, number of intervals per vertex, and the interval length bound. For both problem versions, we (almost) completely settle the parameterized complexity for all combinations of those four parameters, thereby delineating the border of fixed-parameter tractability.",解开网络毫无疑问的计算复杂性。,我们研究了Rozenshtein，Tatti和Gionis [DMKD 2021]引入的网络无纠结问题，该问题是时间图上顶点覆盖物的变体 - 图形集在离散时间步长上变化的图形。他们介绍了两个问题变体。目标是为每个顶点选择最多$ K $的时间间隔，以便涵盖所有时间边缘，并（取决于问题变体）最大间隔长度或间隔长度的总和最小。该问题在查找活动时间表时具有数据挖掘应用程序，以解释复杂网络中实体的相互作用。问题的两个变体都是NP-HARD。在本文中，我们启动涉及以下参数的多元复杂性分析：顶点数，时间图的寿命，每个顶点间隔数和间隔长度绑定。对于这两个问题版本，我们（几乎）（几乎）完全解决这四个参数的所有组合的参数化复杂性，从而描绘了固定参数障碍的边界。,https://arxiv.org/abs/2204.02668,IJCAI,True,False,False,False
1876,Learnability of Competitive Threshold Models.,"Yifan Wang, Guangmo Tong","Modeling the spread of social contagions is central to various applications in social computing. In this paper, we study the learnability of the competitive threshold model from a theoretical perspective. We demonstrate how competitive threshold models can be seamlessly simulated by artificial neural networks with finite VC dimensions, which enables analytical sample complexity and generalization bounds. Based on the proposed hypothesis space, we design efficient algorithms under the empirical risk minimization scheme. The theoretical insights are finally translated into practical and explainable modeling methods, the effectiveness of which is verified through a sanity check over a few synthetic and real datasets. The experimental results promisingly show that our method enjoys a decent performance without using excessive data points, outperforming off-the-shelf methods.",竞争阈值模型的可学习性。,建模社会传染的传播对于社会计算中的各种应用是至关重要的。在本文中，我们从理论角度研究了竞争性阈值模型的可学习性。我们证明了如何通过具有有限VC维度的人工神经网络无缝模拟竞争性阈值模型，从而实现了分析样品的复杂性和泛化边界。根据提出的假设空间，我们在经验风险最小化方案下设计有效的算法。理论见解最终被转化为实用且可解释的建模方法，通过对一些合成和真实数据集进行理智检查，其有效性得到了验证。实验结果有希望表明，我们的方法在不使用过多的数据点的情况下表现不错，表现优于现成的方法。,https://arxiv.org/abs/2205.03750,IJCAI,True,False,False,False
1877,Let's Agree to Agree: Targeting Consensus for Incomplete Preferences through Majority Dynamics.,"Sirin Botan, Simon Rey, Zoi Terzopoulou","We study settings in which agents with incomplete preferences need to make a collective decision. We focus on a process of majority dynamics where issues are addressed one at a time and undecided agents follow the opinion of the majority. We assess the effects of this process on various consensus notions -- such as the Condorcet winner -- and show that in the worst case, myopic adherence to the majority damages existing consensus; yet, simulation experiments indicate that the damage is often mild. We also examine scenarios where the chair of the decision process can control the existence (or the identity) of consensus, by determining the order in which the issues are discussed.",让我们同意同意：针对多数动态的不完整偏好的共识。,我们研究设置，其中不完整的偏好代理需要做出集体决定。我们专注于多数动态的过程，在该过程中一次解决问题，而未定的代理商遵循多数人的意见。我们评估了这一过程对各种共识概念（例如Condorcet获胜者）的影响，并表明在最坏的情况下，近视遵守大多数损害现有共识；然而，模拟实验表明损害通常是轻微的。我们还研究了决策过程主席可以通过确定讨论问题的顺序来控制共识的存在（或身份）的情况。,https://arxiv.org/abs/2205.00881,IJCAI,True,False,False,False
1878,Exploring the Vulnerability of Deep Reinforcement Learning-based Emergency Control for Low Carbon Power Systems.,"Xu Wan, Lanting Zeng, Mingyang Sun","Decarbonization of global power systems significantly increases the operational uncertainty and modeling complexity that drive the necessity of widely exploiting cutting-edge Deep Reinforcement Learning (DRL) technologies to realize adaptive and real-time emergency control, which is the last resort for system stability and resiliency. The vulnerability of the DRL-based emergency control scheme may lead to severe real-world security issues if it can not be fully explored before implementing it practically. To this end, this is the first work that comprehensively investigates adversarial attacks and defense mechanisms for DRL-based power system emergency control. In particular, recovery-targeted (RT) adversarial attacks are designed for gradient-based approaches, aiming to dramatically degrade the effectiveness of the conducted emergency control actions to prevent the system from restoring to a stable state. Furthermore, the corresponding robust defense (RD) mechanisms are proposed to actively modify the observations based on the distances of sequential states. Experiments are conducted based on the standard IEEE reliability test system, and the results show that security risks indeed exist in the state-of-the-art DRL-based power system emergency control models. The effectiveness, stealthiness, instantaneity, and transferability of the proposed attacks and defense mechanisms are demonstrated with both white-box and black-box settings.",探索基于深化学习的紧急控制对低碳电源系统的脆弱性。,全球电力系统的脱碳大大提高了操作不确定性和建模复杂性，这推动了广泛利用尖端深层增强学习（DRL）技术的必要性，以实现适应性和实时的紧急控制，这是系统稳定性和弹性的最后度假胜地。如果无法在实施之前无法对其进行充分探索，则基于DRL的紧急控制计划的脆弱性可能会导致严重的现实安全问题。为此，这是第一项全面研究基于DRL的电力系统紧急控制的对抗性攻击和防御机制的工作。特别是，以恢复目标（RT）对抗攻击是为基于梯度的方法而设计的，旨在极大地降低执行紧急控制动作的有效性，以防止系统恢复到稳定的状态。此外，提出了相应的鲁棒防御（RD）机制，以根据顺序状态的距离积极修改观测值。实验是根据标准IEEE可靠性测试系统进行的，结果表明，基于最新的DRL电力系统紧急控制模型确实存在安全风险。白盒和黑盒设置都证明了拟议攻击和防御机制的有效性，隐身性，即时性和可转移性。,https://scholar.google.com.hk,IJCAI,True,False,False,False
1879,Multi-Tier Platform for Cognizing Massive Electroencephalogram.,"Zheng Chen, Lingwei Zhu, Ziwei Yang, Renyuan Zhang","An end-to-end platform assembling multiple tiers is built for precisely cognizing brain activities. Being fed massive electroencephalogram (EEG) data, the time-frequency spectrograms are conventionally projected into the episode-wise feature matrices (seen as tier-1). A spiking neural network (SNN) based tier is designed to distill the principle information in terms of spike-streams from the rare features, which maintains the temporal implication in the nature of EEGs. The proposed tier-3 transposes time- and space-domain of spike patterns from the SNN; and feeds the transposed pattern-matrices into an artificial neural network (ANN, Transformer specifically) known as tier-4, where a special spanning topology is proposed to match the two-dimensional input form. In this manner, cognition such as classification is conducted with high accuracy. For proof-of-concept, the sleep stage scoring problem is demonstrated by introducing multiple EEG datasets with the largest comprising 42,560 hours recorded from 5,793 subjects. From experiment results, our platform achieves the general cognition overall accuracy of 87% by leveraging sole EEG, which is 2% superior to the state-of-the-art. Moreover, our developed multi-tier methodology offers visible and graphical interpretations of the temporal characteristics of EEG by identifying the critical episodes, which is demanded in neurodynamics but hardly appears in conventional cognition scenarios.",多层平台，用于认识大规模的脑电图。,"组装多层层的端到端平台是为精确认识大脑活动而构建的。被喂养大量的脑电图（EEG）数据，时间频谱图通常投影到插曲特征矩阵中（被视为tier-1）。基于尖峰的神经网络（SNN）层旨在从稀有特征中提取原理信息，从而保持了脑电图性质的时间含义。所提出的层3转移了SNN的尖峰图案的时间和空间域；并将转置模式纳学馈送到人工神经网络（ANN，Transformer）中，该网络特别称为Tier-4，其中提出了一种特殊的跨性拓扑结构以匹配二维输入形式。以这种方式，诸如分类之类的认知是高精度进行的。为了概念验证，通过引入多个脑电图数据集，其中最大的42,560小时记录了5,793名受试者，可以证明睡眠阶段评分问题。从实验结果中，我们的平台通过利用唯一的脑电图来实现87％的总体认知总体准确性，而唯一的脑电图比最新的脑电图高2％。此外，我们开发的多层方法论通过识别关键事件来提供对脑电图的时间特征的可见和图形解释，这在神经动力学中是在神经动力学中所要求的，但在常规的认知情景中几乎没有出现。",https://arxiv.org/abs/2204.09840,IJCAI,True,False,False,False
1880,Robust Subset Selection by Greedy and Evolutionary Pareto Optimization.,"Chao Bian, Yawen Zhou, Chao Qian","Subset selection, which aims to select a subset from a ground set to maximize some objective function, arises in various applications such as influence maximization and sensor placement. In real-world scenarios, however, one often needs to find a subset which is robust against (i.e., is good over) a number of possible objective functions due to uncertainty, resulting in the problem of robust subset selection. This paper considers robust subset selection with monotone objective functions, relaxing the submodular property required by previous studies. We first show that the greedy algorithm can obtain an approximation ratio of $1-e^{-\beta\opgamma}$, where $\beta$ and $\opgamma$ are the correlation and submodularity ratios of the objective functions, respectively; and then propose EPORSS, an evolutionary Pareto optimization algorithm that can utilize more time to find better subsets. We prove that EPORSS can also be theoretically grounded, achieving a similar approximation guarantee to the greedy algorithm. In addition, we derive the lower bound of $\beta$ for the application of robust influence maximization, and further conduct experiments to validate the performance of the greedy algorithm and EPORSS.",通过贪婪和进化帕累托优化的鲁棒子集选择。,旨在从地面集中选择子集以最大化某些目标函数的子集选择是在各种应用中出现的，例如影响最大化和传感器放置。但是，在实际情况下，通常需要找到一个因不确定性而导致的许多可能的目标函数（即良好的）子集，从而导致了可靠的子集选择问题。本文认为具有单调目标函数的鲁棒子集选择，从而放松了先前研究所需的子模型。我们首先表明，贪婪算法可以获得$ 1-e^{ -  \ beta \ opgamma} $的近似值，其中$ \ beta $和$ \ opgamma $分别是目标功能的相关性和suppeructionality比值；然后提出Eporss，这是一种进化的帕累托优化算法，可以利用更多的时间找到更好的子集。我们证明，在理论上也可以基于基础，从而获得与贪婪算法相似的近似保证。此外，我们为适用的最大化而得出了$ \ beta $的下限，并进一步进行实验以验证贪婪算法和eporss的性能。,https://arxiv.org/abs/2205.01415,IJCAI,True,False,False,False
1881,Tolerance is Necessary for Stability: Single-Peaked Swap Schelling Games.,"Davide Bilò, Vittorio Bilò, Pascal Lenzner, Louise Molitor","Residential segregation in metropolitan areas is a phenomenon that can be observed all over the world. It is characterized by the emergence of large regions populated by residents that are homogeneous in terms of ethnicity or other traits. In a recent research trend in the AI community this phenomenon was investigated via game-theoretic models. There, selfish agents of two types are equipped with a monotone utility function that ensures higher utility if an agent has more same-type neighbors. The agents strategically choose their location on a given graph that serves as residential area to maximize their utility. However, sociological polls suggest that real-world agents are actually favoring mixed-type neighborhoods, and hence should be modeled via non-monotone utility functions.   We study Swap Schelling Games with non-monotone utility functions that are single-peaked. In these games pairs of agents may improve their utility by swapping their locations. Our main finding is that tolerance, i.e., that the agents favor fifty-fifty neighborhoods or even being in the minority, is necessary for equilibrium existence on almost regular or bipartite graphs. We show equilibrium existence on almost regular graphs via a potential function argument and we prove that this approach is impossible on arbitrary graphs even with tolerant agents. Regarding the quality of equilibria, we consider the recently introduced degree of integration, that counts the number of agents that live in a heterogeneous neighborhood, as social welfare function. We derive (almost) tight bounds on the Price of Anarchy and the Price of Stability. In particular, we show that the latter is constant on bipartite and almost regular graphs. Moreover, we prove that computing approximations of the social optimum placement and the equilibrium with maximum social welfare is NP-hard even on cubic graphs.",耐受性对于稳定性是必要的：单峰掉期Schelling游戏。,大都市地区的住宅隔离是可以在世界范围内观察到的一种现象。它的特征是，在种族或其他特征方面，居民人口群体的大型地区的出现。在AI社区的最新研究趋势中，这种现象是通过游戏理论模型进行了研究的。在那里，两种类型的自私设备配备了单调实用程序功能，如果代理具有更多相同型邻居，则可以确保更高的效用。代理商从策略上选择其位置在给定图表上，该图形用作住宅区域，以最大程度地提高其效用。但是，社会学民意调查表明，现实世界中的代理人实际上偏爱混合型社区，因此应通过非单调式实用程序功能进行建模。我们研究了单峰的非符号实用程序功能的交换schelling游戏。在这些游戏中，代理商对可以通过交换位置来改善其实用性。我们的主要发现是，公差，即代理人偏爱五十个社区甚至是少数群体，对于几乎规则或两分的图上的平衡存在是必要的。我们通过潜在函数参数在几乎常规图上显示出平衡的存在，我们证明即使使用耐受性剂，在任意图上也无法使用这种方法。关于均衡的质量，我们认为最近引入的整合程度，这将居住在异质社区的代理商数量计为社会福利功能。我们在无政府状态的价格和稳定价格的价格上（几乎）得出（几乎）的范围。特别是，我们表明后者在两分和几乎常规图上是恒定的。此外，我们证明，即使在立方图上，计算社会最佳位置和最大社会福利平衡的近似值也是NP的近似值。,https://arxiv.org/abs/2204.12599,IJCAI,True,False,False,False
1882,General Optimization Framework for Recurrent Reachability Objectives.,"David Klaška, Antonín Kučera, Vít Musil, Vojtěch Řehák","We consider the mobile robot path planning problem for a class of recurrent reachability objectives. These objectives are parameterized by the expected time needed to visit one position from another, the expected square of this time, and also the frequency of moves between two neighboring locations. We design an efficient strategy synthesis algorithm for recurrent reachability objectives and demonstrate its functionality on non-trivial instances.",复发性目标的一般优化框架。,我们考虑一系列复发性可达目标的移动机器人路径计划问题。这些目标是通过从另一个位置，预期的平方以及两个相邻位置之间移动频率访问一个位置所需的预期时间参数。我们设计了一种有效的策略综合算法，以实现复发性目标，并在非平凡实例上证明了其功能。,https://arxiv.org/abs/2205.14057,IJCAI,True,False,False,False
1883,Phragm\'en Rules for Degressive and Regressive Proportionality,"Michal Jaworski, Piotr Skowron","We study two concepts of proportionality in the model of approval-based committee elections. In degressive proportionality small minorities of voters are favored in comparison with the standard linear proportionality. Regressive proportionality, on the other hand, requires that larger subdivisions of voters are privileged. We introduce a new family of rules that broadly generalize Phragm\'en's Sequential Rule spanning the spectrum between degressive and regressive proportionality. We analyze and compare the two principles of proportionality assuming the voters and the candidates can be represented as points in an Euclidean issue space.",短语\'en demalitess for Demalsiss and Recressive ateragilational的规则,我们研究了基于批准的委员会选举模型中的两个相称性概念。与标准线性比例相比，在脱气比例的情况下，少数选民受到青睐。另一方面，回归比例要求选民的较大细分具有特权。我们介绍了一个新的规则家族，该家族广泛地概括了跨越词的顺序规则，该规则涵盖了脱发和回归比例之间的频谱。假设选民和候选人可以作为欧几里得问题空间中的观点表示，我们分析和比较了相称的两个原则。,https://arxiv.org/abs/2201.04248,IJCAI,True,False,False,False
1884,Robust Single Image Dehazing Based on Consistent and Contrast-Assisted Reconstruction.,"De Cheng, Yan Li, Dingwen Zhang, Nannan Wang, Xinbo Gao, Jiande Sun","Single image dehazing as a fundamental low-level vision task, is essential for the development of robust intelligent surveillance system. In this paper, we make an early effort to consider dehazing robustness under variational haze density, which is a realistic while under-studied problem in the research filed of singe image dehazing. To properly address this problem, we propose a novel density-variational learning framework to improve the robustness of the image dehzing model assisted by a variety of negative hazy images, to better deal with various complex hazy scenarios. Specifically, the dehazing network is optimized under the consistency-regularized framework with the proposed Contrast-Assisted Reconstruction Loss (CARL). The CARL can fully exploit the negative information to facilitate the traditional positive-orient dehazing objective function, by squeezing the dehazed image to its clean target from different directions. Meanwhile, the consistency regularization keeps consistent outputs given multi-level hazy images, thus improving the model robustness. Extensive experimental results on two synthetic and three real-world datasets demonstrate that our method significantly surpasses the state-of-the-art approaches.",基于一致且对比度辅助的重建，可靠的单图像去掩饰。,单图像作为基本的低级视觉任务，对于发展强大的智能监视系统至关重要。在本文中，我们尽早努力考虑在各种雾度密度下进行脱掩护，这是一个现实的，而在提出的Singe Image Dehazing的研究中，这是一个现实的问题。为了正确解决这个问题，我们提出了一个新型的密度变化学习框架，以提高图像脱去模型的鲁棒性，并通过各种负面的朦胧图像有助于，以更好地处理各种复杂的朦胧场景。具体而言，向对比度辅助的重建损失（CARL）在一致性规范化的框架下进行了优化的飞行网络。CARL可以通过从不同方向挤压到其干净的目标，从而充分利用负面信息，以促进传统的正向飞行目标函数。同时，一致性正则化在给定多级朦胧的图像的情况下保持一致的输出，从而改善了模型的鲁棒性。对两个合成和三个现实世界数据集的广泛实验结果表明，我们的方法显着超过了最新方法。,https://arxiv.org/abs/2203.15325,IJCAI,True,False,False,False
1885,Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt.,"Xinyin Ma, Xinchao Wang, Gongfan Fang, Yongliang Shen, Weiming Lu","Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pre-trained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.",提示蒸馏：通过增强提示来增强无数据知识蒸馏。,无数据知识蒸馏（DFKD）通过消除原始培训数据的依赖性进行知识蒸馏，最近在加速预训练的语言模型方面取得了令人印象深刻的结果。DFKD的核心是通过反转未压缩模型的参数来重建合成数据集。然而，先前的DFKD方法在很大程度上依赖于重建目标数据分布的手工制作的先验，这可能是不可避免地会偏见的，并且通常无法捕获固有分布。为了解决这个问题，我们提出了一种基于及时的方法，称为及时的方法，该方法使我们能够利用学习的语言先验，该方法有效地统一了综合句子在语义上和语法上是正确的。具体而言，促使DFD利用预先训练的生成模型来提供语言先验，并引入了强化的主题提示器来控制数据综合，使生成的样本在主题上是相关且具有语义上合理的，因此对下游任务友好。如我们的实验所示，所提出的方法基本上改善了合成质量，并在蒸馏性能方面取得了可观的改善。在某些情况下，提示DFD甚至会与数据驱动的知识蒸馏的结果相提并论，并访问了原始培训数据。,https://arxiv.org/abs/2205.07523,IJCAI,True,False,False,False
1886,MMNet: Muscle Motion-Guided Network for Micro-Expression Recognition.,"Hanting Li, Mingzhe Sui, Zhaoqing Zhu, Feng Zhao","Facial micro-expressions (MEs) are involuntary facial motions revealing peoples real feelings and play an important role in the early intervention of mental illness, the national security, and many human-computer interaction systems. However, existing micro-expression datasets are limited and usually pose some challenges for training good classifiers. To model the subtle facial muscle motions, we propose a robust micro-expression recognition (MER) framework, namely muscle motion-guided network (MMNet). Specifically, a continuous attention (CA) block is introduced to focus on modeling local subtle muscle motion patterns with little identity information, which is different from most previous methods that directly extract features from complete video frames with much identity information. Besides, we design a position calibration (PC) module based on the vision transformer. By adding the position embeddings of the face generated by PC module at the end of the two branches, the PC module can help to add position information to facial muscle motion pattern features for the MER. Extensive experiments on three public micro-expression datasets demonstrate that our approach outperforms state-of-the-art methods by a large margin.",MMNET：用于微表达识别的肌肉运动引导网络。,面部微表达（MES）是非自愿的面部动作，揭示了人们的真实感受，并在精神疾病，国家安全和许多人类计算机互动系统的早期干预中起着重要作用。但是，现有的微表达数据集有限，通常对培训良好的分类器构成一些挑战。为了建模微妙的面部肌肉运动，我们提出了一个健壮的微表达识别（MER）框架，即肌肉运动引导网络（MMNET）。具体而言，引入了连续的注意（CA）块，专注于对局部微妙的肌肉运动模式进行建模，几乎没有身份信息，这与大多数以前的方法不同，这些方法直接从完整的视频框架中提取具有许多身份信息的方法。此外，我们根据视觉变压器设计一个位置校准（PC）模块。通过添加PC模块在两个分支末端产生的面部的位置嵌入，PC模块可以帮助将位置信息添加到MER的面部肌肉运动图案中。在三个公共微表达数据集上进行的广泛实验表明，我们的方法以大幅度优于最先进的方法。,https://arxiv.org/abs/2201.05297,IJCAI,True,False,False,False
1887,Data-Efficient Backdoor Attacks.,"Pengfei Xia, Ziqiang Li, Wei Zhang, Bin Li","Recent studies have proven that deep neural networks are vulnerable to backdoor attacks. Specifically, by mixing a small number of poisoned samples into the training set, the behavior of the trained model can be maliciously controlled. Existing attack methods construct such adversaries by randomly selecting some clean data from the benign set and then embedding a trigger into them. However, this selection strategy ignores the fact that each poisoned sample contributes inequally to the backdoor injection, which reduces the efficiency of poisoning. In this paper, we formulate improving the poisoned data efficiency by the selection as an optimization problem and propose a Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the same attack success rate can be achieved with only 47% to 75% of the poisoned sample volume compared to the random selection strategy. More importantly, the adversaries selected according to one setting can generalize well to other settings, exhibiting strong transferability.",数据有效的后门攻击。,最近的研究证明，深层神经网络容易受到后门攻击的影响。具体而言，通过将少量中毒样品混合到训练集中，可以恶意控制训练的模型的行为。现有的攻击方法通过随机从良性集中选择一些干净的数据，然后将触发器嵌入其中，来构建此类对手。但是，这种选择策略忽略了这样一个事实，即每个中毒样品都对后门注射不平等，从而降低了中毒的效率。在本文中，我们通过选择作为优化问题来提高中毒数据效率，并提出一种过滤和升级策略（FUS）来解决它。CIFAR-10和Imagenet-10上的实验结果表明，与随机选择策略相比，该方法可以有效：只有47％至75％的中毒样品体积，可以实现相同的攻击成功率。更重要的是，根据一种设置选择的对手可以很好地推广到其他设置，表现出强大的转移性。,https://arxiv.org/abs/2204.12281,IJCAI,True,False,False,False
1888,Relational Abstractions for Generalized Reinforcement Learning on Symbolic Problems.,"Rushang Karia, Siddharth Srivastava","Reinforcement learning in problems with symbolic state spaces is challenging due to the need for reasoning over long horizons. This paper presents a new approach that utilizes relational abstractions in conjunction with deep learning to learn a generalizable Q-function for such problems. The learned Q-function can be efficiently transferred to related problems that have different object names and object quantities, and thus, entirely different state spaces. We show that the learned generalized Q-function can be utilized for zero-shot transfer to related problems without an explicit, hand-coded curriculum. Empirical evaluations on a range of problems show that our method facilitates efficient zero-shot transfer of learned knowledge to much larger problem instances containing many objects.",关于符号问题的广义增强学习的关系抽象。,由于需要长时间的推理，在具有符号状态空间的问题中进行的强化学习具有挑战性。本文提出了一种新的方法，该方法利用了关系抽象的结合，并深入学习来学习有关此类问题的可推广Q功能。学习的Q功能可以有效地转移到具有不同对象名称和对象数量的相关问题，从而完全不同的状态空间。我们表明，无需明确的手工编码课程即可将学习的广义Q功能用于零射击转移到相关问题。对一系列问题的经验评估表明，我们的方法促进了将学习知识的有效零转移到包含许多对象的更大的问题实例。,https://arxiv.org/abs/2204.12665,IJCAI,True,False,False,False
1889,Rethinking InfoNCE: How Many Negative Samples Do You Need?,"Chuhan Wu, Fangzhao Wu, Yongfeng Huang","InfoNCE loss is a widely used loss function for contrastive model training. It aims to estimate the mutual information between a pair of variables by discriminating between each positive pair and its associated $K$ negative pairs. It is proved that when the sample labels are clean, the lower bound of mutual information estimation is tighter when more negative samples are incorporated, which usually yields better model performance. However, in many real-world tasks the labels often contain noise, and incorporating too many noisy negative samples for model training may be suboptimal. In this paper, we study how many negative samples are optimal for InfoNCE in different scenarios via a semi-quantitative theoretical framework. More specifically, we first propose a probabilistic model to analyze the influence of the negative sampling ratio $K$ on training sample informativeness. Then, we design a training effectiveness function to measure the overall influence of training samples on model learning based on their informativeness. We estimate the optimal negative sampling ratio using the $K$ value that maximizes the training effectiveness function. Based on our framework, we further propose an adaptive negative sampling method that can dynamically adjust the negative sampling ratio to improve InfoNCE based model training. Extensive experiments on different real-world datasets show our framework can accurately predict the optimal negative sampling ratio in different tasks, and our proposed adaptive negative sampling method can achieve better performance than the commonly used fixed negative sampling ratio strategy.",重新思考Infonce：您需要多少个负样本？,Infonce损失是对比度模型训练的广泛使用的损失函数。它的目的是通过区分每对及其相关的$ k $负对来估算一对变量之间的共同信息。事实证明，当样本标签清洁时，当纳入更多负样本时，相互信息估计的下限会更严格，这通常会产生更好的模型性能。但是，在许多实际任务中，标签通常包含噪声，并且在模型训练中纳入过多的嘈杂的负样本可能是次优的。在本文中，我们研究了在不同情况下通过半定量理论框架在不同情况下有多少负样本是最佳的。更具体地说，我们首先提出了一个概率模型，以分析负抽样比$ k $对培训样本信息性的影响。然后，我们设计了培训效果功能，以根据其信息性来衡量培训样本对模型学习的总体影响。我们使用$ k $值估算最佳的负面抽样率，从而最大化训练效果。基于我们的框架，我们进一步提出了一种自适应负抽样方法，该方法可以动态调整负抽样比以改善基于Infonce的模型训练。在不同的现实世界数据集上进行的广泛实验表明，我们的框架可以准确预测不同任务中的最佳负抽样比，并且我们提出的自适应负抽样方法可以比常用的固定固定负面采样比策略获得更好的性能。,https://arxiv.org/abs/2105.13003,IJCAI,True,False,False,False
1890,Declaration-based Prompt Tuning for Visual Question Answering.,"Yuhang Liu, Wei Wei, Daowan Peng, Feida Zhu","In recent years, the pre-training-then-fine-tuning paradigm has yielded immense success on a wide spectrum of cross-modal tasks, such as visual question answering (VQA), in which a visual-language (VL) model is first optimized via self-supervised task objectives, e.g., masked language modeling (MLM) and image-text matching (ITM), and then fine-tuned to adapt to downstream task (e.g., VQA) via a brand-new objective function, e.g., answer prediction. The inconsistency of the objective forms not only severely limits the generalization of pre-trained VL models to downstream tasks, but also requires a large amount of labeled data for fine-tuning. To alleviate the problem, we propose an innovative VL fine-tuning paradigm (named Declaration-based Prompt Tuning, abbreviated as DPT), which jointly optimizes the objectives of pre-training and fine-tuning of VQA model, boosting the effective adaptation of pre-trained VL models to the downstream task. Specifically, DPT reformulates the objective form of VQA task via (1) textual adaptation, which converts the given questions into declarative sentence-form for prompt-tuning, and (2) task adaptation, which optimizes the objective function of VQA problem in the manner of pre-training phase. Experimental results on GQA dataset show that DPT outperforms the fine-tuned counterpart by a large margin regarding accuracy in both fully-supervised (2.68%) and zero-shot/few-shot (over 31%) settings. All the data and codes will be available to facilitate future research.",基于声明的及时调整视觉问题回答。,近年来，训练前的训练范式在广泛的跨模式任务上取得了巨大的成功，例如视觉询问回答（VQA），其中首先是视觉语言（VL）模型通过自我监督的任务目标进行优化，例如，蒙版语言建模（MLM）和图像文本匹配（ITM），然后微调以适应下游任务（例如VQA）通过全新的目标函数，例如回答预测。目标形式的不一致不仅严重限制了预训练的VL模型的概括到下游任务，而且还需要大量的标记数据进行微调。为了减轻问题，我们提出了一种创新的VL微调范式（称为基于声明的及时调整，缩写为DPT），该范式共同优化了VQA模型的预培训和微调的目标，从而有效适应了PRE的有效适应 - 下游任务训练的VL模型。具体而言，DPT通过（1）文本适应重新重新设计了VQA任务的客观形式，该文本适应将给定的问题转换为声明的句子形式以进行及时调整，以及（2）任务适应，该任务适应以方式优化VQA问题的客观功能。训练阶段。GQA数据集的实验结果表明，DPT在完全监督的（2.68％）和零射击/少数拍摄（超过31％）的设置方面的准确性都超过了微调的同行。所有数据和代码都将用于促进未来的研究。,https://arxiv.org/abs/2205.02456,IJCAI,True,False,False,False
1891,Local Differential Privacy Meets Computational Social Choice - Resilience under Voter Deletion.,"Liangde Tao, Lin Chen, Lei Xu, Weidong Shi","The resilience of a voting system has been a central topic in computational social choice. Many voting rules, like {\it plurality}, are shown to be vulnerable. What if a local differential privacy (LDP) mechanism is adopted such that the true preference of a voter is never revealed in pre-election polls? In this case, the attacker can only infer stochastic information of a voter's true preference, and this may cause the manipulation of the electoral result significantly harder. The goal of this paper is to give a quantitative study on the effect of adopting LDP mechanisms on a voting system. We introduce the metric PoLDP (power of LDP) that quantitatively measures the difference between the attacker's manipulation cost under LDP mechanisms and that without LDP mechanisms. The larger PoLDP is, the more robustness LDP mechanisms can add to a voting system. In this paper, we give a full characterization of PoLDP for the voting system with an arbitrary scoring rule. Our work gives a general guidance towards the application of LDP mechanisms.",当地的差异隐私符合计算社会选择 - 选民删除下的弹性。,投票系统的弹性一直是计算社会选择中的一个核心话题。许多投票规则，例如{\ it collecality}，都被证明是脆弱的。如果采用当地的差异隐私（LDP）机制使选民的真正偏好在选举前民意调查中从未揭示？在这种情况下，攻击者只能推断选民真正偏好的随机信息，这可能会导致对选举结果的操纵更加困难。本文的目的是进行定量研究，该研究对采用有利可图机制对投票系统的效果进行定量研究。我们介绍了公制的POLDP（LDP的功率），该公制在LDP机制下定量测量攻击者的操纵成本与没有LDP机制之间的差异。较大的POLDP是，LDP机制更健壮，可以添加到投票系统中。在本文中，我们通过任意评分规则为投票系统提供了POLDP的全面表征。我们的工作为使用最不发达国家机制的应用提供了一般指导。,https://arxiv.org/abs/2205.00771,IJCAI,True,False,False,False
1892,On the Convergence of Fictitious Play: A Decomposition Approach.,"Yurong Chen, Xiaotie Deng, Chenchen Li, David Mguni, Jun Wang, Xiang Yan, Yaodong Yang","Fictitious play (FP) is one of the most fundamental game-theoretical learning frameworks for computing Nash equilibrium in $n$-player games, which builds the foundation for modern multi-agent learning algorithms. Although FP has provable convergence guarantees on zero-sum games and potential games, many real-world problems are often a mixture of both and the convergence property of FP has not been fully studied yet. In this paper, we extend the convergence results of FP to the combinations of such games and beyond. Specifically, we derive new conditions for FP to converge by leveraging game decomposition techniques. We further develop a linear relationship unifying cooperation and competition in the sense that these two classes of games are mutually transferable. Finally, we analyze a non-convergent example of FP, the Shapley game, and develop sufficient conditions for FP to converge.",关于虚拟游戏的融合：一种分解方法。,虚拟游戏（FP）是用于计算$ N $玩家游戏中NASH平衡的最基本的游戏理论学习框架之一，该游戏为现代多项式学习算法奠定了基础。尽管FP在零和潜在游戏中可以证明可证明的融合保证，但是许多现实世界中的问题通常都是两者的混合，而FP的收敛属性尚未得到充分研究。在本文中，我们将FP的收敛结果扩展到了此类游戏及以后的组合。具体而言，我们通过利用游戏分解技术来得出FP收敛的新条件。我们进一步建立了统一合作和竞争的线性关系，这两个类别的游戏是相互转移的。最后，我们分析了Shapley游戏的FP的非代表示例，并为FP提供了足够的条件。,https://arxiv.org/abs/2205.01469,IJCAI,True,False,False,False
1893,Robust Weight Perturbation for Adversarial Training.,"Chaojian Yu, Bo Han, Mingming Gong, Li Shen, Shiming Ge, Bo Du, Tongliang Liu","Overfitting widely exists in adversarial robust training of deep networks. An effective remedy is adversarial weight perturbation, which injects the worst-case weight perturbation during network training by maximizing the classification loss on adversarial examples. Adversarial weight perturbation helps reduce the robust generalization gap; however, it also undermines the robustness improvement. A criterion that regulates the weight perturbation is therefore crucial for adversarial training. In this paper, we propose such a criterion, namely Loss Stationary Condition (LSC) for constrained perturbation. With LSC, we find that it is essential to conduct weight perturbation on adversarial data with small classification loss to eliminate robust overfitting. Weight perturbation on adversarial data with large classification loss is not necessary and may even lead to poor robustness. Based on these observations, we propose a robust perturbation strategy to constrain the extent of weight perturbation. The perturbation strategy prevents deep networks from overfitting while avoiding the side effect of excessive weight perturbation, significantly improving the robustness of adversarial training. Extensive experiments demonstrate the superiority of the proposed method over the state-of-the-art adversarial training methods.",对抗训练的强大重量扰动。,在对抗深网的对抗性训练中，过度拟合过度存在。一种有效的补救措施是对抗重量扰动，它通过最大化对抗性示例的分类损失，从而在网络训练过程中注入了最差的重量扰动。对抗重量扰动有助于减少稳健的概括差距；但是，这也破坏了稳健性的改善。因此，调节重量扰动的标准对于对抗训练至关重要。在本文中，我们提出了这种标准，即损失固定条件（LSC），以进行受约束的扰动。使用LSC，我们发现对具有少量分类损失的对抗数据进行重量扰动至关重要，以消除强大的过度拟合。对具有较大分类损失的对抗数据的重量扰动是不需要的，甚至可能导致稳健性差。基于这些观察结果，我们提出了一种强大的扰动策略来限制体重扰动的程度。扰动策略可防止深层网络过度拟合，同时避免重量过度扰动的副作用，从而显着提高对抗性训练的鲁棒性。广泛的实验证明了所提出的方法比最先进的对抗训练方法的优越性。,https://arxiv.org/abs/2205.14826,IJCAI,True,False,False,False
1894,Speaker-Guided Encoder-Decoder Framework for Emotion Recognition in Conversation.,"Yinan Bao, Qianwen Ma, Lingwei Wei, Wei Zhou, Songlin Hu","The emotion recognition in conversation (ERC) task aims to predict the emotion label of an utterance in a conversation. Since the dependencies between speakers are complex and dynamic, which consist of intra- and inter-speaker dependencies, the modeling of speaker-specific information is a vital role in ERC. Although existing researchers have proposed various methods of speaker interaction modeling, they cannot explore dynamic intra- and inter-speaker dependencies jointly, leading to the insufficient comprehension of context and further hindering emotion prediction. To this end, we design a novel speaker modeling scheme that explores intra- and inter-speaker dependencies jointly in a dynamic manner. Besides, we propose a Speaker-Guided Encoder-Decoder (SGED) framework for ERC, which fully exploits speaker information for the decoding of emotion. We use different existing methods as the conversational context encoder of our framework, showing the high scalability and flexibility of the proposed framework. Experimental results demonstrate the superiority and effectiveness of SGED.",演讲者指导的编码器框架在对话中进行情感识别。,对话（ERC）任务中的情感识别旨在预测对话中发音的情感标签。由于说话者之间的依赖性是复杂而动态的，这包括言论和言论者间的依赖性，因此说话者特定信息的建模是ERC中的至关重要的作用。尽管现有的研究人员提出了各种说话者互动建模的方法，但他们不能共同探索动态的言论和言论者的依赖性，从而导致对上下文的理解不足并进一步阻碍情绪预测。为此，我们设计了一种新颖的扬声器建模方案，该方案以动态方式共同探索言论和言论者的依赖性。此外，我们为ERC提出了一个演讲者引导的编码编码器（SGED）框架，该框架完全利用了说话者信息来解码情感。我们使用不同的现有方法作为我们框架的对话上下文编码器，显示了提出的框架的高扩展性和灵活性。实验结果证明了SGED的优势和有效性。,https://arxiv.org/abs/2206.03173,IJCAI,True,False,False,False
1895,Toward Policy Explanations for Multi-Agent Reinforcement Learning.,"Kayla Boggess, Sarit Kraus, Lu Feng","Advances in multi-agent reinforcement learning(MARL) enable sequential decision making for a range of exciting multi-agent applications such as cooperative AI and autonomous driving. Explaining agent decisions are crucial for improving system transparency, increasing user satisfaction, and facilitating human-agent collaboration. However, existing works on explainable reinforcement learning mostly focus on the single-agent setting and are not suitable for addressing challenges posed by multi-agent environments. We present novel methods to generate two types of policy explanations for MARL: (i) policy summarization about the agent cooperation and task sequence, and (ii) language explanations to answer queries about agent behavior. Experimental results on three MARL domains demonstrate the scalability of our methods. A user study shows that the generated explanations significantly improve user performance and increase subjective ratings on metrics such as user satisfaction.",迈向多代理强化学习的政策解释。,多机构增强学习（MARL）的进步可以为一系列令人兴奋的多代理应用程序（例如合作AI和自主驾驶）提供顺序决策。解释代理决策对于提高系统透明度，提高用户满意度以及促进人类代理协作至关重要。但是，现有关于可解释的强化学习的作品主要集中在单一代理设置上，并且不适合解决由多机构环境提出的挑战。我们提出了新的方法来生成MARL的两种类型的策略解释：（i）有关代理合作和任务顺序的政策摘要，以及（ii）语言解释以回答有关代理行为的查询。三个MARL结构域的实验结果证明了我们方法的可扩展性。一项用户研究表明，生成的解释可显着提高用户性能，并提高对用户满意度等指标的主观评分。,https://arxiv.org/abs/2204.12568,IJCAI,True,False,False,False
1896,Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting.,"hongyuan yu, tian li, weichen yu, jianguo li, yan huang, liang wang, alex liu","Multivariate time-series forecasting is a critical task for many applications, and graph time-series network is widely studied due to its capability to capture the spatial-temporal correlation simultaneously. However, most existing works focus more on learning with the explicit prior graph structure, while ignoring potential information from the implicit graph structure, yielding incomplete structure modeling. Some recent works attempts to learn the intrinsic or implicit graph structure directly, while lacking a way to combine explicit prior structure with implicit structure together. In this paper, we propose Regularized Graph Structure Learning (RGSL) model to incorporate both explicit prior structure and implicit structure together, and learn the forecasting deep networks along with the graph structure. RGSL consists of two innovative modules. First, we derive an implicit dense similarity matrix through node embedding, and learn the sparse graph structure using the Regularized Graph Generation (RGG) based on the Gumbel Softmax trick. Second, we propose a Laplacian Matrix Mixed-up Module (LM3) to fuse the explicit graph and implicit graph together. We conduct experiments on three real-word datasets. Results show that the proposed RGSL model outperforms existing graph forecasting algorithms with a notable margin, while learning meaningful graph structure simultaneously. Our code and models are made publicly available at https://github.com/alipay/RGSL.git.",带有语义知识的多变量时间序列预测的正规图结构学习。,多元时间序列预测是许多应用程序的关键任务，并且由于其能力同时捕获时空相关性，因此对图表时间序列网络进行了广泛的研究。但是，大多数现有的作品更多地集中在使用明确的先验图结构上学习，同时忽略隐式图结构中的潜在信息，从而产生不完整的结构建模。最近的一些作品试图直接学习内在或隐式图结构，同时缺乏将显式先验结构与隐式结构结合在一起的方法。在本文中，我们提出了正则图结构学习（RGSL）模型，以将显式的先验结构和隐式结构融合在一起，并将预测深网与图形结构一起学习。RGSL由两个创新的模块组成。首先，我们通过节点嵌入得出一个隐式密集的相似性矩阵，并根据Gumbel SoftMax技巧学习稀疏的图结构（RGG）。其次，我们提出了一个laplacian矩阵混合模块（LM3），以将显式图和隐式图融合在一起。我们在三个现实单词数据集上进行实验。结果表明，所提出的RGSL模型在同时学习有意义的图形结构的同时，优于现有的图形预测算法。我们的代码和模型可在https://github.com/alipay/rgsl.git上公开提供。,https://yanrockhuang.github.io/,IJCAI,True,False,True,False
1897,Poisoning Deep Learning Based Recommender Model in Federated Learning Scenarios.,"Dazhong Rong, Qinming He, Jianhai Chen","Various attack methods against recommender systems have been proposed in the past years, and the security issues of recommender systems have drawn considerable attention. Traditional attacks attempt to make target items recommended to as many users as possible by poisoning the training data. Benifiting from the feature of protecting users' private data, federated recommendation can effectively defend such attacks. Therefore, quite a few works have devoted themselves to developing federated recommender systems. For proving current federated recommendation is still vulnerable, in this work we probe to design attack approaches targeting deep learning based recommender models in federated learning scenarios. Specifically, our attacks generate poisoned gradients for manipulated malicious users to upload based on two strategies (i.e., random approximation and hard user mining). Extensive experiments show that our well-designed attacks can effectively poison the target models, and the attack effectiveness sets the state-of-the-art.",在联合学习方案中中毒基于深度学习的建议模型。,在过去的几年中，已经提出了针对推荐系统的各种攻击方法，并且推荐系统的安全问题引起了极大的关注。传统攻击试图通过毒害培训数据来使目标物品推荐给尽可能多的用户。从保护用户的私人数据的功能中，联邦建议可以有效地捍卫此类攻击。因此，很多作品致力于开发联合推荐系统。为了证明当前的联合建议仍然很容易受到伤害，在这项工作中，我们探究了针对联合学习方案中基于深度学习的建议模型的设计攻击方法。具体而言，我们的攻击会产生中毒的梯度，以根据两种策略（即随机近似和硬用户挖掘）上传操纵的恶意用户。广泛的实验表明，我们精心设计的攻击可以有效地毒害目标模型，并且攻击效率为最新的攻击效果。,https://arxiv.org/abs/2204.13594,IJCAI,True,False,False,False
1898,Causes of Effects: Learning Individual Responses from Population Data.,"Scott Mueller, Ang Li, Judea Pearl","The problem of individualization is recognized as crucial in almost every field. Identifying causes of effects in specific events is likewise essential for accurate decision making. However, such estimates invoke counterfactual relationships, and are therefore indeterminable from population data. For example, the probability of benefiting from a treatment concerns an individual having a favorable outcome if treated and an unfavorable outcome if untreated. Experiments conditioning on fine-grained features are fundamentally inadequate because we can't test both possibilities for an individual. Tian and Pearl provided bounds on this and other probabilities of causation using a combination of experimental and observational data. Even though those bounds were proven tight, narrower bounds, sometimes significantly so, can be achieved when structural information is available in the form of a causal model. This has the power to solve central problems, such as explainable AI, legal responsibility, and personalized medicine, all of which demand counterfactual logic. We analyze and expand on existing research by applying bounds to the probability of necessity and sufficiency (PNS) along with graphical criteria and practical applications.",效果原因：从人口数据中学习个人反应。,在几乎每个领域，个性化问题都被认为是至关重要的。在特定事件中识别效果原因对于准确的决策也至关重要。但是，此类估计值调用反事实关系，因此从人口数据中不确定。例如，从治疗中受益的可能性涉及一个人，如果接受治疗，则具有有利的结果和不利的结果。基于细粒度特征的实验从根本上是不足的，因为我们无法测试个人的两种可能性。田和珍珠通过实验和观察数据的组合就因果关系和其他因果概率提供了界限。即使这些界限被证明是紧密的，狭窄的范围，有时甚至是显着的，当以因果模型的形式获得结构信息时，也可以实现。这有能力解决中心问题，例如可解释的AI，法律责任和个性化医学，所有这些都需要反事实逻辑。我们通过将界限应用于必要性和充分性（PNS）以及图形标准和实际应用来分析现有研究和扩展。,https://arxiv.org/abs/2104.13730,IJCAI,True,False,False,False
1899,Simple and Effective Relation-based Embedding Propagation for Knowledge Representation Learning.,"Huijuan Wang, Siming Dai, Weiyue Su, Hui Zhong, Zeyang Fang, Zhengjie Huang, Shikun Feng, Zeyu Chen, Yu Sun, Dianhai Yu","Relational graph neural networks have garnered particular attention to encode graph context in knowledge graphs (KGs). Although they achieved competitive performance on small KGs, how to efficiently and effectively utilize graph context for large KGs remains an open problem. To this end, we propose the Relation-based Embedding Propagation (REP) method. It is a post-processing technique to adapt pre-trained KG embeddings with graph context. As relations in KGs are directional, we model the incoming head context and the outgoing tail context separately. Accordingly, we design relational context functions with no external parameters. Besides, we use averaging to aggregate context information, making REP more computation-efficient. We theoretically prove that such designs can avoid information distortion during propagation. Extensive experiments also demonstrate that REP has significant scalability while improving or maintaining prediction quality. Notably, it averagely brings about 10% relative improvement to triplet-based embedding methods on OGBL-WikiKG2 and takes 5%-83% time to achieve comparable results as the state-of-the-art GC-OTE.",简单有效的基于关系的嵌入传播，用于知识表示学习。,关系图神经网络特别注意知识图（kgs）中的编码图上下文。尽管他们在小公斤上取得了竞争性的表现，但如何有效地利用大公斤的图形上下文仍然是一个开放的问题。为此，我们提出了基于关系的嵌入传播（REP）方法。这是一种后处理技术，可将预训练的KG嵌入具有图形上下文。由于kg中的关系是定向的，因此我们分别对传入的头部上下文和即将离开的尾部上下文进行建模。因此，我们设计了无外部参数的关系上下文功能。此外，我们使用平均来汇总上下文信息，从而使REP提高计算效率。从理论上讲，我们证明了这种设计可以避免在传播过程中避免信息失真。广泛的实验还表明，REP在改善或维持预测质量的同时具有显着的可伸缩性。值得注意的是，它平均带来了大约10％的相对改善，可在OGBL-Wikikg2上使用基于三重态的嵌入方法，并花费5％-83％的时间获得可比的结果作为最先进的GC-ote。,https://arxiv.org/abs/2205.06456,IJCAI,True,False,False,False
1900,SparseTT: Visual Tracking with Sparse Transformers.,"Zhihong Fu, Zehua Fu, Qingjie Liu, Wenrui Cai, Yunhong Wang","Transformers have been successfully applied to the visual tracking task and significantly promote tracking performance. The self-attention mechanism designed to model long-range dependencies is the key to the success of Transformers. However, self-attention lacks focusing on the most relevant information in the search regions, making it easy to be distracted by background. In this paper, we relieve this issue with a sparse attention mechanism by focusing the most relevant information in the search regions, which enables a much accurate tracking. Furthermore, we introduce a double-head predictor to boost the accuracy of foreground-background classification and regression of target bounding boxes, which further improve the tracking performance. Extensive experiments show that, without bells and whistles, our method significantly outperforms the state-of-the-art approaches on LaSOT, GOT-10k, TrackingNet, and UAV123, while running at 40 FPS. Notably, the training time of our method is reduced by 75% compared to that of TransT. The source code and models are available at https://github.com/fzh0917/SparseTT.",Sparsett：稀疏变压器的视觉跟踪。,变压器已成功应用于视觉跟踪任务，并显着促进跟踪性能。旨在建模远程依赖性的自我注意机制是变形金刚成功的关键。但是，自我注意力缺乏专注于搜索区域中最相关的信息，因此很容易被背景分散注意力。在本文中，我们通过将最相关的信息集中在搜索区域中，以稀疏的注意机制缓解了这个问题，从而实现了非常准确的跟踪。此外，我们引入了双头预测指标，以提高前景 - 背景分类和目标边界框的回归的准确性，从而进一步改善了跟踪性能。广泛的实验表明，如果没有铃铛和哨声，我们的方法在40 fps运行时，在Lasot，got-10k，trackingnet和uav123上的最先进方法大大优于最先进的方法。值得注意的是，与Transt相比，我们方法的训练时间减少了75％。源代码和型号可在https://github.com/fzh0917/sparett上获得。,https://arxiv.org/abs/2205.03776,IJCAI,True,False,False,False
1901,Distilling Inter-Class Distance for Semantic Segmentation.,"Zhengbo Zhang, Chunluan Zhou, Zhigang Tu","Knowledge distillation is widely adopted in semantic segmentation to reduce the computation cost.The previous knowledge distillation methods for semantic segmentation focus on pixel-wise feature alignment and intra-class feature variation distillation, neglecting to transfer the knowledge of the inter-class distance in the feature space, which is important for semantic segmentation. To address this issue, we propose an Inter-class Distance Distillation (IDD) method to transfer the inter-class distance in the feature space from the teacher network to the student network. Furthermore, semantic segmentation is a position-dependent task,thus we exploit a position information distillation module to help the student network encode more position information. Extensive experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show that our method is helpful to improve the accuracy of semantic segmentation models and achieves the state-of-the-art performance. E.g. it boosts the benchmark model(""PSPNet+ResNet18"") by 7.50% in accuracy on the Cityscapes dataset.",蒸馏出语义分割的类间距离。,在语义分段中广泛采用知识蒸馏以降低计算成本。先前的知识蒸馏方法的语义分割方法的重点是像素的特征特征对齐和阶级内特征变化蒸馏，从特征空间，这对于语义分割很重要。为了解决此问题，我们提出了一种类间距离蒸馏（IDD）方法，以将特征空间中的类间距离从教师网络转移到学生网络。此外，语义分割是一项依赖位置的任务，因此我们利用位置信息蒸馏模块来帮助学生网络编码更多的位置信息。在三个受欢迎的数据集上进行了广泛的实验：CityScapes，Pascal VOC和ADE20K表明，我们的方法有助于提高语义细分模型的准确性并实现最先进的性能。例如。它在CityScapes数据集上的准确性将基准模型（“ PSPNET+RESNET18”）提高了7.50％。,https://arxiv.org/abs/2205.03650,IJCAI,True,False,False,False
1902,Multi-level Consistency Learning for Semi-supervised Domain Adaptation.,"Zizheng Yan, Yushuang Wu, Guanbin Li, Yipeng Qin, Xiaoguang Han, Shuguang Cui","Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from a fully labeled source domain to a scarcely labeled target domain. In this paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA. Specifically, our MCL regularizes the consistency of different views of target domain samples at three levels: (i) at inter-domain level, we robustly and accurately align the source and target domains using a prototype-based optimal transport method that utilizes the pros and cons of different views of target samples; (ii) at intra-domain level, we facilitate the learning of both discriminative and compact target feature representations by proposing a novel class-wise contrastive clustering loss; (iii) at sample level, we follow standard practice and improve the prediction accuracy by conducting a consistency-based self-training. Empirically, we verified the effectiveness of our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet, and Office-Home datasets, and the experimental results demonstrate that our MCL framework achieves the state-of-the-art performance.",半监督域适应的多级一致性学习。,半监督域的适应性（SSDA）旨在将从完全标记的源域学习的知识应用于几乎没有标记的目标域。在本文中，我们为SSDA提出了一个多级一致性学习（MCL）框架。具体而言，我们的MCL将目标域样本的不同视图的一致性定于三个级别：（i）在域间级别，我们使用基于原型的最佳传输方法来稳健，准确地对准源和目标域，该方法利用了PROS和PROS和PROS域目标样本不同观点的缺点；（ii）在域内层面上，我们通过提出新颖的班级对比聚类损失来促进歧视性和紧凑的目标特征表示。（iii）在样本级别，我们遵循标准实践，并通过进行基于一致性的自我训练来提高预测准确性。从经验上讲，我们验证了MCL框架对三个流行的SSDA基准的有效性，即Visda2017，Domainnet和Office-home数据集，实验结果表明我们的MCL框架可以实现最新的性能。,https://arxiv.org/abs/2205.04066,IJCAI,True,False,False,False
1903,Few-Shot Adaptation of Pre-Trained Networks for Domain Shift.,"Wenyu Zhang, Li Shen, Wanyue Zhang, Chuan-Sheng Foo","Deep networks are prone to performance degradation when there is a domain shift between the source (training) data and target (test) data. Recent test-time adaptation methods update batch normalization layers of pre-trained source models deployed in new target environments with streaming data to mitigate such performance degradation. Although such methods can adapt on-the-fly without first collecting a large target domain dataset, their performance is dependent on streaming conditions such as mini-batch size and class-distribution, which can be unpredictable in practice. In this work, we propose a framework for few-shot domain adaptation to address the practical challenges of data-efficient adaptation. Specifically, we propose a constrained optimization of feature normalization statistics in pre-trained source models supervised by a small support set from the target domain. Our method is easy to implement and improves source model performance with as few as one sample per class for classification tasks. Extensive experiments on 5 cross-domain classification and 4 semantic segmentation datasets show that our method achieves more accurate and reliable performance than test-time adaptation, while not being constrained by streaming conditions.",用于域移动的预训练网络几乎没有适应。,当源（训练）数据和目标（测试）数据之间存在域移动时，深网很容易降级。最近的测试时间适应方法更新了通过流数据部署在新目标环境中的预训练源模型的批归归式层，以减轻这种性能降低。尽管此类方法可以在不首先收集大型目标域数据集的情况下进行调整，但它们的性能取决于流媒体条件，例如迷你批量大小和类别分布，在实践中可能是无法预测的。在这项工作中，我们提出了一个框架，以适应几个域的适应性，以应对数据有效适应的实际挑战。具体而言，我们提出了在预训练的源模型中对特征归一化统计量的约束优化，该模型由目标域的小支持集监督。我们的方法易于实现，并改善每类用于分类任务的示例较小的源模型性能。对5个跨域分类和4个语义分割数据集进行了广泛的实验表明，我们的方法比测试时间适应更准确，更可靠，同时不受流媒体条件的限制。,https://arxiv.org/abs/2205.15234,IJCAI,True,False,False,False
1904,C3-STISR: Scene Text Image Super-resolution with Triple Clues.,"Minyi Zhao, Miao Wang, Fan Bai, Bingjia Li, Jie Wang, Shuigeng Zhou","Scene text image super-resolution (STISR) has been regarded as an important pre-processing task for text recognition from low-resolution scene text images. Most recent approaches use the recognizer's feedback as clues to guide super-resolution. However, directly using recognition clue has two problems: 1) Compatibility. It is in the form of probability distribution, has an obvious modal gap with STISR - a pixel-level task; 2) Inaccuracy. it usually contains wrong information, thus will mislead the main task and degrade super-resolution performance. In this paper, we present a novel method C3-STISR that jointly exploits the recognizer's feedback, visual and linguistical information as clues to guide super-resolution. Here, visual clue is from the images of texts predicted by the recognizer, which is informative and more compatible with the STISR task; while linguistical clue is generated by a pre-trained character-level language model, which is able to correct the predicted texts. We design effective extraction and fusion mechanisms for the triple cross-modal clues to generate a comprehensive and unified guidance for super-resolution. Extensive experiments on TextZoom show that C3-STISR outperforms the SOTA methods in fidelity and recognition performance. Code is available in https://github.com/zhaominyiz/C3-STISR.",C3-STISR：带有三重线索的场景文本图像超分辨率。,场景文本图像超分辨率（STISR）被视为从低分辨率场景文本图像中识别文本识别的重要预处理任务。最近的方法将识别器的反馈用作指导超分辨率的线索。但是，直接使用识别线索有两个问题：1）兼容性。它是概率分布的形式，具有STISR的明显模态差距 - 像素级任务；2）不准确。它通常包含错误的信息，因此会误导主要任务并降低超分辨率性能。在本文中，我们提出了一种新型方法C3-STISR，该方法将识别器的反馈，视觉和语言信息共同利用作为指导超分辨率的线索。在这里，视觉线索来自识别器预测的文本的图像，该文本的信息丰富且与STISR任务更兼容。尽管语言线索是由预先训练的字符级语言模型生成的，该模型能够纠正预测的文本。我们为三重跨模式线索设计有效的提取和融合机制，以生成全面而统一的超分辨率指导。TextZoom上的广泛实验表明，C3-STISR在保真度和识别性能方面的表现优于SOTA方法。代码可在https://github.com/zhaominyiz/c3-stisr中找到。,https://arxiv.org/abs/2204.14044,IJCAI,True,False,False,False
1905,"BiCo-Net: Regress Globally, Match Locally for Robust 6D Pose Estimation.","Zelin Xu, Yichen Zhang, Ke Chen, Kui Jia","The challenges of learning a robust 6D pose function lie in 1) severe occlusion and 2) systematic noises in depth images. Inspired by the success of point-pair features, the goal of this paper is to recover the 6D pose of an object instance segmented from RGB-D images by locally matching pairs of oriented points between the model and camera space. To this end, we propose a novel Bi-directional Correspondence Mapping Network (BiCo-Net) to first generate point clouds guided by a typical pose regression, which can thus incorporate pose-sensitive information to optimize generation of local coordinates and their normal vectors. As pose predictions via geometric computation only rely on one single pair of local oriented points, our BiCo-Net can achieve robustness against sparse and occluded point clouds. An ensemble of redundant pose predictions from locally matching and direct pose regression further refines final pose output against noisy observations. Experimental results on three popularly benchmarking datasets can verify that our method can achieve state-of-the-art performance, especially for the more challenging severe occluded scenes. Source codes are available at https://github.com/Gorilla-Lab-SCUT/BiCo-Net.",BICO-NET：全球回归，局部匹配，以进行健壮的6D姿势估计。,学习强大的6D姿势功能的挑战在于1）严重的阻塞和2）深度图像中的系统噪声。受点对功能的成功启发，本文的目标是通过在型号和摄像机空间之间的本地匹配的对象对恢复从RGB-D图像分割的对象实例的6D姿势。为此，我们提出了一个新颖的双向对应图映射网络（BICO-NET），以首先生成以典型的姿势回归为引导的点云，因此可以将姿势敏感的信息纳入，以优化局部坐标及其正常矢量的产生。由于通过几何计算进行姿势预测仅依赖于一对局部方向点，因此我们的BICO-NET可以针对稀疏和遮挡的点云实现稳健性。来自局部匹配和直接姿势回归的冗余姿势预测的合奏进一步完善了最终姿势输出，以噪音观察。三个普遍的基准测试数据集的实验结果可以验证我们的方法可以实现最先进的性能，尤其是对于更具挑战性的严重封闭场景。源代码可在https://github.com/gorilla-lab-scut/bico-net上找到。,https://arxiv.org/abs/2205.03536,IJCAI,True,False,False,False
1906,HCFRec: Hash Collaborative Filtering via Normalized Flow with Structural Consensus for Efficient Recommendation.,"Fan Wang, Weiming Liu, Chaochao Chen, Mengying Zhu, Xiaolin Zheng","The ever-increasing data scale of user-item interactions makes it challenging for an effective and efficient recommender system. Recently, hash-based collaborative filtering (Hash-CF) approaches employ efficient Hamming distance of learned binary representations of users and items to accelerate recommendations. However, Hash-CF often faces two challenging problems, i.e., optimization on discrete representations and preserving semantic information in learned representations. To address the above two challenges, we propose HCFRec, a novel Hash-CF approach for effective and efficient recommendations. Specifically, HCFRec not only innovatively introduces normalized flow to learn the optimal hash code by efficiently fit a proposed approximate mixture multivariate normal distribution, a continuous but approximately discrete distribution, but also deploys a cluster consistency preserving mechanism to preserve the semantic structure in representations for more accurate recommendations. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our HCFRec compared to the state-of-art methods in terms of effectiveness and efficiency.",HCFREC：通过归一化流量进行结构共识，以进行有效建议。,用户项目交互的越来越多的数据量表使其对于有效，有效的推荐系统而言具有挑战性。最近，基于哈希的协作过滤（HASH-CF）方法采用了有效的用户和项目的二进制表示的有效的锤击距离来加速建议。但是，Hash-CF通常面临两个具有挑战性的问题，即对离散表示形式进行优化并保留学习表示的语义信息。为了应对上述两个挑战，我们提出了HCFREC，这是一种新型的Hash-CF方法，以进行有效，有效的建议。具体而言，HCFREC不仅通过有效拟合提出的近似混合物多变量正态分布，连续但近似离散的分布来进行创新的归一化流程，以学习最佳哈希代码，而且还部署了群集一致性的保留机制，以将语义结构保留在表示形式中，以使其在表示形式中保留更多准确的建议。在六个现实世界数据集上进行的广泛实验证明了我们HCFREC的优越性，而在有效性和效率方面，与制作方法相比。,https://arxiv.org/abs/2205.12042,IJCAI,True,False,False,False
1907,Dynamic Sparse Training for Deep Reinforcement Learning.,"Ghada Sokar, Elena Mocanu, Decebal Constantin Mocanu, Mykola Pechenizkiy, Peter Stone","Deep reinforcement learning has achieved significant success in many decision-making tasks in various fields. However, it requires a large training time of dense neural networks to obtain a good performance. This hinders its applicability on low-resource devices where memory and computation are strictly constrained. In a step towards enabling deep reinforcement learning agents to be applied to low-resource devices, in this work, we propose for the first time to dynamically train deep reinforcement learning agents with sparse neural networks from scratch. We adopt the evolution principles of dynamic sparse training in the reinforcement learning paradigm and introduce a training algorithm that optimizes the sparse topology and the weight values jointly to dynamically fit the incoming data. Our approach is easy to be integrated into existing deep reinforcement learning algorithms and has many favorable advantages. First, it allows for significant compression of the network size which reduces the memory and computation costs substantially. This would accelerate not only the agent inference but also its training process. Second, it speeds up the agent learning process and allows for reducing the number of required training steps. Third, it can achieve higher performance than training the dense counterpart network. We evaluate our approach on OpenAI gym continuous control tasks. The experimental results show the effectiveness of our approach in achieving higher performance than one of the state-of-art baselines with a 50\% reduction in the network size and floating-point operations (FLOPs). Moreover, our proposed approach can reach the same performance achieved by the dense network with a 40-50\% reduction in the number of training steps.",深入强化学习的动态稀疏训练。,深厚的强化学习在各个领域的许多决策任务中都取得了重大成功。但是，它需要大量的密集神经网络训练时间才能获得良好的表现。这阻碍了其在严格限制内存和计算的低资源设备上的适用性。在朝着使深钢筋学习能力应用于低资源设备的一步之中，在这项工作中，我们首次提出了动态训练来自SCRATCH的稀疏神经网络的深钢筋学习剂。我们在增强学习范式中采用动态稀疏训练的演化原理，并引入了一种训练算法，该算法优化了稀疏拓扑和重量值，以动态拟合传入的数据。我们的方法很容易被整合到现有的深度强化学习算法中，并且具有许多优惠的优势。首先，它允许对网络大小进行重大压缩，从而大大降低内存和计算成本。这不仅会加速代理推断，而且还会加速其培训过程。其次，它加快了代理学习过程，并允许减少所需的训练步骤的数量。第三，它可以比培训密集的对应网络获得更高的性能。我们在OpenAI Gym连续控制任务上评估了我们的方法。实验结果表明，与最先进的基线之一相比，我们的方法在达到更高的性能方面的有效性，网络大小和浮点操作（FLOPS）降低了50 \％。此外，我们提出的方法可以达到密集网络所达到的相同性能，而训练步骤的数量减少了40-50 \％。,https://arxiv.org/abs/2106.04217,IJCAI,True,False,False,False
1908,Grape: Grammar Preserving Rule Embedding,"qihao zhu, zeyu sun, wenjie zhang, yingfei xiong, lu zhang",nan,葡萄：语法保存规则嵌入, ,https://xiongyingfei.github.io/,IJCAI,False,True,False,False
1909,Game Redesign in No-regret Game Playing.,"Yuzhe Ma, Young Wu, Xiaojin Zhu","We study the game redesign problem in which an external designer has the ability to change the payoff function in each round, but incurs a design cost for deviating from the original game. The players apply no-regret learning algorithms to repeatedly play the changed games with limited feedback. The goals of the designer are to (i) incentivize all players to take a specific target action profile frequently; and (ii) incur small cumulative design cost. We present game redesign algorithms with the guarantee that the target action profile is played in T-o(T) rounds while incurring only o(T) cumulative design cost. Game redesign describes both positive and negative applications: a benevolent designer who incentivizes players to take a target action profile with better social welfare compared to the solution of the original game, or a malicious attacker whose target action profile benefits themselves but not the players. Simulations on four classic games confirm the effectiveness of our proposed redesign algorithms.",游戏重新设计无重格游戏游戏。,我们研究了游戏重新设计问题，其中外部设计师可以在每回合中更改收益功能，但会产生与原始游戏偏离的设计成本。玩家使用无重格学习算法来反复使用有限的反馈来反复玩更改的游戏。设计师的目标是（i）激励所有玩家经常采取特定的目标动作概况；（ii）产生小累积设计成本。我们提供游戏重新设计算法，并保证目标动作概况在T-O（T）回合中播放，同时仅产生O（T）累积设计成本。游戏重新设计描述了正面和负面应用：与原始游戏的解决方案相比，激励玩家以更好的社会福利采取目标行动概况，或者是针对目标动作概况的恶意攻击者，而目标行动概况使自己受益于自己但没有玩家。四个经典游戏的模拟证实了我们提议的重新设计算法的有效性。,https://arxiv.org/abs/2110.11763,IJCAI,True,False,False,False
1910,Post-processing of Differentially Private Data: A Fairness Perspective.,"Keyu Zhu, Ferdinando Fioretto, Pascal Van Hentenryck","Post-processing immunity is a fundamental property of differential privacy: it enables arbitrary data-independent transformations to differentially private outputs without affecting their privacy guarantees. Post-processing is routinely applied in data-release applications, including census data, which are then used to make allocations with substantial societal impacts. This paper shows that post-processing causes disparate impacts on individuals or groups and analyzes two critical settings: the release of differentially private datasets and the use of such private datasets for downstream decisions, such as the allocation of funds informed by US Census data. In the first setting, the paper proposes tight bounds on the unfairness of traditional post-processing mechanisms, giving a unique tool to decision-makers to quantify the disparate impacts introduced by their release. In the second setting, this paper proposes a novel post-processing mechanism that is (approximately) optimal under different fairness metrics, either reducing fairness issues substantially or reducing the cost of privacy. The theoretical analysis is complemented with numerical simulations on Census data.",差异私人数据的后处理：公平的观点。,后处理免疫是具有差异隐私的基本属性：它可以将任意数据独立的转换转换为差异化私人产出而不会影响其隐私保证。后处理通常用于数据释放应用程序，包括人口普查数据，然后将其用于对具有重大社会影响的分配。本文表明，后处理会对个人或小组产生不同的影响，并分析两个关键设置：释放差异私有数据集以及使用此类私人数据集用于下游决策，例如美国人口普查数据提供的资金分配。在第一个环境中，本文提出了关于传统后处理机制不公平性的紧密界限，为决策者提供了独特的工具，以量化其发布所带来的不同影响。在第二个环境中，本文提出了一种新颖的后处理机制，该机制（大约）在不同的公平指标下是最佳的，要么大大降低公平性问题或降低隐私成本。理论分析与人口普查数据的数值模拟相辅相成。,https://arxiv.org/abs/2201.09425,IJCAI,True,False,False,False
1911,Fourier Analysis-based Iterative Combinatorial Auctions.,"Jakob Weissteiner, Chris Wendler, Sven Seuken, Ben Lubin, Markus Püschel","Recent advances in Fourier analysis have brought new tools to efficiently represent and learn set functions. In this paper, we bring the power of Fourier analysis to the design of combinatorial auctions (CAs). The key idea is to approximate bidders' value functions using Fourier-sparse set functions, which can be computed using a relatively small number of queries. Since this number is still too large for practical CAs, we propose a new hybrid design: we first use neural networks (NNs) to learn bidders' values and then apply Fourier analysis to the learned representations. On a technical level, we formulate a Fourier transform-based winner determination problem and derive its mixed integer program formulation. Based on this, we devise an iterative CA that asks Fourier-based queries. We experimentally show that our hybrid ICA achieves higher efficiency than prior auction designs, leads to a fairer distribution of social welfare, and significantly reduces runtime. With this paper, we are the first to leverage Fourier analysis in CA design and lay the foundation for future work in this area. Our code is available on GitHub: https://github.com/marketdesignresearch/FA-based-ICAs.",基于傅立叶分析的迭代组合拍卖。,傅立叶分析的最新进展带来了有效代表和学习设定功能的新工具。在本文中，我们将傅立叶分析的力量带入了组合拍卖（CAS）的设计。关键想法是使用傅立叶 - 帕斯斯集合功能近似竞标者的值函数，可以使用相对较少的查询来计算。由于对于实用CAS而言，这个数字仍然太大，因此我们提出了一种新的混合设计：我们首先使用神经网络（NNS）来学习投标值的值，然后将傅立叶分析应用于学习的表示形式。在技术层面上，我们制定了一个基于傅立叶变换的获胜者确定问题，并得出了其混合整数计划的配方。基于此，我们设计了一个迭代CA，询问基于傅立叶的查询。我们在实验上表明，与先前的拍卖设计相比，混合ICA的效率更高，导致社会福利的分布更公平，并大大降低了运行时。在本文中，我们是第一个利用CA设计中的傅立叶分析的人，并为该领域的未来工作奠定了基础。我们的代码可在github：https：//github.com/marketdesignresearch/fa-lase-icas上获得。,https://arxiv.org/abs/2009.10749,IJCAI,True,False,False,False
1912,Empirical Bayesian Approaches for Robust Constraint-based Causal Discovery under Insufficient Data.,"Zijun Cui, Naiyu Yin, Yuru Wang, Qiang Ji","Causal discovery is to learn cause-effect relationships among variables given observational data and is important for many applications. Existing causal discovery methods assume data sufficiency, which may not be the case in many real world datasets. As a result, many existing causal discovery methods can fail under limited data. In this work, we propose Bayesian-augmented frequentist independence tests to improve the performance of constraint-based causal discovery methods under insufficient data: 1) We firstly introduce a Bayesian method to estimate mutual information (MI), based on which we propose a robust MI based independence test; 2) Secondly, we consider the Bayesian estimation of hypothesis likelihood and incorporate it into a well-defined statistical test, resulting in a robust statistical testing based independence test. We apply proposed independence tests to constraint-based causal discovery methods and evaluate the performance on benchmark datasets with insufficient samples. Experiments show significant performance improvement in terms of both accuracy and efficiency over SOTA methods.",在数据不足下，基于强大约束的因果发现的经验贝叶斯方法。,因果发现是学习给定观察数据的变量之间的因果关系，对于许多应用程序很重要。现有的因果发现方法假设数据足够，在许多现实世界数据集中可能并非如此。结果，在有限的数据下，许多现有的因果发现方法可能会失败。在这项工作中，我们提出了贝叶斯的频繁独立性测试，以在数据不足下提高基于约束的因果发现方法的性能：1）我们首先引入了一种贝叶斯方法来估计互信息（MI），我们提出了一个可靠的方法基于MI的独立测试；2）其次，我们考虑了假设可能性的贝叶斯估计，并将其纳入定义明确的统计检验中，从而进行了基于统计测试的强大独立性检验。我们将提出的独立测试应用于基于约束的因果发现方法，并评估样品不足的基准数据集上的性能。实验在SOTA方法的准确性和效率方面表现出显着的性能提高。,https://arxiv.org/abs/2206.08448,IJCAI,True,False,False,False
1913,On Discrete Truthful Heterogeneous Two-Facility Location.,"Panagiotis Kanellopoulos, Alexandros A. Voudouris, Rongsen Zhang","We revisit the discrete heterogeneous two-facility location problem, in which there is a set of agents that occupy nodes of a line graph, and have private approval preferences over two facilities. When the facilities are located at some nodes of the line, each agent derives a cost that is equal to her total distance from the facilities she approves. The goal is to decide where to locate the two facilities, so as to (a) incentivize the agents to truthfully report their preferences, and (b) achieve a good approximation of the minimum total (social) cost or the maximum cost among all agents. For both objectives, we design deterministic strategyproof mechanisms with approximation ratios that significantly outperform the state-of-the-art, and complement these results with (almost) tight lower bounds.",在离散的真实异构两国位置上。,我们重新审视离散的异质两面性位置问题，其中有一组代理占据了线图的节点，并且对两个设施具有私人批准偏好。当设施位于线路的某些节点上时，每个代理商的成本等于她与她所批准的设施的总距离。目的是决定在哪里找到这两种设施，以（a）激励代理商真实地报告其偏好，以及（b）实现所有最低总成本或所有代理商中最高成本的良好近似。对于这两个目标，我们设计具有近似值比的确定性策略性机制，其表现明显优于最先进的机制，并以（几乎）紧密的下限来补充这些结果。,https://arxiv.org/abs/2109.04234,IJCAI,True,False,False,False
1914,Individual Fairness Guarantees for Neural Networks.,"Elias Benussi, Andrea Patane, Matthew Wicker, Luca Laurenti, Marta Kwiatkowska","We consider the problem of certifying the individual fairness (IF) of feed-forward neural networks (NNs). In particular, we work with the $\epsilon$-$\delta$-IF formulation, which, given a NN and a similarity metric learnt from data, requires that the output difference between any pair of $\epsilon$-similar individuals is bounded by a maximum decision tolerance $\delta \geq 0$. Working with a range of metrics, including the Mahalanobis distance, we propose a method to overapproximate the resulting optimisation problem using piecewise-linear functions to lower and upper bound the NN's non-linearities globally over the input space. We encode this computation as the solution of a Mixed-Integer Linear Programming problem and demonstrate that it can be used to compute IF guarantees on four datasets widely used for fairness benchmarking. We show how this formulation can be used to encourage models' fairness at training time by modifying the NN loss, and empirically confirm our approach yields NNs that are orders of magnitude fairer than state-of-the-art methods.",个人公平保证神经网络。,我们考虑证明馈送前传神经网络（NNS）的个人公平性（IF）的问题。特别是，我们使用$ \ epsilon $  -  $ \ delta $  - 如果配方，鉴于nn和从数据中学到的相似性指标，要求任何一对$ \ epsilon $  - 类似的个人是由最大决策容忍$ \ delta \ geq 0 $界定。使用一系列指标（包括Mahalanobis距离），我们提出了一种使用分段线性函数过度陈述所得优化问题的方法，以在输入空间上降低NN的NN非线性。我们将此计算编码为混合企业线性编程问题的解决方案，并证明如果在四个数据集中保证了广泛用于公平基准测试的数据集，则可以使用它来计算。我们展示了如何通过修改NN损失来鼓励模型在训练时间的公平性，并从经验上证实我们的方法产生的NNS比最先进的方法更公平。,https://arxiv.org/abs/2205.05763,IJCAI,True,False,False,False
1915,Online Bin Packing with Predictions.,"Spyros Angelopoulos, Shahin Kamali, Kimia Shadkami","Bin packing is a classic optimization problem with a wide range of applications from load balancing in networks to supply chain management. In this work we study the online variant of the problem, in which a sequence of items of various sizes must be placed into a minimum number of bins of uniform capacity. The online algorithm is enhanced with a (potentially erroneous) prediction concerning the frequency of item sizes in the sequence. We design and analyze online algorithms with efficient tradeoffs between their consistency (i.e., the competitive ratio assuming no prediction error) and their robustness (i.e., the competitive ratio under adversarial error), and whose performance degrades gently as a function of the error. Previous work on this problem has only addressed the extreme cases with respect to the prediction error, and has relied on overly powerful and error-free prediction oracles.",在线垃圾箱包装带有预测。,BIN包装是一个经典的优化问题，从网络中的负载平衡到供应链管理的广泛应用。在这项工作中，我们研究了该问题的在线变体，其中必须将各种大小的一系列项目置于最小数量的均匀容量箱中。通过（可能错误的）预测来增强在线算法，该预测涉及序列中的项目大小的频率。我们在其一致性（即没有预测错误的竞争比率）及其稳健性（即，在对抗性错误下的竞争比）和它们的性能轻轻降低，它们的稳健性在线算法（即，其竞争比率没有预测错误）在线设计和分析在线算法。关于此问题的先前工作仅解决了有关预测错误的极端情况，并且依赖于过于强大且无错误的预测序列。,https://arxiv.org/abs/2102.03311,IJCAI,True,False,False,False
1916,Summary Markov Models for Event Sequences.,"Debarun Bhattacharjya, Saurabh Sihag, Oktie Hassanzadeh, Liza Bialik","Datasets involving sequences of different types of events without meaningful time stamps are prevalent in many applications, for instance when extracted from textual corpora. We propose a family of models for such event sequences -- summary Markov models -- where the probability of observing an event type depends only on a summary of historical occurrences of its influencing set of event types. This Markov model family is motivated by Granger causal models for time series, with the important distinction that only one event can occur in a position in an event sequence. We show that a unique minimal influencing set exists for any set of event types of interest and choice of summary function, formulate two novel models from the general family that represent specific sequence dynamics, and propose a greedy search algorithm for learning them from event sequence data. We conduct an experimental investigation comparing the proposed models with relevant baselines, and illustrate their knowledge acquisition and discovery capabilities through case studies involving sequences from text.",事件序列的摘要马尔可夫模型。,在许多应用程序中，涉及不同类型事件的序列的数据集在许多应用程序中普遍存在，例如从文本语料库中提取时。我们为此类事件序列（摘要马尔可夫模型）提出了一个模型家族，其中观察事件类型的可能性仅取决于其影响事件类型集的历史事件的摘要。这个马尔可夫模型家族是由Granger因果模型的时间序列激励，其重要区别是，事件序列中只能在一个位置中发生一个事件。我们表明，对于任何感兴趣的事件类型和摘要功能的选择，都存在一个独特的最小影响集，从一般家族中制定了代表特定序列动态的两个新型模型，并提出了一种贪婪的搜索算法，以从事件序列数据中学习它们。我们进行了一项实验研究，将提出的模型与相关基线进行比较，并通过涉及文本序列的案例研究来说明他们的知识获取和发现功能。,https://arxiv.org/abs/2205.03375,IJCAI,True,False,False,False
1917,Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention.,"Kai Liu, Tianyi Wu, Cong Liu, Guodong Guo","Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by each query attending to all keys/values, various methods have constrained the range of attention within local regions, where each query only attends to keys/values within a hand-crafted window. However, these hand-crafted window partition mechanisms are data-agnostic and ignore their input content, so it is likely that one query maybe attends to irrelevant keys/values. To address this issue, we propose a Dynamic Group Attention (DG-Attention), which dynamically divides all queries into multiple groups and selects the most relevant keys/values for each group. Our DG-Attention can flexibly model more relevant dependencies without any spatial constraint that is used in hand-crafted window based attention. Built on the DG-Attention, we develop a general vision transformer backbone named Dynamic Group Transformer (DGT). Extensive experiments show that our models can outperform the state-of-the-art methods on multiple common vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation.",动态组变压器：具有动态组注意的通用视觉变压器主链。,最近，变形金刚在各种视觉任务中表现出了有希望的表现。为了减少每个查询对所有键/值的查询引起的二次计算复杂性，各种方法都限制了本地区域内的关注范围，每个查询仅关注手工制作的窗口内的键/值。但是，这些手工制作的窗口分区机制是数据敏捷的，而忽略了其输入内容，因此很可能有一个查询可能会涉及无关的键/值。为了解决这个问题，我们提出了一个动态组的注意（DG注意），该问题将所有查询动态分为多个组，并为每个组选择最相关的密钥/值。我们的DG注意力可以灵活地对更相关的依赖性建模，而无需在基于手工制作的窗户注意力中使用任何空间约束。基于DG注意事项，我们开发了一个名为Dynamic Group Transformer（DGT）的通用视觉变压器骨干。广泛的实验表明，我们的模型可以胜过多个常见视觉任务的最新方法，包括图像分类，语义分割，对象检测和实例分割。,https://arxiv.org/abs/2203.03937,IJCAI,True,False,False,False
1918,Fairness without the Sensitive Attribute via Causal Variational Autoencoder.,"Vincent Grari, Sylvain Lamprier, Marcin Detyniecki","In recent years, most fairness strategies in machine learning models focus on mitigating unwanted biases by assuming that the sensitive information is observed. However this is not always possible in practice. Due to privacy purposes and var-ious regulations such as RGPD in EU, many personal sensitive attributes are frequently not collected. We notice a lack of approaches for mitigating bias in such difficult settings, in particular for achieving classical fairness objectives such as Demographic Parity and Equalized Odds. By leveraging recent developments for approximate inference, we propose an approach to fill this gap. Based on a causal graph, we rely on a new variational auto-encoding based framework named SRCVAE to infer a sensitive information proxy, that serve for bias mitigation in an adversarial fairness approach. We empirically demonstrate significant improvements over existing works in the field. We observe that the generated proxy's latent space recovers sensitive information and that our approach achieves a higher accuracy while obtaining the same level of fairness on two real datasets, as measured using com-mon fairness definitions.",通过因果变异自动编码器没有敏感属性的公平性。,近年来，通过假设观察到敏感信息，机器学习模型中的大多数公平策略都集中在减轻不必要的偏见上。但是，这在实践中并不总是可能。由于隐私目的和诸如欧盟的RGPD之类的销售法规，经常没有收集许多个人敏感属性。我们注意到缺乏减轻这种困难环境中偏见的方法，尤其是为了实现经典的公平目标，例如人口统计学和均衡的赔率。通过利用最新的发展进行近似推断，我们提出了一种填补这一空白的方法。基于因果图，我们依靠一个名为SRCVAE的新的基于自动编码的框架来推断敏感的信息代理，该信息可用于在对抗性公平方法中缓解偏见。我们从经验上证明了对该领域现有作品的重大改进。我们观察到，生成的代理的潜在空间恢复了敏感信息，并且我们的方法在使用com-mon公平定义测量的两个真实数据集上获得了更高的准确性，同时获得了两个真实数据集的相同水平的公平性。,https://arxiv.org/abs/2109.04999,IJCAI,True,False,False,False
1919,CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction.,"Jiaju Lin, Qin Chen, Jie Zhou, Jian Jin, Liang He","Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.",杯子：基于课程学习的及时调整，以进行隐式事件参数提取。,隐式事件参数提取（EAE）旨在确定可以散布在文档上的参数。以前的大多数工作都集中在学习参数和给定的触发因素之间的直接关系，而与远程依赖关系的隐式关系并未得到很好的研究。此外，最近基于神经网络的方法取决于大量的标记数据进行培训，这是由于高标签成本而无法获得的。在本文中，我们提出了一种基于课程学习的及时调整（CUP）方法，该方法通过四个学习阶段解决了隐性的EAE。阶段是根据语义图中与触发节点的关系定义的，该阶段很好地捕获了参数和触发器之间的长距离依赖关系。此外，我们将基于及时的编码器模型集成在一起，以从每个阶段中从预训练的语言模型（PLM）中获取相关的知识，在该阶段中，及时模板适应了学习进度以增强参数的推理。两个众所周知的基准数据集的实验结果显示了我们提出的方法的巨大优势。特别是，我们在完全监督和低数据的场景中胜过最先进的模型。,https://arxiv.org/abs/2205.00498,IJCAI,True,False,False,False
1920,Distilling Governing Laws and Source Input for Dynamical Systems from Videos.,"Lele Luan, Yang Liu, Hao Sun","Distilling interpretable physical laws from videos has led to expanded interest in the computer vision community recently thanks to the advances in deep learning, but still remains a great challenge. This paper introduces an end-to-end unsupervised deep learning framework to uncover the explicit governing equations of dynamics presented by moving object(s), based on recorded videos. Instead in the pixel (spatial) coordinate system of image space, the physical law is modeled in a regressed underlying physical coordinate system where the physical states follow potential explicit governing equations. A numerical integrator-based sparse regression module is designed and serves as a physical constraint to the autoencoder and coordinate system regression, and, in the meanwhile, uncover the parsimonious closed-form governing equations from the learned physical states. Experiments on simulated dynamical scenes show that the proposed method is able to distill closed-form governing equations and simultaneously identify unknown excitation input for several dynamical systems recorded by videos, which fills in the gap in literature where no existing methods are available and applicable for solving this type of problem.",从视频中提取管理法律和源输入。,从视频中提取可解释的物理定律，由于深度学习的进步，最近对计算机视觉社区的兴趣扩大，但仍然是一个巨大的挑战。本文介绍了一个端到端的无监督深度学习框架，以根据记录的视频揭示移动对象提出的明确管理方程。取而代之的是，在图像空间的像素（空间）坐标系统中，物理定律是在回归的基础物理坐标系中建模的，物理状态遵循潜在的显式管理方程。设计了基于数值集成器的稀疏回归模块，并作为对自动编码器和坐标系回归的物理约束，同时，从学到的物理状态中揭示了简约的封闭形式的管程。模拟动态场景上的实验表明，所提出的方法能够提炼封闭形式的管理方程式，并同时确定视频记录的几种动态系统的未知激发输入，该系统填补了文献中没有现有方法的空白，可用于求解且可用于求解。这种类型的问题。,https://arxiv.org/abs/2205.01314,IJCAI,True,False,False,False
1921,Searching for Optimal Subword Tokenization in Cross-domain NER.,"Ruotian Ma, Yiding Tan, Xin Zhou, Xuanting Chen, Di Liang, Sirui Wang, Wei Wu, Tao Gui, Qi Zhang","Input distribution shift is one of the vital problems in unsupervised domain adaptation (UDA). The most popular UDA approaches focus on domain-invariant representation learning, trying to align the features from different domains into similar feature distributions. However, these approaches ignore the direct alignment of input word distributions between domains, which is a vital factor in word-level classification tasks such as cross-domain NER. In this work, we shed new light on cross-domain NER by introducing a subword-level solution, X-Piece, for input word-level distribution shift in NER. Specifically, we re-tokenize the input words of the source domain to approach the target subword distribution, which is formulated and solved as an optimal transport problem. As this approach focuses on the input level, it can also be combined with previous DIRL methods for further improvement. Experimental results show the effectiveness of the proposed method based on BERT-tagger on four benchmark NER datasets. Also, the proposed method is proved to benefit DIRL methods such as DANN.",在跨域NER中搜索最佳子字令牌。,输入分布转移是无监督域适应（UDA）中的重要问题之一。最受欢迎的UDA方法集中在域不变表示学习上，试图将不同域中的功能调整为相似的特征分布。但是，这些方法忽略了域之间的输入单词分布的直接对齐，这是单词级分类任务（例如跨域NER）的重要因素。在这项工作中，我们通过引入子词级解决方案X-Pience来为输入单词级分布移动，从而为跨域NER开发了新的灯光。具体而言，我们将源域的输入单词重新划分以接近目标子词分布，该分布是作为最佳运输问题制定和解决的。由于这种方法着重于输入级别，因此它也可以与先前的DIRL方法相结合，以进一步改进。实验结果表明，基于四个基准NER数据集的Bert-Tagger所提出的方法的有效性。同样，事实证明，所提出的方法受益于诸如Dann之类的DIRL方法。,https://arxiv.org/abs/2206.03352,IJCAI,True,False,False,False
1922,Mutual Distillation Learning Network for Trajectory-User Linking.,"Wei Chen, Shuzhe Li, Chao Huang, Yanwei Yu, Yongguo Jiang, Junyu Dong","Trajectory-User Linking (TUL), which links trajectories to users who generate them, has been a challenging problem due to the sparsity in check-in mobility data. Existing methods ignore the utilization of historical data or rich contextual features in check-in data, resulting in poor performance for TUL task. In this paper, we propose a novel Mutual distillation learning network to solve the TUL problem for sparse check-in mobility data, named MainTUL. Specifically, MainTUL is composed of a Recurrent Neural Network (RNN) trajectory encoder that models sequential patterns of input trajectory and a temporal-aware Transformer trajectory encoder that captures long-term time dependencies for the corresponding augmented historical trajectories. Then, the knowledge learned on historical trajectories is transferred between the two trajectory encoders to guide the learning of both encoders to achieve mutual distillation of information. Experimental results on two real-world check-in mobility datasets demonstrate the superiority of MainTUL against state-of-the-art baselines. The source code of our model is available at https://github.com/Onedean/MainTUL.",用于轨迹 - 用户链接的相互蒸馏学习网络。,将轨迹链接到生成它们的用户的轨迹 - 用户链接（TUL），由于登记入住移动性数据的稀疏性，这是一个充满挑战的问题。现有方法忽略了登机数据中对历史数据或丰富上下文特征的利用，从而导致任务的性能差。在本文中，我们提出了一个新型的相互蒸馏学习网络，以解决稀疏的入住移动性数据的问题，名为Vailtul。具体而言，维护由复发性神经网络（RNN）轨迹编码器组成，该轨迹编码器对输入轨迹的顺序模式和时间感知的变压器轨迹编码器进行建模，该模式可捕获相应的增强历史轨迹的长期依赖性。然后，在两个轨迹编码器之间传递了有关历史轨迹的知识，以指导两个编码器的学习以实现信息的相互蒸馏。两个现实世界登记入住数据集的实验结果表明，维持对最新基线的优势。我们的模型的源代码可在https://github.com/onedean/maintul上获得。,https://arxiv.org/abs/2205.03773,IJCAI,True,False,False,False
1923,SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech.,"Zhenhui Ye, Zhou Zhao, Yi Ren, Fei Wu","The recent progress in non-autoregressive text-to-speech (NAR-TTS) has made fast and high-quality speech synthesis possible. However, current NAR-TTS models usually use phoneme sequence as input and thus cannot understand the tree-structured syntactic information of the input sequence, which hurts the prosody modeling. To this end, we propose SyntaSpeech, a syntax-aware and light-weight NAR-TTS model, which integrates tree-structured syntactic information into the prosody modeling modules in PortaSpeech \cite{ren2021portaspeech}. Specifically, 1) We build a syntactic graph based on the dependency tree of the input sentence, then process the text encoding with a syntactic graph encoder to extract the syntactic information. 2) We incorporate the extracted syntactic encoding with PortaSpeech to improve the prosody prediction. 3) We introduce a multi-length discriminator to replace the flow-based post-net in PortaSpeech, which simplifies the training pipeline and improves the inference speed, while keeping the naturalness of the generated audio. Experiments on three datasets not only show that the tree-structured syntactic information grants SyntaSpeech the ability to synthesize better audio with expressive prosody, but also demonstrate the generalization ability of SyntaSpeech to adapt to multiple languages and multi-speaker text-to-speech. Ablation studies demonstrate the necessity of each component in SyntaSpeech. Source code and audio samples are available at https://syntaspeech.github.io",Syntaspeech：语法感知的生成对抗文本到语音。,非自动回归文本到语音（NAR-TTS）的最新进展使快速和高质量的语音合成成为可能。但是，当前的NAR-TTS模型通常使用音素序列作为输入，因此无法理解输入序列的树结构句法信息，这会损害韵律建模。为此，我们提出了SyntaSpeech，这是一种语法感知和轻巧的NAR-TTS模型，该模型将树结构化的语法信息集成到Portaspeech \ cite {Ren2021portapeech}中的韵律模型模型中。具体而言，1）我们基于输入句的依赖关系构建句法图，然后使用语法图编码器处理文本编码以提取句法信息。2）我们将提取的句法编码与portaspeech结合在一起，以改善韵律预测。3）我们引入了一个多长度歧视器，以替换PortAspeech中基于流的后网，该歧视器简化了训练管道并提高了推理速度，同时保持了生成的音频的自然性。三个数据集上的实验不仅表明，树结构化的句法信息授予Syntaspeech与表达韵律合成更好的音频的能力，而且还展示了语法适应多种语言和多说话的多语言和多说话者文本对语的通用能力。消融研究表明，在语法中，每个组件的必要性。源代码和音频样本可在https://syntaspeech.github.io上获得,https://arxiv.org/abs/2204.11792,IJCAI,True,False,False,False
